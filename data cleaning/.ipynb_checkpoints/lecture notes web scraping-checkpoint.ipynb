{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec77bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from html import escape\n",
    "import ast \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "963f71e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lecture_dataset(base_url):\n",
    "    response = requests.get(base_url)\n",
    "    # Check if the request was successful\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Locate all <dd> elements containing a <strong> with class=\"label label-lecture\"\n",
    "    main_events = [\n",
    "        dd.find(\"p\") for dd in soup.find_all(\"dd\", class_=\"module-event main\")\n",
    "        if dd.find(\"strong\", class_=\"label label-lecture\") is not None\n",
    "    ]\n",
    "\n",
    "    for main in main_events:\n",
    "        if (main.next_sibling.name == 'dt') | (main.next_sibling.name == 'dd'):\n",
    "            main_events.remove(main)\n",
    "    main_events.pop(23)\n",
    "\n",
    "    additional = soup.find_all(\"span\")\n",
    "    # Prepare lists to store data\n",
    "    lecture_numbers = []\n",
    "    lecture_names = []\n",
    "    content_keywords = []\n",
    "    lecture_notes_urls = []\n",
    "\n",
    "    # Extract data from main events\n",
    "    for main in main_events:\n",
    "        # Lecture Number\n",
    "        lecture_number = main.find(\"strong\", class_=\"label label-lecture\").text.replace(\"LEC\", \"\").strip()\n",
    "        lecture_numbers.append(lecture_number)\n",
    "\n",
    "        # Lecture Name\n",
    "        lecture_name = main.find(\"a\").text.strip()\n",
    "        lecture_names.append(lecture_name)\n",
    "\n",
    "        # Lecture Notes URL\n",
    "        notes_path = main.find_all(\"a\")[1][\"href\"]\n",
    "        full_notes_url = base_url + notes_path\n",
    "        lecture_notes_urls.append(full_notes_url)\n",
    "    for add in additional:\n",
    "        keywords_text = add.text.replace(\"Keywords:\", \"\").strip()\n",
    "        content_keywords.append(keywords_text)\n",
    "    content_keywords.insert(13, \"None\")\n",
    "    content_keywords.append(\"None\")\n",
    "    content_keywords.append(\"None\")\n",
    "    # Create a DataFrame\n",
    "    data = {\n",
    "        \"Lecture Number\": lecture_numbers,\n",
    "        \"Lecture Name\": lecture_names,\n",
    "        \"Content Keywords\": content_keywords,\n",
    "        \"Lecture Notes URL\": lecture_notes_urls\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "    # Save to a CSV file\n",
    "#     output_file = \"lecture_notes_dataset.csv\"\n",
    "#     df.to_csv(output_file, index=False)\n",
    "#     print(f\"Dataset successfully saved to {output_file}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "723de70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to process table elements\n",
    "def process_table_to_string(table):\n",
    "    \"\"\"\n",
    "    Processes an HTML table element into a string representation.\n",
    "    Each row and cell is included, preserving the structure.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        cells = [cell.get_text(strip=True) for cell in row.find_all([\"th\", \"td\"])]\n",
    "        rows.append(cells)\n",
    "    return rows\n",
    "\n",
    "# Single lecture URL\n",
    "#lecture_url = \"https://dsc-courses.github.io/dsc10-2024-sp/resources/lectures/lec01/lec01.html\"\n",
    "\n",
    "def extract_lecture_content(lecture_url):\n",
    "    # Fetch the lecture page\n",
    "    response = requests.get(lecture_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container (assume it's the body for this example)\n",
    "        content_container = soup.body\n",
    "\n",
    "        # Initialize a list to preserve the order of elements\n",
    "        ordered_elements = []\n",
    "\n",
    "        # Loop through all children in the container\n",
    "        for child in content_container.descendants:\n",
    "            if (child.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"img\", \"p\", \"li\", \"div\", \"table\"]):\n",
    "                if child.name == \"img\" and child.has_attr(\"src\"):\n",
    "                    # For images, save the resolved absolute URL\n",
    "                    src = child[\"src\"]\n",
    "                    img_url = src if src.startswith(\"http\") else requests.compat.urljoin(lecture_url, src)\n",
    "                    ordered_elements.append({\"tag\": \"img\", \"content\": img_url})\n",
    "                elif child.name == \"div\":\n",
    "                    if child.find(\"pre\"):\n",
    "                        if 'jp-Cell-inputWrapper' in list(itertools.chain(child[\"class\"])):\n",
    "                            element = child.find(\"pre\").text.strip()\n",
    "                            ordered_elements.append({\"tag\": \"pre input\", \"content\": element})\n",
    "                        elif 'jp-Cell-outputWrapper' in list(itertools.chain(child[\"class\"])):\n",
    "                            element = child.find(\"pre\").text.strip()\n",
    "                            ordered_elements.append({\"tag\": \"pre output\", \"content\": element})\n",
    "                    else:\n",
    "                        continue\n",
    "                elif child.name == \"table\":\n",
    "                    # Process table into a structured representation\n",
    "                    table_content = process_table_to_string(child)\n",
    "                    ordered_elements.append({\"tag\": \"table\", \"content\": table_content})\n",
    "                else:\n",
    "                    # Clean text content: remove ¶ and strip whitespace\n",
    "                    clean_text = child.text.replace(\"¶\", \"\").strip()\n",
    "                    ordered_elements.append({\"tag\": child.name, \"content\": clean_text})\n",
    "\n",
    "        # Convert the ordered elements into a DataFrame\n",
    "        df = pd.DataFrame(ordered_elements)\n",
    "        df['linebreaks'] = df['content'].apply(num_linebreaks)\n",
    "        duplicates = []\n",
    "        for index, row in df.iterrows():\n",
    "            if row[\"tag\"] == \"li\" and row[\"linebreaks\"] > 0:\n",
    "                duplicates.extend(range(index + 1, index + 1 + row[\"linebreaks\"]))\n",
    "        return df.drop(duplicates)[['tag','content']]\n",
    "\n",
    "#         # Save the data to a CSV file\n",
    "#         output_file = \"lec01_ordered_contents.csv\"\n",
    "#         df.to_csv(output_file, index=False)\n",
    "\n",
    "#         print(f\"Lecture contents successfully saved to {output_file}!\")\n",
    "#     else:\n",
    "#         print(f\"Failed to fetch the page: {lecture_url}, status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeb04c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_linebreaks(content):\n",
    "    return content.count('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31f57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to process tables\n",
    "def process_table_to_HTML(content):\n",
    "    \"\"\"\n",
    "    Converts a list of lists (table data) into an HTML table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table_data = ast.literal_eval(content)  # Safely evaluate the string representation of lists\n",
    "        html_result = \"<table>\\n\"\n",
    "        for i, row in enumerate(table_data):\n",
    "            tag = \"th\" if i == 0 else \"td\"  # Use <th> for the first row (header), <td> for others\n",
    "            html_result += \"<tr>\\n\"\n",
    "            for cell in row:\n",
    "                html_result += f\"<{tag}>{escape(cell)}</{tag}>\\n\"\n",
    "            html_result += \"</tr>\\n\"\n",
    "        html_result += \"</table>\\n\"\n",
    "        return html_result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing table: {e}\")\n",
    "        return \"<p>Error rendering table</p>\\n\"\n",
    "\n",
    "# Helper function to process list items\n",
    "def process_list_items(content):\n",
    "    \"\"\"\n",
    "    Processes list items to handle nested bullets.\n",
    "    If `\\n` is present in the content, create nested bullets.\n",
    "    Otherwise, create simple default bullets.\n",
    "    \"\"\"\n",
    "    lines = content.split(\"\\n\")  # Split by newline for potential nested bullets\n",
    "    html_result = \"\"\n",
    "\n",
    "    if len(lines) == 1:\n",
    "        # Simple bullet (no nesting)\n",
    "        html_result += f\"<ul>\\n<li>{escape(lines[0].strip())}</li></ul>\\n\"\n",
    "    else:\n",
    "        # Nested bullets\n",
    "        html_result += f\"<ul>\\n<li>{escape(lines[0].strip())}\\n\"\n",
    "        html_result += \"<ul>\\n\"\n",
    "        for nested_line in lines[1:]:\n",
    "            html_result += f\"<li>{escape(nested_line.strip())}</li>\\n\"\n",
    "        html_result += \"</ul>\\n</li>\\n</ul>\\n\"\n",
    "\n",
    "    return html_result\n",
    "\n",
    "def generate_html(input_csv, output_html):\n",
    "    # Load the CSV into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Start building the HTML structure with CSS for better styling\n",
    "    html_content = \"\"\"<!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>Lecture 1 Content</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            line-height: 1.6;\n",
    "        }\n",
    "        img {\n",
    "            max-width: 70%;\n",
    "            height: auto;\n",
    "            display: block;\n",
    "            margin: 10px 0;\n",
    "        }\n",
    "        ul, ol {\n",
    "            padding-left: 20px;\n",
    "            margin-bottom: 1em;\n",
    "        }\n",
    "        ul ul, ol ol {\n",
    "            padding-left: 20px;\n",
    "        }\n",
    "        li {\n",
    "            margin-bottom: 0.5em;\n",
    "        }\n",
    "        .section {\n",
    "            margin-bottom: 20px;\n",
    "            padding: 10px;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        /* Styling for Jupyter Notebook-style code cells */\n",
    "        pre.code {\n",
    "            background-color: #f7f7f7;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 5px;\n",
    "            padding: 10px;\n",
    "            margin: 10px 0;\n",
    "            font-family: \"Courier New\", Courier, monospace;\n",
    "            font-size: 10px;\n",
    "            color: #333;\n",
    "            white-space: pre-wrap;\n",
    "        }\n",
    "        /* Styling for Jupyter Notebook-style output cells */\n",
    "        pre.output {\n",
    "            font-size: 10px;\n",
    "            color: #555;\n",
    "            white-space: pre-wrap;\n",
    "        }\n",
    "        /* Styling for tables */\n",
    "        table {\n",
    "            width: 50%;\n",
    "            border-collapse: collapse;\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "        th, td {\n",
    "            border: 1px solid #ddd;\n",
    "            padding: 8px;\n",
    "            text-align: left;\n",
    "        }\n",
    "        th {\n",
    "            background-color: #f2f2f2;\n",
    "        }\n",
    "    </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    \"\"\"\n",
    "\n",
    "    # Track whether a section is open\n",
    "    section_open = False\n",
    "\n",
    "    # Loop through the rows of the DataFrame and construct HTML elements\n",
    "    for _, row in df.iterrows():\n",
    "        tag = row['tag']\n",
    "        content = row['content']\n",
    "\n",
    "        # Close the previous section and start a new one for h1, h2, or h3 tags\n",
    "        if tag in {\"h1\", \"h2\", \"h3\"}:\n",
    "            if section_open:\n",
    "                html_content += \"</div>\\n\"  # Close the current section\n",
    "            html_content += f'<div class=\"section\">\\n<{tag}>{escape(content)}</{tag}>\\n'\n",
    "            section_open = True\n",
    "        else:\n",
    "            # Handle images\n",
    "            if tag == \"img\":\n",
    "                html_content += f'<img src=\"{escape(content)}\" alt=\"Image\">\\n'\n",
    "            # Handle code input\n",
    "            elif tag == \"pre input\":\n",
    "                html_content += f\"<pre class='code'>{escape(content)}</pre>\\n\"\n",
    "            # Handle code output\n",
    "            elif tag == \"pre output\":\n",
    "                html_content += f\"<pre class='output'>{escape(content)}</pre>\\n\"\n",
    "            # Handle unordered lists\n",
    "            elif tag == \"li\":\n",
    "                html_content += process_list_items(content)\n",
    "            # Handle tables\n",
    "            elif tag == \"table\":\n",
    "                html_content += process_table_to_HTML(content)\n",
    "            # Handle other tags\n",
    "            else:\n",
    "                clean_content = content.replace(\"¶\", \"\").strip()\n",
    "                html_content += f\"<{tag}>{escape(clean_content)}</{tag}>\\n\"\n",
    "\n",
    "    # Close the last open section, if any\n",
    "    if section_open:\n",
    "        html_content += \"</div>\\n\"\n",
    "\n",
    "    # Close the HTML structure\n",
    "    html_content += \"</body>\\n</html>\"\n",
    "    \n",
    "    filepath = f\"html_files/{output_html}\"\n",
    "    \n",
    "    # Write the HTML content to the output file using UTF-8 encoding\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML file successfully created at {filepath}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd1274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lecture_content_for_question_generation(input_html, output_csv):\n",
    "\n",
    "    # Read the HTML file\n",
    "    with open(input_html, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "    # Extract the lecture number and title from the first <h1> tag\n",
    "    h1_tag = soup.find(\"h1\")\n",
    "    if h1_tag:\n",
    "        lecture_header = h1_tag.text.strip()\n",
    "        # Extract lecture number and title from the <h1> content\n",
    "        if \"–\" in lecture_header:\n",
    "            lecture_number = lecture_header.split(\"–\")[0].replace(\"Lecture \", \"\").strip()\n",
    "            lecture_title = lecture_header.split(\"–\")[1].strip()\n",
    "        else:\n",
    "            lecture_number = \"\"\n",
    "            lecture_title = lecture_header\n",
    "    else:\n",
    "        lecture_number = \"\"\n",
    "        lecture_title = \"\"\n",
    "\n",
    "    # Extract sections based on <div class=\"section\">\n",
    "    sections = []\n",
    "    for section in soup.find_all(\"div\", class_=\"section\"):\n",
    "        # Extract the section name (from h2 or h3, not h1)\n",
    "        section_name_tag = section.find([\"h2\", \"h3\"])\n",
    "        section_name = section_name_tag.text.strip() if section_name_tag else \"\"\n",
    "\n",
    "        # Gather text content from the div, excluding images and section name\n",
    "        section_text = []\n",
    "        for tag in section.find_all():\n",
    "            if tag.name == \"pre\" and \"class\" in tag.attrs:\n",
    "                # Check for code or output classes\n",
    "                if \"code\" in tag[\"class\"]:\n",
    "                    section_text.append(f\"Input: {tag.text.strip()}\")\n",
    "                elif \"output\" in tag[\"class\"]:\n",
    "                    section_text.append(f\"Output: {tag.text.strip()}\")\n",
    "            elif tag not in {section_name_tag, h1_tag} and tag.name not in {\"img\", \"ul\"}:\n",
    "                # Exclude section name and h1 content, include other text\n",
    "                section_text.append(tag.text.strip())\n",
    "\n",
    "        # Combine the section text\n",
    "        combined_text = \" \".join(section_text).strip()\n",
    "\n",
    "        # Add the section details to the list if the section text is not empty\n",
    "        if combined_text:\n",
    "            sections.append({\n",
    "                \"lecture_number\": lecture_number,\n",
    "                \"lecture_title\": lecture_title,\n",
    "                \"section_name\": section_name,\n",
    "                \"section\": combined_text\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(sections)\n",
    "\n",
    "    filepath = f\"lecture_notes/{output_csv}\"\n",
    "        \n",
    "    # Save the dataset to a CSV file\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"Dataset successfully created and saved to {output_csv}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1cab9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Create a database of all lecture topics\n",
    "# URL of the website to scrape\n",
    "base_url = \"https://dsc-courses.github.io/dsc10-2024-sp/\"\n",
    "lectures = lecture_dataset(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35fcca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture contents successfully saved to web scraping/lec01_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec02_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec03_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec04_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec05_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec06_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec07_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec08_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec09_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec10_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec12_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec13_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec15_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec16_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec17_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec18_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec19_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec20_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec21_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec22_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec23_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec24_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec25_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec26_ordered_contents.csv!\n",
      "Lecture contents successfully saved to web scraping/lec28_ordered_contents.csv!\n"
     ]
    }
   ],
   "source": [
    "# Step 2. Scrape each individual lecture for key content\n",
    "lecture_content_output_csv = []\n",
    "for i in lectures['Lecture Notes URL']:\n",
    "    if i.split('/')[-2] not in {'lec11', 'lec14', 'lec27'}:\n",
    "        lecture_content = extract_lecture_content(i)\n",
    "        output_file = f\"{i.split('/')[-2]}_ordered_contents.csv\"\n",
    "        filepath = f\"web scraping/{output_file}\"\n",
    "        lecture_content.to_csv(filepath, index=False)\n",
    "        print(f\"Lecture contents successfully saved to {filepath}!\")\n",
    "        lecture_content_output_csv.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7a8aa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML file successfully created at html_files/lec01_contents.html!\n",
      "HTML file successfully created at html_files/lec03_contents.html!\n",
      "HTML file successfully created at html_files/lec04_contents.html!\n",
      "HTML file successfully created at html_files/lec06_contents.html!\n",
      "HTML file successfully created at html_files/lec07_contents.html!\n",
      "HTML file successfully created at html_files/lec08_contents.html!\n",
      "HTML file successfully created at html_files/lec12_contents.html!\n",
      "HTML file successfully created at html_files/lec15_contents.html!\n",
      "HTML file successfully created at html_files/lec16_contents.html!\n",
      "HTML file successfully created at html_files/lec17_contents.html!\n",
      "HTML file successfully created at html_files/lec18_contents.html!\n",
      "HTML file successfully created at html_files/lec19_contents.html!\n",
      "HTML file successfully created at html_files/lec20_contents.html!\n",
      "HTML file successfully created at html_files/lec22_contents.html!\n",
      "HTML file successfully created at html_files/lec23_contents.html!\n",
      "HTML file successfully created at html_files/lec24_contents.html!\n",
      "HTML file successfully created at html_files/lec25_contents.html!\n",
      "HTML file successfully created at html_files/lec26_contents.html!\n",
      "HTML file successfully created at html_files/lec28_contents.html!\n"
     ]
    }
   ],
   "source": [
    "# Step 3. Generate HTML code from key lecture content, grouped by h3 section\n",
    "html_files = []\n",
    "for csv in lecture_content_output_csv:\n",
    "    if csv not in {\"web scraping/lec02_ordered_contents.csv\", \"web scraping/lec05_ordered_contents.csv\",\"web scraping/lec09_ordered_contents.csv\", \"web scraping/lec10_ordered_contents.csv\", \"web scraping/lec13_ordered_contents.csv\", \"web scraping/lec21_ordered_contents.csv\"}:\n",
    "        output_file = f\"{csv.split('/')[-1].split('_')[0]}_contents.html\"\n",
    "        generate_html(csv, output_file)\n",
    "        filepath = f\"html_files/{output_file}\"\n",
    "        html_files.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57950bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully created and saved to lecture_notes/lec01_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec03_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec04_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec06_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec07_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec08_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec12_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec15_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec16_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec17_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec18_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec19_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec20_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec22_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec23_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec24_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec25_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec26_dataset.csv!\n",
      "Dataset successfully created and saved to lecture_notes/lec28_dataset.csv!\n"
     ]
    }
   ],
   "source": [
    "# Step 4. Create lecture notes for each lecture, grouped by topic\n",
    "lecture_notes = []\n",
    "for html in html_files:\n",
    "    output_csv = f\"{html.split('/')[-1].split('_')[0]}_dataset.csv\"\n",
    "    filepath = f\"lecture_notes/{output_csv}\"\n",
    "    lecture_content_for_question_generation(html, filepath)\n",
    "    lecture_notes.append(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

,Quarter,Year,Exam_type,Quiz_type,Problem_title,Problem_subtitle,Question,Answer,Average,Difficulty_score
0,Fa,23,Quiz,1,Problem 1,Problem 1,"After a trip to the zoo, Anthony wrote the following block of
code.
zebra = 5
lion = 4
cow = 1
zebra = zebra * 2
lion = abs(cow - lion)
zebra = zebra + lion ** 2
cow = (zebra + lion) / 2 * lion
After running the above block of code, what is the value of
cow?",33.0,60.0,Medium
1,Fa,23,Quiz,1,Problem 2,Problem 2,"Consider the following four assignment statements.
bass = ""5""
tuna = 2
sword = [""4.0"", 5, 12.5, -10, ""2023""]
gold = [4, ""6"", ""CSE"", ""doc""]",,,
2,Fa,23,Quiz,1,Problem 2,Problem 2.1,What is the value of the expression bass * tuna?,"""55""",48.0,Hard
3,Fa,23,Quiz,1,Problem 2,Problem 2.2,"Which of the following expressions results in an error?

 int(sword[0])
 float(sword[1])
 int(sword[2])
 int(sword[3])
 float(sword[4])",int(sword[0]),51.0,Medium
4,Fa,23,Quiz,1,Problem 2,Problem 2.3,"Which of the following expressions evaluates to
""DSC10""?

 gold[3].replace(""o"", ""s"").title() + str(gold[0] + gold[1])
 gold[3].replace(""o"", ""s"").upper() + str(gold[0] + int(gold[1]))
 gold[3].replace(""o"", ""s"").upper() + str(gold[1] + int(gold[0]))
 gold[3].replace(""o"", ""s"").title() + str(gold[0] + int(gold[1]))","gold[3].replace(""o"", ""s"").upper() + str(gold[0] + int(gold[1]))",92.0,Easy
5,Fa,23,Quiz,1,Problem 3,Problem 3,"Consider the following assignment statement.
puffin = np.array([5, 9, 13, 17, 21])",,,
6,Fa,23,Quiz,1,Problem 3,Problem 3.1,"Provide arguments to call np.arange with so that the
array penguin is identical to the array
puffin.
penguin = np.arange(____)","We need to provide np.arange
with three arguments: 5, anything in (21,
25], 4. For instance, something line
penguin = np.arange(5, 25, 4) would work.",90.0,Easy
7,Fa,23,Quiz,1,Problem 3,Problem 3.2,"Fill in the blanks so that the array parrot is also
identical to the array puffin.
Hint: Start by choosing y so that
parrot has length 5.
parrot = __(x)__ * np.arange(0, __(y)__, 2) + __(z)__","x: 2
y: anything in (8,
10]
z: 5",74.0,Medium
8,Fa,23,Quiz,1,Problem 4,Problem 4,"Suppose students is a DataFrame of all students who took
DSC 10 last quarter. students has one row per student,
where:

The index contains students’ PIDs as strings starting with
""A"".
The ""Overall"" column contains students’ overall
percentage grades as floats.
The ""Animal"" column contains students’ favorite
animals as strings.",,,
9,Fa,23,Quiz,1,Problem 4,Problem 4.1,"What type is students.get(""Overall"")? If this expression
errors, select “this errors.""

 float
 string
 array
 Series
 this errors",Series,73.0,Medium
10,Fa,23,Quiz,1,Problem 4,Problem 4.2,"What type is students.get(""PID"")? If this expression
errors, select “this errors.""

 float
 string
 array
 Series
 this errors",this errors,67.0,Medium
11,Fa,23,Quiz,1,Problem 4,Problem 4.3,"Supposing that students is already sorted by
""Overall"" in descending order, fill in the
blanks so that animal_one and animal_two
both evaluate to ""giraffe"".
animal_one = students.get(__(x)__).loc[__(y)__]
animal_two = students.get(__(x)__).iloc[__(z)__]","x: ""Animal""
y: ""A12345678""
z: 5",69.0,Medium
12,Fa,23,Quiz,1,Problem 4,Problem 4.4,"If students wasn’t already sorted by
""Overall"" in descending order, which of your answers would
need to change?

 Neither y nor z would need to change
 Both y and z would need to change
 y only
 z only",z only,82.0,Easy
13,Fa,23,Quiz,2,Problem 1,Problem 1,"Fill in the blanks so that count_1 and
count_2 both evaluate to the number of items in
items with a ""Cost"" of 0.
count_1 = items.groupby(__(a)__).__(b)__().get(""Item"").loc[__(c)__]
count_2 = items[__(d)__].shape[0]","a: ""Cost""
b: count
c: 0
d: items.get(""Cost"") == 0",81.0,Easy
14,Fa,23,Quiz,2,Problem 2,Problem 2,"The DataFrame keepers has 5 rows, each of which
represent a different shopkeeper in the Animal Crossing: New
Horizons universe.
keepers is shown below in its entirety.


How many rows are in the following DataFrame? Give your answer as an
integer.
keepers.merge(items.take(np.arange(6)), 
              left_on=""Store"", 
              right_on=""Location"")",10,54.0,Medium
15,Fa,23,Quiz,2,Problem 3,Problem 3,"Which type of plot should we use to visualize the distribution of the
""Location"" column in the items DataFrame?

 Scatter plot
 Line plot
 Bar chart
 Histogram",Bar chart,74.0,Medium
16,Fa,23,Quiz,2,Problem 4,Problem 4,"Nintendo collected data on the heights of a sample of Animal
Crossing: New Horizons players. A histogram of the heights in their
sample is given below.

What percentage of players in Nintendo’s sample are
at least 62.5 inches tall? Give your answer as an integer rounded to the
nearest multiple of 5.",80%,73.0,Medium
17,Fa,23,Quiz,2,Problem 5,Problem 5,"Consider the function tom_nook, defined below. Recall
that if x is an integer, x % 2 is
0 if x is even and 1 if
x is odd.
def tom_nook(crossing):
    bells = 0
    for nook in np.arange(crossing):
        if nook % 2 == 0:
            bells = bells + 1
        else:
            bells = bells - 2
    return bells
What value does tom_nook(8) evaluate to?

 -6
 -4
 -2
 0
 2
 4
 6",-4,79.0,Easy
18,Fa,23,Quiz,3,Problem 1,Problem 1,"Among all Costco members in San Diego, the average monthly spending
in October 2023 was $350 with a standard deviation of
$40.",,,
19,Fa,23,Quiz,3,Problem 1,Problem 1.1,"The amount Ciro spent at Costco in October 2023 was
-1.5 in standard units. What is this amount in dollars?
Give your answer as an integer.",290,93.0,Easy
20,Fa,23,Quiz,3,Problem 1,Problem 1.2,"What is the minimum possible percentage of San Diego members that
spent between $250 and $450 in October
2023?

 16%
 22%
 36%
 60%
 78%
 84%",84%,61.0,Medium
21,Fa,23,Quiz,3,Problem 1,Problem 1.3,"Now, suppose we’re given that the distribution of monthly spending in
October 2023 for all San Diego members is roughly normal. Given this
fact, fill in the blanks:

""In October 2023, 95% of San Diego members spent
between __(m)__ dollars and __(n)__ dollars.""


What are m and n? Give your answers as integers rounded to the
nearest multiple of 10.","m: 270, n: 430",81.0,Easy
22,Fa,23,Quiz,3,Problem 2,Problem 2,"Suppose we have access to a simple random sample of all US Costco
members of size 145. Our sample is stored in a
DataFrame named us_sample, in which the
""Spend"" column contains the October 2023 spending of each
sampled member in dollars.",,,
23,Fa,23,Quiz,3,Problem 2,Problem 2.1,"Fill in the blanks:

""The average of the ""Spend"" column in us_sample is a ____,
while the average October 2023 spending of all US members is a ____.""



 sample statistic; population parameter
 population statistic; sample parameter
 sample parameter; population statistic
 population parameter; sample statistic",sample statistic; population parameter,94.0,Easy
24,Fa,23,Quiz,3,Problem 2,Problem 2.2,"Fill in the blanks below so that us_left and
us_right are the left and right endpoints of a
46% confidence interval for the average October 2023
spending of all US members.
costco_means = np.array([])
for i in np.arange(5000):
    resampled_spends = __(x)__
    costco_means = np.append(costco_means, resampled_spends.mean())
left = np.percentile(costco_means, __(y)__)
right = np.percentile(costco_means, __(z)__)
Which of the following could go in blank (x)? Select all that
apply.

 us_sample.sample(145, replace=True).get(""Spend"")
 us_sample.sample(145, replace=False).get(""Spend"")
 np.random.choice(us_sample.get(""Spend""), 145)
 np.random.choice(us_sample.get(""Spend""), 145, replace=True)
 np.random.choice(us_sample.get(""Spend""), 145, replace=False)
 None of the above.

What goes in blanks (y) and (z)? Give your answers as integers.","x:

us_sample.sample(145, replace=True).get(""Spend"")
np.random.choice(us_sample.get(""Spend""), 145)
np.random.choice(us_sample.get(""Spend""), 145, replace=True)

y: 27
z: 73",79.0,Easy
25,Fa,23,Quiz,3,Problem 2,Problem 2.3,"True or False: 46% of all US members in
us_sample spent between left and
right in October 2023.

 True
 False",False,85.0,Easy
26,Fa,23,Quiz,3,Problem 2,Problem 2.4,"True or False: If we repeat the code from part (b) 200 times, each
time bootstrapping from a new random sample of 145 members drawn from
all US members, then about 92 of the intervals we
create will contain the average October 2023 spending of all US
members.

 True
 False",True,51.0,Medium
27,Fa,23,Quiz,3,Problem 2,Problem 2.5,"True or False: If we repeat the code from part (b) 200 times, each
time bootstrapping from us_sample, then about
92 of the intervals we create will contain the average
October 2023 spending of all US members.

 True
 False",False,30.0,Hard
28,Fa,23,Quiz,4,Problem 1,Problem 1,"You survey 100 DSC majors and 140 CSE majors to ask them which video
streaming service they use most. The resulting distributions are given
in the table below. Note that each column sums to 1.",,,
29,Fa,23,Quiz,4,Problem 1,Problem 1.1,"What is the total variation distance (TVD) between the distribution
for DSC majors and the distribution for CSE majors? Give your answer as
an exact decimal.",0.15,89.0,Easy
30,Fa,23,Quiz,4,Problem 1,Problem 1.2,"Suppose we only break down video streaming services into four
categories: Netflix, Hulu, Disney+, and Other (which now includes Amazon
Prime Video). Now we recalculate the TVD between the two distributions.
How does the TVD now compare to your answer to part (a)?

 less than (a)
 equal to (a)
 greater than (a)",less than (a),93.0,Easy
31,Fa,23,Quiz,4,Problem 2,Problem 2,"You want to estimate the proportion of DSC majors who have a Netflix
subscription. To do so, you will survey a random sample of DSC majors
and ask them whether they have a Netflix subscription. You will then
create a 95% confidence interval for the proportion of “yes"" answers in
the population, based on the responses in your sample. You decide that
your confidence interval should have a width of at most 0.10.",,,
32,Fa,23,Quiz,4,Problem 2,Problem 2.1,"In order for your confidence interval to have a width of at most
0.10, the standard deviation of the distribution of the sample
proportion must be at most T. What is
T? Give your answer as an exact
decimal.",0.025,46.0,Hard
33,Fa,23,Quiz,4,Problem 2,Problem 2.2,"Using the fact that the standard deviation of any dataset of 0s and
1s is no more than 0.5, calculate the minimum number of people you would
need to survey so that the width of your confidence interval is at most
0.10. Give your answer as an integer.",400,81.0,Easy
34,Fa,23,Quiz,4,Problem 3,Problem 3,"Arya was curious how many UCSD students used Hulu over Thanksgiving
break. He surveys 250 students and finds that 130 of them did use Hulu
over break and 120 did not.
Using this data, Arya decides to test following hypotheses:

Null Hypothesis: Over Thanksgiving break, an
equal number of UCSD students did use Hulu and did not use
Hulu.
Alternative Hypothesis: Over Thanksgiving break,
more UCSD students did use Hulu than did not use
Hulu.",,,
35,Fa,23,Quiz,4,Problem 3,Problem 3.1,"Which of the following could be used as a test statistic for the
hypothesis test?

 The proportion of students who did use Hulu minus the proportion of
students who did not use Hulu.
 The absolute value of the proportion of students who did use Hulu
minus the proportion of students who did not use Hulu.
 The proportion of students who did use Hulu plus the proportion of
students who did not use Hulu.
 The absolute value of the proportion of students who did use Hulu
plus the proportion of students who did not use Hulu.","The proportion of students who did use Hulu
minus the proportion of students who did not use Hulu.",81.0,Easy
36,Fa,23,Quiz,4,Problem 3,Problem 3.2,"For the test statistic that you chose in part (a), what is the
observed value of the statistic? Give your answer either as an exact
decimal or a simplified fraction.",0.04,90.0,Easy
37,Fa,23,Quiz,4,Problem 3,Problem 3.3,"If the p-value of the hypothesis test is 0.053, what can we conclude,
at the standard 0.05 significance level?

 We reject the null hypothesis.
 We fail to reject the null hypothesis.
 We accept the null hypothesis.",We fail to reject the null hypothesis.,87.0,Easy
38,Fa,24,Quiz,1,Problem 1,Problem 1,"What does the following expression evaluate to? Write your answer
exactly how the output would appear in Python.
5 * (4 ** 3 - 40) / (2 * 8) ** .5 + 10 / 3 + 2 * (5 / 6 - 1 / 2)",34.0,61.0,Medium
39,Fa,24,Quiz,1,Problem 2,Problem 2,"Select all the answer choices below that correspond to valid
assignment statements. For the selected choice(s) only, write the value
of the variable.

 ""seal"" = abs(-7) + round(23.442, 1)
 whale = max(round(17.82), 5, -20)
 shark = abs(max(4, 3, min(10, 5.5, 11)), 100, 30)
 squid = ""error"" + ""?"" * 4","Option 2: 18
Option 4: ""error????""",78.0,Easy
40,Fa,24,Quiz,1,Problem 3,Problem 3,"Suppose that weights is an array containing the weights,
in kilograms, of several leopard sharks living in La Jolla Cove. Several
leopard sharks represented in weights weigh more than 10
kilograms.
Suppose that we have imported a module called sharkpy by
running the code import sharkpy. The sharkpy
module includes a function heavy that takes as input an
array of shark weights and returns a smaller array containing only the
weights that are above 10 kilograms.
Using the heavy function and an array method of
your choice, write an expression that evaluates to the weight,
in kilograms, of the lightest leopard shark in weights that
weighs more than 10 kilograms.",sharkpy.heavy(weights).min(),59.0,Medium
41,Fa,24,Quiz,1,Problem 4,Problem 4,"Suppose we run the following code.
creatures = ""crab, eel, clam""
oceans = ""Pacific, Atlantic""
creatures = creatures.title()
oceans = oceans.upper()
creatures = creatures.replace(""clam"", ""lobster"")
What does the expression creatures + "", "" + oceans
evaluate to? Write your answer exactly how the output
would appear in Python.","""Crab, Eel, Clam, PACIFIC, ATLANTIC""",66.0,Medium
42,Fa,24,Quiz,1,Problem 5,Problem 5,"Define narwhal and jellyfish as
follows.
narwhal = np.arange(6, 12, 3)
jellyfish = np.arange(12, 6, -3)",,,
43,Fa,24,Quiz,1,Problem 5,Problem 5.1,"True or False: narwhal.sum() and
jellyfish.sum() evaluate to the same value.

 True
 False",False,86.0,Easy
44,Fa,24,Quiz,1,Problem 5,Problem 5.2,"True or False: (narwhal+jellyfish)[0] and
(narwhal+jellyfish)[1] evaluate to the same value.

 True
 False",True,92.0,Easy
45,Fa,24,Quiz,1,Problem 6,Problem 6,"The DataFrame finding_nemo contains information about
characters in the movie Finding Nemo. Each row represents a
character, and the columns include:

""Name"" (str): the unique name of the character (ex.
""Nemo"", ""Dory"")
""Species"" (str): the species of the character (ex.
""clownfish"", ""blue tang"")

There is one character in finding_nemo named
""Bruce"". Which of the following lines of code evaluates to
Bruce’s species?

 finding_nemo.get(""Species"").loc[""Bruce""]
 finding_nemo.get(""Species"").iloc[""Bruce""]
 finding_nemo.get(""Species"").loc[""Bruce""].iloc[0]
 finding_nemo.set_index(""Name"").get(""Species"").loc[""Bruce""]
 finding_nemo.set_index(""Name"").get(""Species"").iloc[""Bruce""]
 finding_nemo.set_index(""Name"").get(""Species"").loc[""Bruce""].iloc[0]","finding_nemo.set_index(""Name"").get(""Species"").loc[""Bruce""]",80.0,Easy
46,Fa,24,Quiz,2,Problem 1,Problem 1,"Select the correct way to fill in the blank such that the code below
evaluates to a DataFrame with a single row for each UC campus, where the
""Degrees"" column contains the average number of degrees
awarded by that campus per academic year (over the past six years).
uc.groupby(______).mean()

 ""Year""
 [""Campus"", ""Year""]
 ""Campus""
 [""Campus"", ""Degrees""]
 ""Degrees""
 [""Year"", ""Degrees""]","""Campus""",40.0,Hard
47,Fa,24,Quiz,2,Problem 2,Problem 2,"Fill in the blank so that the expression below evaluates to the
number of years (out of the past six years) in which UC San Diego
awarded more than 12,000 degrees.
uc[______].shape[0]","(uc.get(""Campus"") == ""San Diego"") & (uc.get(""Degrees"") > 12000)",61.0,Medium
48,Fa,24,Quiz,2,Problem 3,Problem 3,"Suppose we plot a density histogram showing the distribution of the
""Degrees"" column in uc, using the argument
bins=np.arange(0, uc.get(""Degrees"").max() + 3000, 2000)
.
Suppose there are 12 rows of uc corresponding to
campuses and years in which fewer than 2000 degrees were awarded. How
tall will the first bar in the histogram be? Give your answer as an
exact decimal or simplified fraction.",0.0001,33.0,Hard
49,Fa,24,Quiz,2,Problem 4,Problem 4,,,,
50,Fa,24,Quiz,2,Problem 4,Problem 4.1,"Typically, graduation ceremonies happen only at the end of an
academic year. For example, students who earn a degree in the 2024-2025
academic year celebrate with a graduation ceremony in 2025.
The function ceremony_year takes as input a string
formatted like those in the ""Year"" column of , and
returns an int corresponding to the year of the
graduation ceremony for that academic year. For example,
ceremony_year(""2024-2025"") should return 2025.
Fill in the return statement of this function below.","def ceremony_year(academic_year):
                  return int(academic_year.split(""-"")[1])",70.0,Medium
51,Fa,24,Quiz,2,Problem 4,Problem 4.2,"What does the following expression evaluate to? Write your answer
exactly how the output would appear in Python.
uc.get(""Year"").apply(ceremony_year).min()",2019,87.0,Easy
52,Fa,24,Quiz,2,Problem 5,Problem 5,"The DataFrame mascots has 10 rows, one for each UC
school, and only two columns, ""school"" and
""mascot"". Assuming that the entries in the
""school"" column are formatted exactly how they appear in
the ""Campus"" column of uc, how many rows does
the following merged DataFrame have?
uc.merge(mascots, left_on=""Campus"", right_on=""school"")

 10
 60
 70
 600
 None of these.",60,65.0,Medium
53,Fa,24,Quiz,2,Problem 6,Problem 6,"Define the function puzzle as below.
def puzzle(word):
    if not(len(word) > 2):
        return False
    elif word in ""UC San Diego"": 
        return True
    else:
        return not(""UC"" in word)  
Select all the function calls that evaluate to True.

 puzzle(""UC San"")
 puzzle(""UC"")
 puzzle(""UCB"")
 puzzle(""HDSI"")","Option 1: puzzle(""UC San"")
Option 4: puzzle(""HDSI"")",74.0,Medium
54,Fa,24,Quiz,3,Problem 1,Problem 1,"The DataFrame space_reptiles contains 1000 rows of
information about all space reptiles living on
Statistica, which we’ll think of as a population. For each reptile, we
have its ""length"" in meters, ""age"" in years,
and ""number_of_eyes"". The first five rows of
space_reptiles are shown below.",,,
55,Fa,24,Quiz,3,Problem 1,Problem 1.1,"Fill in the blanks in the sample_of_reptiles function.
The function has two parameters, ""sample_size"" (int), which
will be a positive integer, and ""column"" (str), which will
be the name of one of the columns in space_reptiles. The
function should take a sample of reptiles from
space_reptiles, with replacement, of the
specified size, and return the average value in the given column for the
sample.
def sample_of_reptiles(sample_size, column):
    return space_reptiles.sample(__(x)__).__(y)__    
x:
y:","x: sample_size, replace=True
y: get(column).mean()",85.0,Easy
56,Fa,24,Quiz,3,Problem 1,Problem 1.2,"True or False: The function call
sample_of_reptiles(1000, ""length"") is an example of
bootstrapping.",False,77.0,Easy
57,Fa,24,Quiz,3,Problem 1,Problem 1.3,"Calculate the variance of the data in the first five
rows of the ""number_of_eyes"" column of
space_reptiles: 2, 4, 6, 8, 10. Give your answer as an
integer.",8,50.0,Medium
58,Fa,24,Quiz,3,Problem 1,Problem 1.4,"Suppose the next row in the ""number_of_eyes"" column
contains 6. If we add this value to our dataset and then recompute the
variance, it would...

 decrease because the new value is less than the greatest value
 decrease because the new value is equal to the mean.
 remain the same because the new value is equal to the median.
 increase because the data set has more values than it did
 increase because the new value is a positive number.","decrease because the new value is equal to
the mean.",62.0,Medium
59,Fa,24,Quiz,3,Problem 2,Problem 2,"Statistica’s forests are filled with tall creatures called
whingdingdillies. You have a large random sample of 400
whingdingdillies. In this sample, the mean height is 30m and the
standard deviation is 4m. Suppose that whingdingdilly heights are
normally distributed.",,,
60,Fa,24,Quiz,3,Problem 2,Problem 2.1,"What are the endpoints of a CLT-based 95% confidence interval for the
mean height of whingdingdillies? Each value should be a single
number.","left endpoint = 29.6, right endpoint =
30.4",50.0,Medium
61,Fa,24,Quiz,3,Problem 2,Problem 2.2,"Determine the values of the variables v and
w in the code below so that wdd_prop evaluates
to the approximate proportion of whingdingdillies with heights between
30m and 33m. Each value should be a single number.
wdd_prop = stats.norm.cdf(v) - stats.norm.cdf(w)","v = .75, w = 0",52.0,Medium
62,Fa,24,Quiz,3,Problem 2,Problem 2.3,"Above, we stated an assumption that whingdingdilly heights are
normally distributed. For which part(s) of this question did we
need that assumption?

 2.1 only
 2.2 only
 both 2.1 and 2.2
 neither 2.1 nor 2.2",2.2 only,57.0,Medium
63,Fa,24,Quiz,3,Problem 2,Problem 2.4,"After a frightening encounter, you discover that whingdingdillies can
run very fast. You collect a sample of 400 whingdingdilly speeds, then
use this sample to generate a bootstrapped distribution of resample mean
speeds. Afterwards, you wonder how your bootstrapped distribution would
have looked if you had instead been able to collect a random sample of
size 900. Which of the following overlaid histograms shows two
bootstrapped distributions of resample mean speeds, based on samples of
size 400 and 900?",Option A,59.0,Medium
64,Wi,24,Quiz,1,Problem 1,Problem 1,"Select all the true statements below.

 Mixing ints and floats in an arithmetic
expression will always result in a float.
 Dividing two ints will sometimes result in an
int.
 Any float can be converted to a string using the
function str().
 Any string can be converted to a float using the
function float().",Option 1 and Option 3,87.0,Easy
65,Wi,24,Quiz,1,Problem 2,Problem 2,"Consider the assignment statement below.
pear = [6, 10.1, ""pear"", 13]
What does the expression np.array(pear) evaluate to?

 array([6, 10.1, ""pear"", 13])
 array([6, 10.1, pear, 13])
 array([""6"", ""10.1"", ""pear"", ""13""])
 array([""pear""])
 array([pear])
 This expression errors","array([""6"", ""10.1"", ""pear"", ""13""])",50.0,Medium
66,Wi,24,Quiz,1,Problem 3,Problem 3,"Suppose x and y are both ints
that have been previously defined, with x < y. Now,
define:
peach = np.arange(x, y, 2)
Say that the spread of peach is the difference
between the largest and smallest values in peach. The
spread should be a non-negative integer.",,,
67,Wi,24,Quiz,1,Problem 3,Problem 3.1,"Using array methods, write an expression that
evaluates to the spread of peach.",peach.max() - peach.min(),62.0,Medium
68,Wi,24,Quiz,1,Problem 3,Problem 3.2,"Without using any methods or functions, write an
expression that evaluates to the spread of peach.
Hint: Use [ ].","peach[len(peach) - 1] - peach[0] or
peach[-1] - peach[0]",36.0,Hard
69,Wi,24,Quiz,1,Problem 3,Problem 3.3,"Choose the correct way to fill in the blank in this sentence:

The spread of peach is ______ the
value of y - x.


 always less than
 sometimes less than and sometimes equal to
 always greater than
 sometimes greater than and sometimes equal to
 always equal to",always less than,48.0,Hard
70,Wi,24,Quiz,1,Problem 4,Problem 4,"Suppose fruits is a DataFrame of the fruits Ashley
bought at the grocery store, where:

The ""fruit"" column contains the name of the fruit,
as a string. All values in this column are distinct.
The ""price"" column contains the amount in dollars
spent on the fruit, as a float.
The ""pounds"" column contains the number of
pounds purchased, as an int.",,,
71,Wi,24,Quiz,1,Problem 4,Problem 4.1,"Fill in the blanks below to add a new column to fruits
called ""price_per_ounce"" that contains the price per ounce
of each of the fruits in fruits. There are 16
ounces in a pound.
    fruits = fruits.__(x)__(price_per_ounce = __(y)__ / __(z)__)",,71.0,Medium
72,Wi,24,Quiz,1,Problem 4,Problem 4.2,"Write a line of code that evaluates to the amount of money, in
dollars, that Ashley spent on fruit at the grocery store.","fruits.get(""price"").sum() or
sum(fruits.get(""price""))",62.0,Medium
73,Wi,24,Quiz,1,Problem 4,Problem 4.3,"Fill in the blanks so that the expression below evaluates to the name
of the fruit with the highest price per ounce.
    (fruits.sort_values(by = ""price_per_ounce"", ascending = __(x)__)
           .get(__(y)__).iloc[0])",,87.0,Easy
74,Wi,24,Quiz,1,Problem 4,Problem 4.4,"Assuming that ""mango"" is one of the fruits Ashley
bought, fill in the blanks so that the expression below evaluates to the
price per ounce of ""mango"".
    fruits.__(x)__(""fruit"").get(""price_per_ounce"").__(y)__[""mango""]",,41.0,Hard
75,Wi,24,Quiz,2,Problem 1,Problem 1,"We’d like to visualize the distribution of the ""Mfr""
column in the laptops DataFrame. Fill in the blanks so that
the code below draws an appropriate plot.
laptops.groupby(__(a)__).__(b)__.get([__(c)__]).plot(kind=__(d)__)","Answer (a): ""Mfr""",92.0,Easy
76,Wi,24,Quiz,2,Problem 2,Problem 2,"Fill in the blanks so that rotten_apple evaluates to the
number of laptops manufactured by ""Apple"" that are priced
below the median price of all laptops.
x = __(a)__
y = __(b)__
rotten_apple = laptops[x __(c)__ y].__(d)__",,71.0,Medium
77,Wi,24,Quiz,2,Problem 3,Problem 3,"Select all the true statements below.

 When you set the index of a DataFrame, the specified column becomes
the index, and the old index becomes a column.
 After grouping a DataFrame, the resulting number of rows is less than
or equal to the original number of rows.
 The area of a density histogram is always equal to 1.
 A scatter plot has one numerical axis and one categorical axis.
 The height of the tallest bar in a density histogram is always less
than 1.",Option 2 and Option 3,84.0,Easy
78,Wi,24,Quiz,2,Problem 4,Problem 4,"A density histogram of ""Screen Size"" is shown below.

What percentage of computer models have a
""Screen Size"" of at least 16 inches but less than 18
inches? Give your answer as an integer rounded to the nearest
multiple of 5.",,72.0,Medium
79,Wi,24,Quiz,2,Problem 5,Problem 5,,,,
80,Wi,24,Quiz,2,Problem 5,Problem 5.1,"Using groupby, write an expression that
evaluates to the average price of laptops with the ""macOS""
operating system.","laptops.groupby(""OS"").mean().get(""Price"").loc[""macOS""]",72.0,Medium
81,Wi,24,Quiz,2,Problem 5,Problem 5.2,"Without using groupby, write an
expression that evaluates to the average price of laptops with the
""macOS"" operating system (the same quantity as above).","laptops[laptops.get(""OS"") == ""macOS""].get(""Price"").mean()",71.0,Medium
82,Wi,24,Quiz,3,Problem 1,Problem 1,"Suppose the function first_two takes as input a string
and returns a string with the first two characters only. For example
first_two(""panda"") is ""pa"". Using this
function, write an expression that evaluates to a Series containing the
two-character airline designator for each flight in
flights.","flights.get(""flight_num"").apply(first_two)",78.0,Easy
83,Wi,24,Quiz,3,Problem 2,Problem 2,,,,
84,Wi,24,Quiz,3,Problem 2,Problem 2.1,"Fill in the blanks below so that grouped is a DataFrame
showing how many flights on each airline departed from each airport.
    grouped = flights.groupby(___(x)___).___(y)___.get([""flight_num""])",,82.0,Easy
85,Wi,24,Quiz,3,Problem 2,Problem 2.2,"Suppose the expression
grouped.shape[0] == flights.shape[0] evaluates to
True. Select all true statements below.

 No two flights in flights were on the same airline.
 No two flights in flights had the same departure and
arrival airports.
 Among all flights in flights that were on the same
airline, no two had the same departure airport.
 Among all flights in flights from the same departure
airport, no two were on the same airline.",Option 3 and Option 4,80.0,Easy
86,Wi,24,Quiz,3,Problem 3,Problem 3,"Suppose we have another DataFrame more_flights which
contains the same columns as flights, but different rows.
Define merged as follows.
        merged = flights.merge(more_flights, on = ""airline"")
Suppose that in merged, there are 108 flights where the
airline is ""United"", and in more_flights,
there are 12 flights where the airline is ""United"". If
flights has 15 rows in total, how many of these rows are
not for ""United"" flights? Give your answer
as an integer.",6,64.0,Medium
87,Wi,24,Quiz,3,Problem 4,Problem 4,,,,
88,Wi,24,Quiz,3,Problem 4,Problem 4.1,"Fill in the blanks below so that prob evaluates to the
probability that a randomly selected flight from the
flights DataFrame arrives at ""SAN"" given that
it departs from ""LAX"".
san = flights.get(""arrival"") == ""SAN""
lax = flights.get(""departure"") == ""LAX""
prob = flights[san___(x)___lax].shape[0] / ___(y)___",,66.0,Medium
89,Wi,24,Quiz,3,Problem 4,Problem 4.2,"Write an expression that evaluates to the probability that a randomly
selected flight from the flights DataFrame is
not on the airline ""Delta"".","1 - flights[flights.get(""airline"") == ""Delta""].shape[0] / flights.shape[0]
or
flights[flights.get(""airline"") != ""Delta""].shape[0] / flights.shape[0]
or (flights.get(""airline"") != ""Delta"").mean()",,
90,Wi,24,Quiz,3,Problem 5,Problem 5,"Use the function defined below to answer the questions on the
right.
def discount(duration, price):
    if duration > 6:
        price = price * 0.9
    if duration > 4:
        price = price * 0.95 
    elif duration > 3:
        price = price * 0.98
    else:
        price = price * 0.99 
    price = price - 5 
    return price",,,
91,Wi,24,Quiz,3,Problem 5,Problem 5.1,"What is the output of discount(7, 100)?

 100 * 0.9 * 0.95 * 0.98 - 5
 100 * 0.9 * 0.95 - 5
 100 * 0.9 - 5
 100 * 0.9",Option 2,72.0,Medium
92,Wi,24,Quiz,3,Problem 5,Problem 5.2,"Find x and y such that
discount(x, y) evaluates to 97 * 0.98 - 5.",,95.0,Easy
93,Wi,24,Quiz,4,Problem 1,Problem 1,"Suppose we’ve imported the scipy module. To the nearest
0.5, what does the following expression evaluate to?
scipy.stats.norm.cdf(-2) * 100",2.5,57.0,Medium
94,Wi,24,Quiz,4,Problem 2,Problem 2,"Select all the true statements below.

 The average of the deviations from the mean is a meaningful measure
of the spread of the data.
 It is possible for the standard deviation of a dataset to equal
zero.
 It is possible for the standard deviation of a dataset to be
negative.
 Given the standard deviation of a dataset, we can determine its
mean.
 Given the standard deviation of a dataset, we can determine its
variance.",Option 2 and Option 5,80.0,Easy
95,Wi,24,Quiz,4,Problem 3,Problem 3,"The Oscars, or Academy Awards, are the highest awards in the film
industry, awarded each year to the best movies of that year. The
oscars DataFrame contains a row for each movie that has
ever been nominated for an Oscar. The ""name"" column
contains the name of the movie and the ""rating"" column
contains a rating of the movie on a 0 to 100 scale. This number
incorporates many factors, but we won’t worry about how it is
computed.",,,
96,Wi,24,Quiz,4,Problem 3,Problem 3.1,"Fill in the blanks below to collect a simple random
sample of 400 movies from the oscars DataFrame,
then calculate 10,000 bootstrapped sample mean ratings.
my_sample = __(x)__
n_resamples = 10000
boot_means = np.array([])
for i in np.arange(n_resamples):
    resample = __(y)__
    mean = __(z)__
    boot_means = np.append(boot_means, mean)",,85.0,Easy
97,Wi,24,Quiz,4,Problem 3,Problem 3.2,"In each blank, circle the word that correctly fills in the
sentence.

A histogram of boot_means shows a(n)
probability / empirical distribution
of a statistic / parameter.","empirical, statistic",77.0,Easy
98,Wi,24,Quiz,4,Problem 3,Problem 3.3,"Suppose we use the array boot_means to calculate a 90%
confidence interval for the mean rating of Oscar-nominated movies.
Select all correct conclusions we can draw about this
interval.

 There is a 90% chance that the true mean rating of all
Oscar-nominated movies falls within this interval.
 The sample mean rating is within 90% of the true mean rating of all
Oscar-nominated movies.
 If we looked at the ratings of many Oscar-nominated movies, about 90%
of them would fall within this range.
 None of the above.",None of the above.,74.0,Medium
99,Wi,24,Quiz,4,Problem 3,Problem 3.4,"Suppose both of the following expressions evaluate to
True.

my_sample.get(""rating"").mean() == 61.25
np.std(my_sample.get(""rating"")) == 15

What are the left and right endpoints of a 95% CLT-based confidence
interval for the mean rating of Oscar-nominated movies?","left endpoint: 59.75, right endpoint:
62.75",54.0,Medium
100,Sp,24,Quiz,1,Problem 1,Problem 1,,,,
101,Sp,24,Quiz,1,Problem 1,Problem 1.1,"True or False: int(3.4) and round(3.4)
evaluate to the same value.

 True
 False",True,89.0,Easy
102,Sp,24,Quiz,1,Problem 1,Problem 1.2,"True or False: For any float x, int(x) and
round(x) evaluate to the same value.

 True
 False",False,95.0,Easy
103,Sp,24,Quiz,1,Problem 2,Problem 2,"Consider the following code.
iris = 3 / 1
poppy = 8 - 6
daisy = np.array([8, 1, 5])
lily = np.array([4, 2])
poppy = iris ** iris - iris * poppy",,,
104,Sp,24,Quiz,1,Problem 2,Problem 2.1,"What is the value of poppy after this code is
executed?",21.0,54.0,Medium
105,Sp,24,Quiz,1,Problem 2,Problem 2.2,"What is the result of the expression daisy + lily?

 array([8, 1, 5, 4, 2])
 array([12, 3, 5])
 array([12, 3])
 This expression errors.",Option D,68.0,Medium
106,Sp,24,Quiz,1,Problem 2,Problem 2.3,"What is the result of the expression
daisy + lily[0]?

 array([8, 1, 5, 4])
 array([12, 5, 9])
 array([10, 3, 7])
 This expression errors.",Option B,79.0,Easy
107,Sp,24,Quiz,1,Problem 3,Problem 3,"You are tracking the growth of a flower stem over a seven-day period.
The flower stem starts out at 24.5 cm and ends up at 29.7 cm.
Write one line of code that calculates the average daily growth, in
centimeters, and assigns the result to the variable
avg_growth. Do not round your answer.",avg_growth = (29.7 - 24.5) / 7,84.0,Easy
108,Sp,24,Quiz,1,Problem 4,Problem 4,"Suppose flower_data is a DataFrame with information on
different species of flowers, where:

The ""species"" column contains the name of the
species of flower, as a string. Each value in this column is
unique.
The ""petals"" column contains the average number of
petals of flowers of this species, as an int.
The ""length"" column contains the average stem length
of flowers of this species in inches, as a float.",,,
109,Sp,24,Quiz,1,Problem 4,Problem 4.1,"One of these three columns is a good choice to use as the index of
this DataFrame. Write a line of code that sets this column as the index
of flower_data, and assigns the resulting DataFrame to the
variable flowers.","flowers = flower_data.set_index(""species"")",79.0,Easy
110,Sp,24,Quiz,1,Problem 4,Problem 4.2,"Which of the following expressions evaluates to a DataFrame that is
sorted by ""petals"" in descending order?

 flowers.sort_values(by = ""petals"", ascending = True)
 flowers.sort_values(by = ""petals"", ascending = False)
 flowers.get(""petals"").sort_values(ascending = True)
 flowers.get(""petals"").sort_values(ascending = False)",Option B,83.0,Easy
111,Sp,24,Quiz,1,Problem 4,Problem 4.3,"Suppose that the 4th row of flowers corresponds to a
rare species of flower named ""fire lily"". Fill in the
blanks below so that both of these expressions evaluate to the stem
length in inches of ""fire lily"".
i. flowers.get(""length"").loc[__(x)__]
ii. flowers.get(""length"").iloc[__(y)__]","(x): ""fire lily"", (y):
3",83.0,Easy
112,Sp,24,Quiz,1,Problem 4,Problem 4.4,"Suppose that the 3rd row of flowers corresponds to the
species ""stinking corpse lily"". Using the
flowers DataFrame and the string method
.split(), write an expression that evaluates to
""corpse"".","flowers.index[2].split("" "")[1]",46.0,Hard
113,Sp,24,Quiz,2,Problem 1,Problem 1,"Write an expression that evaluates to the number of art pieces made
in 1950 that cost less than $10,000.","art[(art.get(""year"") == 1950) & (art.get(""price"") < 10000)].shape[0]",72.0,Medium
114,Sp,24,Quiz,2,Problem 2,Problem 2,,,,
115,Sp,24,Quiz,2,Problem 2,Problem 2.1,"Fill in the blanks in the code below to find the name of the artist
in art who has the lowest mean price of art pieces.
art.groupby(___(x)___).___(y)___.sort_values(by=""price"").index[0]","(x): ""artist"", (y):
mean()",94.0,Easy
116,Sp,24,Quiz,2,Problem 2,Problem 2.2,"Fill in the blanks in the code below to find the name of the artist
in art who made the most art pieces in a single year.
(art.groupby(___(x)___).___(y)___.reset_index()
        .sort_values(by=""title"", ascending=False)
        .get(""artist"").iloc[0])","(x):
[""artist"", ""year""] or [""year"", ""artist""], (y):
count()",59.0,Medium
117,Sp,24,Quiz,2,Problem 3,Problem 3,,,,
118,Sp,24,Quiz,2,Problem 3,Problem 3.1,"Which of the following correctly plots a density histogram showing
the distribution of ""price"" in art? Select all
that apply.

 art.get([""price""]).plot(kind=""hist"", density=True)
 art.get([""price""]).plot(kind=""hist"")
 art.drop(columns=[""artist"", ""year"", ""price""]).plot(kind=""hist"", density=True)
 art.plot(kind=""hist"", y=""price"")
 art.plot(kind=""hist"", y=""price"", density=True)
 art.plot(kind=""hist"", x=""price"", density=True)",Options 1 and 5,83.0,Easy
119,Sp,24,Quiz,2,Problem 3,Problem 3.2,"The density histogram below shows the distribution of
""price"" in art. If the museum has 100 art
pieces in total, how many pieces cost at least $3,000 but less than
$4,500?",30,55.0,Medium
120,Sp,24,Quiz,2,Problem 4,Problem 4,,,,
121,Sp,24,Quiz,2,Problem 4,Problem 4.1,"Fill in the return statement of the function
is_expensive, which takes as input the price of an art
piece (as a float, in dollars) and returns True if the
price is more than 20 million dollars. Otherwise, it returns
False.
def is_expensive(price):
        return ___(a)___",price > 20_000_000,74.0,Medium
122,Sp,24,Quiz,2,Problem 4,Problem 4.2,"Write one line of code to add a new column called exp to
the art DataFrame, which categorizes if each art piece is
worth more than 20 million dollars, using Boolean values. You must use
the is_expensive function you wrote above. Make sure to
modify art!","art = art.assign(exp = art.get(""price"").apply(is_expensive))",73.0,Medium
123,Sp,24,Quiz,2,Problem 4,Problem 4.3,"Next, we make a new DataFrame called expensive as
follows.
expensive = art[art.get(exp)]
merged = art.merge(expensive, on=""artist"")
Van Gogh is one artist represented in art and exactly
half of his pieces in art are worth over 20 million
dollars. If Van Gogh’s art appears in 72 rows of the merged
DataFrame, how many rows does Van Gogh actually have in the original
art DataFrame?",12,16.0,Hard
124,Sp,24,Quiz,3,Problem 1,Problem 1,"Which of the following statements are true in general? Select all
that apply.

 Parameters are fixed, but statistics can change depending on the
sample.
 Parameters and statistics can both fluctuate depending on the
sample.
 For simple random samples, statistics give better estimates of
parameters when the sample size is larger.
 The distribution of a statistic is the same regardless of the sample
size.
 None of the above.",,90.0,Easy
125,Sp,24,Quiz,3,Problem 2,Problem 2,"The DataFrame restaurants contains information about a
sample of restaurants in San Diego County. We have each restaurant’s
""name"" (str), ""rating"" (int), average
""meal_price"" (float), and type of
""cuisine"" (str), such as ""Thai"" or
""Italian"".",,,
126,Sp,24,Quiz,3,Problem 2,Problem 2.1,"You are interested in estimating the average
""meal_price"" across all Italian restaurants in San Diego
County using only the data in restaurants. Fill in the
following code so that italian_means evaluates to an array
of 1000 bootstrapped estimates for this parameter.
    def bootstrap_means(data, n_samples):
        means = np.array([])
        for i in range(n_samples):
            resample = data.sample(__(a)__, replace = __(b)__)
            means = np.append(means, __(c)__)
        return means

    italian_restaurants = __(d)__
    italian_means = bootstrap_means(italian_restaurants, __(e)__)",,73.0,Medium
127,Sp,24,Quiz,3,Problem 2,Problem 2.2,"Next, fill in the blanks below so that italian_CI
evaluates to an 88% bootstrapped confidence interval for the average
""meal_price"" across all Italian restaurants in San Diego
County.
    lower_bound = np.percentile(italian_means, __(a)__)
    upper_bound = np.percentile(italian_means, __(b)__)
    italian_CI = [lower_bound, upper_bound]",,83.0,Easy
128,Sp,24,Quiz,3,Problem 2,Problem 2.3,"Suppose italian_CI evaluates to [25, 35]. Which of the
following statements are correct? Select all that apply.

 If we randomly selected 1000 Italian restaurants from the population
of Italian restaurants in San Diego County, about 880 of them will have
an average ""meal_price""  between $25 and $35.
 There is an 88% chance that the average ""meal_price"" of
Italian restaurants in San Diego County falls between $25 and $35.
 88% of all Italian restaurants have an average
""meal_price"" between $25 and $35.
 None of the above.",,64.0,Medium
129,Sp,24,Quiz,3,Problem 3,Problem 3,"Which of the following can be used to generate a simple
random sample of ""rating""s from 10 restaurants in
restaurants? Select all that apply.
Option 1:
    sample = restaurants.take(np.arange(10)).get(""rating"")
Option 2:
    sample = restaurants.sample(10, replace = False).get(""rating"")
Option 3:
    sample = restaurants.sample(10, replace = True).get(""rating"")
Option 4:
    positions = np.random.choice(np.arange(0, restaurants.shape[0]), 
                                 10, replace = False)
    sample = restaurants.take(positions).get(""rating"")
Option 5:
    positions = np.random.choice(np.arange(0, restaurants.shape[0]), 
                                 10, replace = True)
    sample = restaurants.take(positions).get(""rating"")

 Option 1
 Option 2
 Option 3
 Option 4
 Option 5",,65.0,Medium
130,Sp,24,Quiz,4,Problem 1,Problem 1,,,,
131,Sp,24,Quiz,4,Problem 1,Problem 1.1,"Suppose that the trees on UCSD’s campus have a mean height of 100
feet and a variance of 36 feet. If the height of a
specific tree is 124 feet, what would its height be in standard units
for this distribution? Simplify your answer.",4,73.0,Medium
132,Sp,24,Quiz,4,Problem 1,Problem 1.2,"Let A be the answer to the previous
question. Choose the best interpretation of A.

 A randomly selected tree on UCSD’s campus has an A percent probability of being at least 124
feet tall.
 A 124 foot tree is A times taller
than the average tree on UCSD’s campus.
 A 124 foot tree is A standard
deviations taller than the average tree on UCSD’s campus.
 At least 95% of trees on UCSD’s campus have a height within A standard deviations of the mean height.","A 124 foot tree is A standard deviations taller than the average
tree on UCSD’s campus.",84.0,Easy
133,Sp,24,Quiz,4,Problem 2,Problem 2,"You are told that scipy.stats.norm.cdf(-1.4) evaluates
to 0.08075665923377107. Suppose you have a standard normal
curve with mean at x=0 and standard
deviation 1. What is the area under the curve from x=0 to x=1.4? Give your answer as a number rounded
to 2 decimal places.",0.42,57.0,Medium
134,Sp,24,Quiz,4,Problem 3,Problem 3,"Suppose we measure the height in feet of a sample of trees on UCSD’s
campus and use this sample to generate a 95% CLT-based confidence
interval for the mean height of trees on campus. Let W be the width of this confidence
interval.
If we instead were to measure the height of the same sample in
inches, and again generate a 95% CLT-based confidence interval for the
mean, what would be the width of this confidence interval in terms of
W? There are 12 inches in 1 foot.

 \dfrac{W}{12}
 \dfrac{W}{\sqrt{12}}
 W
 12W
 144W",12W,63.0,Medium
135,Sp,24,Quiz,4,Problem 4,Problem 4,"Which of the following quantities must be known in order to construct
a CLT-based confidence interval for the population mean? Select all that
apply.

 Shape of the population (normal or not)
 Shape of the sample (normal or not)
 Mean of the population
 Mean of the sample
 Standard deviation of the population
 Standard deviation of the sample
 Size of the population
 Size of the sample","Mean of the sample, Standard deviation of
the sample, Size of the sample",73.0,Medium
136,Sp,24,Quiz,4,Problem 5,Problem 5,"We want to collect a sample of trees and use this sample to determine
the proportion of all trees that are oak trees (a population parameter).
We want to create a 95% confidence interval that is at most 0.04 wide.
Which of the following inequalities should we use to find the smallest
viable sample size we could collect?

 \text{sample size} \geq (4 *
\frac{0.5}{0.04})^2
 \text{sample size} \geq (2 *
\frac{0.5}{0.04})^2
 \text{sample size} \geq (4 *
\frac{1}{0.04})^2
 \text{sample size} \geq (2 *
\frac{1}{0.04})","\text{sample size}
\geq (4 * \frac{0.5}{0.04})^2",52.0,Medium
137,Sp,24,Quiz,4,Problem 6,Problem 6,"Suppose that the trees on UCSD’s campus are 35% eucalyptus, 25% pine,
and the remaining 40% some other variety. Write one line of
code to simulate the act of randomly sampling 40 trees from
this distribution, with replacement. Your code should output an array of
length 3 where the elements represent the number of eucalyptus, pine,
and other trees, respectively.","np.random.multinomial(40, [0.35, 0.25, 0.40])",85.0,Easy
138,Fa,21,Midterm,,Problem 1,Problem 1,,,,
139,Fa,21,Midterm,,Problem 1,Problem 1.1,"Which of these would it make sense to use as the index of
flights?

 'DATE'
 'FLIGHT'
 'FROM'
 'TO'
 None of these are good choices for the index","None of these are good choices for the
index
When choosing an index, we have to make sure that the index is
different for each row of the DataFrame. The index in this case should
uniquely identify the flight.
'DATE'does not uniquely identify a flight because there
are many different flights in a single day. 'FLIGHT' does
not uniquely identify a flight because airlines reuse flight numbers on
a daily basis, as we are told in the data description. Neither
'FROM' nor 'TO' uniquely identifies a flight,
as there are many flights each day that depart from each airport and
arrive at each airport.
Therefore, there is no single column that’s sufficient to uniquely
identify a flight, but if we could use multiple columns to create what’s
called a multi-index, we’d probably want to use 'DATE' and
'FLIGHT' because each row of our DataFrame should have a
unique pair of values in these columns. That’s because airlines don’t
reuse flight numbers within a single day.",57.0,Medium
140,Fa,21,Midterm,,Problem 1,Problem 1.2,"What type of variable is 'FLIGHT'?

 Categorical
 Numerical","Categorical
'FLIGHT' is a categorical variable because it doesn’t
make sense to do arithmetic with the values in the 'FLIGHT'
column. 'FLIGHT' is just a label for each flight, and the
fact that it includes some numbers does not make it numerical. We could
have just as well used letter codes to distinguish flights.",98.0,Easy
141,Fa,21,Midterm,,Problem 2,Problem 2,,,,
142,Fa,21,Midterm,,Problem 2,Problem 2.1,"Which of these correctly evaluates to the number of flights King
Triton took to San Diego (airport code 'SAN')?

 flights.loc['SAN'].shape[0]
 flights[flights.get('TO') == 'SAN'].shape[0]
 flights[flights.get('TO') == 'SAN'].shape[1]
 len(flights.sort_values('TO', ascending=False).loc['SAN'])","flights[flights.get('TO') == 'SAN'].shape[0]
The strategy is to create a DataFrame with only the flights that went
to San Diego, then count the number of rows. The first step is to query
with the condition flights.get('TO') == 'SAN' and the
second step is to extract the number of rows with
.shape[0].
Some of the other answer choices use .loc['SAN'] but
.loc only works with the index, and flights
does not have airport codes in its index.",95.0,Easy
143,Fa,21,Midterm,,Problem 2,Problem 2.2,"Fill in the blanks below so that the result also evaluates to the
number of flights King Triton took to San Diego (airport code
'SAN').
flights.groupby(__(a)__).count().get('FLIGHT').__(b)__            
What goes in blank (a)?

 'DATE'
 'FLIGHT'
 'FROM'
 'TO'

What goes in blank (b)?

 .index[0]
 .index[-1]
 .loc['SAN']
 .iloc['SAN']
 .iloc[0]

True or False: If we change .get('FLIGHT') to
.get('SEAT'), the results of the above code block will not
change. (You may assume you answered the previous two subparts
correctly.)

 True
 False","'TO',
.loc['SAN'], True
The strategy here is to group all of King Triton’s flights according
to where they landed, and count up the number that landed in San Diego.
The expression flights.groupby('TO').count() evaluates to a
DataFrame indexed by arrival airport where, for any arrival airport,
each column has a count of the number of King Triton’s flights that
landed at that airport. To get the count for San Diego, we need the
entry in any column for the row corresponding to San Diego. The code
.get('FLIGHT') says we’ll use the 'FLIGHT'
column, but any other column would be equivalent. To access the entry of
this column corresponding to San Diego, we have to use .loc
because we know the name of the value in the index should be
'SAN', but we don’t know the row number or integer
position.",89.0,Easy
144,Fa,21,Midterm,,Problem 2,Problem 2.3,"Consider the DataFrame san, defined below.
san = flights[(flights.get('FROM') == 'SAN') & (flights.get('TO') == 'SAN')]
Which of these DataFrames must have the same number
of rows as san?

 flights[(flights.get('FROM') == 'SAN') and (flights.get('TO') == 'SAN')]
 flights[(flights.get('FROM') == 'SAN') | (flights.get('TO') == 'SAN')]
 flights[(flights.get('FROM') == 'LAX') & (flights.get('TO') == 'SAN')]
 flights[(flights.get('FROM') == 'LAX') & (flights.get('TO') == 'LAX')]","flights[(flights.get('FROM') == 'LAX') & (flights.get('TO') == 'LAX')]
The DataFrame san contains all rows of
flights that have a departure airport of 'SAN'
and an arrival airport of 'SAN'. But as you may know, and
as you’re told in the data description, there are no flights from an
airport to itself. So san is actually an empty DataFrame
with no rows!
We just need to find which of the other DataFrames would necessarily
be empty, and we can see that
flights[(flights.get('FROM') == 'LAX') & (flights.get('TO') == 'LAX')]
will be empty for the same reason.
Note that none of the other answer choices are correct. The first
option uses the Python keyword and instead of the symbol
&, which behaves unexpectedly but does not give an
empty DataFrame. The second option will be non-empty because it will
contain all flights that have San Diego as the departure airport or
arrival airport, and we already know from the first few rows of
flight that there are some of these. The third option will
contain all the flights that King Triton has taken from
'LAX' to 'SAN'. Perhaps he’s never flown this
route, or perhaps he has. This DataFrame could be empty, but it’s not
necessarily going to be empty, as the question requires.",70.0,Medium
145,Fa,21,Midterm,,Problem 3,Problem 3,"Fill in the blanks below so that the result is a DataFrame with the
same columns as flights plus a new column,
'SPEED', containing the average speed of each flight, in
miles per hour.
flights.__(a)__(SPEED=__(b)__)",,,
146,Fa,21,Midterm,,Problem 3,Problem 3.1,"What goes in blank (a)?

 groupby
 assign
 rename
 drop
 merge","assign
We want to add a new column, so we must use assign. We
can also tell that the answer will be assign because it’s
the only DataFrame method that takes an input of the form
SPEED=___. Remember that when using assign, we get to call
the new column anything we want, and we don’t use quotes around its
name.",100.0,Easy
147,Fa,21,Midterm,,Problem 3,Problem 3.2,What goes in blank (b)?,"(flights.get('DIST') / flights.get('HOURS'))
In this blank, we’ll need a Series or array containing the average
speed of each flight, in miles per hour.
To calculate the average speed of an individual flight in miles per
hour, we’d simply divide the total number of miles by the total amount
of time in hours. For example, a flight that travels 500 miles in one
hour travels at 500 miles per hour. Note that this is an
average speed; at some points of the journey, the plane may
have been moving faster than this speed, at other times slower. Because
we are calculating an average speed for the whole trip by simply
dividing, we don’t need to use .mean().
Once we know how to calculate the average speed for an individual
flight, we can do the same operation on each flight all at once using
Series arithmetic. flights.get('DIST') is a Series
containing the distances of each flight, and
flights.get('HOURS') is a Series containing the times of
each flight, in the same order. When we divide these two Series,
corresponding entries are divided and the result is a Series of average
speeds for each flight, as desired.",93.0,Easy
148,Fa,21,Midterm,,Problem 4,Problem 4,We define the seasons as follows:,,,
149,Fa,21,Midterm,,Problem 4,Problem 4.1,"We want to create a function date_to_season that takes
in a date as formatted in the 'DATE' column of
flights and returns the season corresponding to that date.
Which of the following implementations of date_to_season
works correctly? Select all that apply.
Option 1:
def date_to_season(date):
    month_as_num = int(date.split('-')[1])
    if month_as_num >= 3 and month_as_num < 6:
        return 'Spring'
    elif month_as_num >= 6 and month_as_num < 9:
        return 'Summer'
    elif month_as_num >= 9 and month_as_num < 12:
        return 'Fall'
    else:
        return 'Winter'
Option 2:
def date_to_season(date):
    month_as_num = int(date.split('-')[1])
    if month_as_num >= 3 and month_as_num < 6:
        return 'Spring'
    if month_as_num >= 6 and month_as_num < 9:
        return 'Summer'
    if month_as_num >= 9 and month_as_num < 12:
        return 'Fall'
    else:
        return 'Winter'
Option 3:
def date_to_season(date):
    month_as_num = int(date.split('-')[1])
    if month_as_num < 3:
        return 'Winter'
    elif month_as_num < 6:
        return 'Spring'
    elif month_as_num < 9:
        return 'Summer'
    elif month_as_num < 12:
        return 'Fall'
    else:
        return 'Winter' 

 Option 1
 Option 2
 Option 3
 None of these implementations of date_to_season work
correctly","Option 1, Option 2, Option 3
All three options start with the same first line of code:
month_as_num = int(date.split('-')[1]). This takes the
date, originally a string formatted such as '2021-09-07',
separates it into a list of three strings such as
['2021', '09', '07'], extracts the element in position 1
(the middle position), and converts it to an int such as 9.
Now we have the month as a number we can work with more easily.
According to the definition of seasons, the months in each season are
as follows:




Season
Month
month_as_num




Spring
March, April, May
3, 4, 5


Summer
June, July, August
6, 7, 8


Fall
September, October, November
9, 10, 11


Winter
December, January, February
12, 1, 2




Option 1 correctly assigns months to seasons by checking if the month
falls in the appropriate range for 'Spring', then
'Summer', then 'Fall'. Finally, if all of
these conditions are false, the else branch will return the
correct answer of 'Winter' when month_as_num
is 12, 1, or 2.
Option 2 is also correct, and in fact, it does the same exact thing
as Option 1 even though it uses if where Option 1 used
elif. The purpose of elif is to check a
condition only when all previous conditions are false. So if we have an
if followed by an elif, the elif
condition will only be checked when the if condition is
false. If we have two sequential if conditions, typically
the second condition will be checked regardless of the outcome of the
first condition, which means two if statements can behave
differently than an if followed by an elif. In
this case, however, since the if statements cause the
function to return and therefore stop executing, the only
way to get to a certain if condition is when all previous
if conditions are false. If any prior if
condition was true, the function would have returned already! So this
means the three if conditions in Option 2 are equivalent to
the if, elif, elif structure of
Option 1. Note that the else case in Option 1 is reached
when all prior conditions are false, whereas the else in
Option 2 is paired only with the if statement immediately
preceding it. But since we only ever get to that third if
statement when the first two if conditions are false, we
still only reach the else branch when all three
if conditions are false.
Option 3 works similarly to Option 1, except it separates the months
into more categories, first categorizing January and February as
'Winter', then checking for 'Spring',
'Summer', and 'Fall'. The only month that
winds up in the else branch is December. We can think of
Option 3 as the same as Option 1, except the Winter months have been
separated into two groups, and the group containing January and February
is extracted and checked first.",76.0,Easy
150,Fa,21,Midterm,,Problem 4,Problem 4.2,"Assuming we’ve defined date_to_season correctly in the
previous part, which of the following lines of code correctly computes
the season for each flight in flights?

 date_to_season(flights.get('DATE'))
 date_to_season.apply(flights).get('DATE')
 flights.apply(date_to_season).get('DATE')
 flights.get('DATE').apply(date_to_season)","flights.get('DATE').apply(date_to_season)
Our function date_to_season takes as input a single date
and converts it to a season. We cannot input a whole Series of dates, as
in the first answer choice. We instead need to apply the
function to the whole Series of dates. The correct syntax to do that is
to first extract the Series of dates from the DataFrame and then use
.apply, passing in the name of the function we wish to
apply to each element of the Series. Therefore, the correct answer is
flights.get('DATE').apply(date_to_season).",97.0,Easy
151,Fa,21,Midterm,,Problem 5,Problem 5,"Suppose we create a DataFrame called socal containing
only King Triton’s flights departing from SAN, LAX, or SNA (John Wayne
Airport in Orange County). socal has 10 rows; the bar chart
below shows how many of these 10 flights departed from each airport.


Consider the DataFrame that results from merging socal
with itself, as follows:
double_merge = socal.merge(socal, left_on='FROM', right_on='FROM')
How many rows does double_merge have?","38
There are two flights from LAX. When we merge socal with
itself on the 'FROM' column, each of these flights gets
paired up with each of these flights, for a total of four rows in the
output. That is, the first flight from LAX gets paired with both the
first and second flights from LAX. Similarly, the second flight from LAX
gets paired with both the first and second flights from LAX.
Following this logic, each of the five flights from SAN gets paired
with each of the five flights from SAN, for an additional 25 rows in the
output. For SNA, there will be 9 rows in the output. The total is
therefore 2^2 + 5^2 + 3^2 = 4 + 25 + 9 =
38 rows.",27.0,Hard
152,Fa,21,Midterm,,Problem 6,Problem 6,"We define a “route” to be a departure and arrival airport pair. For
example, all flights from 'SFO' to 'SAN' make
up the “SFO to SAN route”. This is different from the “SAN to SFO
route”.
Fill in the blanks below so that
most_frequent.get('FROM').iloc[0] and
most_frequent.get('TO').iloc[0] correspond to the departure
and destination airports of the route that King Triton has spent the
most time flying on.
most_frequent = flights.groupby(__(a)__).__(b)__
most_frequent = most_frequent.reset_index().sort_values(__(c)__)",,,
153,Fa,21,Midterm,,Problem 6,Problem 6.1,What goes in blank (a)?,"['FROM', 'TO']
We want to organize flights by route. This means we need to group by
both 'FROM' and 'TO' so any flights with the
same pair of departure and arrival airports get grouped together. To
group by multiple columns, we must use a list containing all these
column names, as in flights.groupby(['FROM', 'TO']).",72.0,Medium
154,Fa,21,Midterm,,Problem 6,Problem 6.2,"What goes in blank (b)?

 count()
 mean()
 sum()
 max()","sum()
Every .groupby command needs an aggregation function!
Since we are asked to find the route that King Triton has spent the most
time flying on, we want to total the times for all flights on a given
route.
Note that .count() would tell us how many flights King
Triton has taken on each route. That’s meaningful information, but not
what we need to address the question of which route he spent the most
time flying on.",58.0,Medium
155,Fa,21,Midterm,,Problem 6,Problem 6.3,"What goes in blank (c)?

 by='HOURS', ascending=True
 by='HOURS', ascending=False
 by='HOURS', descending=True
 by='DIST', ascending=False","by='HOURS', ascending=False
We want to know the route that King Triton spent the most time flying
on. After we group flights by route, summing flights on the same route,
the 'HOURS' column contains the total amount of time spent
on each route. We need most_frequent.get('FROM').iloc[0]
and most_frequent.get('TO').iloc[0] to correspond with the
departure and destination airports of the route that King Triton has
spent the most time flying on. To do this, we need to sort in descending
order of time, to bring the largest time to the top of the DataFrame. So
we must sort by 'HOURS' with
ascending=False.",94.0,Easy
156,Fa,21,Midterm,,Problem 7,Problem 7,"The seat-back TV on one of King Triton’s more recent flights was very
dirty and was full of fingerprints. The fingerprints made an interesting
pattern. We’ve stored the x and y positions of each fingerprint in the
DataFrame fingerprints, and created the following
scatterplot using
fingerprints.plot(kind='scatter', x='x', y='y')",,,
157,Fa,21,Midterm,,Problem 7,Problem 7.1,"True or False: The histograms that result from the following two
lines of code will look very similar.
fingerprints.plot(kind='hist', 
                  y='x',
                  density=True,
                  bins=np.arange(0, 8, 2))
and
fingerprints.plot(kind='hist', 
                  y='y',
                  density=True,
                  bins=np.arange(0, 8, 2))

 True
 False","True
The only difference between the two code snippets is the data values
used. The first creates a histogram of the x-values in
fingerprints, and the second creates a histogram of the
y-values in fingerprints.
Both histograms use the same bins:
bins=np.arange(0, 8, 2). This means the bin endpoints are
[0, 2, 4, 6], so there are three distinct bins: [0, 2), [2,
4), and [4, 6]. Remember the
right-most bin of a histogram includes both endpoints, whereas others
include the left endpoint only.
Let’s look at the x-values first. If we divide the
scatterplot into nine equally-sized regions, as shown below, note that
eight of the nine regions have a very similar number of data points.

Aside from the middle region, about \frac{1}{8} of the data falls in each region.
That means \frac{3}{8} of the data has
an x-value in the first bin [0,
2), \frac{2}{8} of the data has
an x-value in the middle bin [2,
4), and \frac{3}{8} of the data
has an x-value in the rightmost bin [4, 6]. This distribution of
x-values into bins determines what the histogram will look
like.
Now, if we look at the y-values, we’ll find that \frac{3}{8} of the data has a
y-value in the first bin [0,
2), \frac{2}{8} of the data has
a y-value in the middle bin [2,
4), and \frac{3}{8} of the data
has a y-value in the last bin [4,
6]. That’s the same distribution of data into bins as the
x-values had, so the histogram of y-values
will look just like the histogram of y-values.
Alternatively, an easy way to see this is to use the fact that the
scatterplot is symmetric over the line y=x, the line that makes a 45 degree angle
with the origin. In other words, interchanging the x and
y values doesn’t change the scatterplot noticeably, so the
x and y values have very similar
distributions, and their histograms will be very similar as a
result.",88.0,Easy
158,Fa,21,Midterm,,Problem 7,Problem 7.2,"Below, we’ve drawn a histogram using the line of code
fingerprints.plot(kind='hist', 
                  y='x',
                  density=True,
                  bins=np.arange(0, 8, 2))
However, our Jupyter Notebook was corrupted, and so the resulting
histogram doesn’t quite look right. While the height of the first bar is
correct, the histogram doesn’t contain the second or third bars, and the
y-axis is replaced with letters.

Which of the four options on the y-axis is closest to where the
height of the middle bar should be?

 A
 B
 C
 D

Which of the four options on the y-axis is closest to where the
height of the rightmost bar should be?

 A
 B
 C
 D","B, then C
We’ve already determined that the first bin should contain \frac{3}{8} of the values, the middle bin
should contain \frac{2}{8} of the
values, and the rightmost bin should contain \frac{3}{8} of the values. The middle bar of
the histogram should therefore be two-thirds as tall as the first bin,
and the rightmost bin should be equally as tall as the first bin. The
only reasonable height for the middle bin is B, as it’s closest to
two-thirds of the height of the first bar. Similarly, the rightmost bar
must be at height C, as it’s the only one close to the height of the
first bar.",94.0,Easy
159,Fa,21,Midterm,,Problem 8,Problem 8,"It turns out that King Triton is so busy that he doesn’t even book
his own flights – he has a travel agent who books his flights for him.
He doesn’t get to choose the airline that he flies on, but his travel
agent gave him the following table, which describes the probability of
each of his flights in 2022 being on Delta, United, American, or another
airline:",,,
160,Fa,21,Midterm,,Problem 8,Problem 8.1,"What is the probability that all 3 flights are on United? Give your
answer as an exact decimal between 0 and 1 (not a
Python expression).","0.027
For all three flights to be on United, we need the first flight to be
on United, and the second, and the third. Since these are independent
events that do not impact one another, and we need all three flights to
separately be on United, we need to multiply these probabilities, giving
an answer of 0.3*0.3*0.3 = 0.027.
Note that on an exam without calculator access, you could leave your
answer as (0.3)^3.",93.0,Easy
161,Fa,21,Midterm,,Problem 8,Problem 8.2,"What is the probability that all 3 flights are on Delta, or all on
United, or all on American? Give your answer as an
exact decimal between 0 and 1 (not a Python
expression).","0.099
We already calculated the probability of all three flights being on
United as (0.3)^3 = 0.027. Similarly,
the probability of all three flights being on Delta is (0.4)^3 = 0.064, and the probability of all
three flights being on American is (0.2)^3 =
0.008. Since we cannot satisfy more than one of these conditions
at the same time, we can separately add their probabilities to find a
total probability of 0.027 + 0.064 + 0.008 =
0.099.",76.0,Easy
162,Fa,21,Midterm,,Problem 8,Problem 8.3,"True or False: The probability that all 3 flights are on the same
airline is equal to the probability you computed in the previous
subpart.

 True
 False","False
It’s not quite the same because the previous subpart doesn’t include
the probability that all three flights are on the same airline which is
not one of Delta, United, or American. For example, there is a small
probability that all three flights are on Allegiant or all three flights
are on Southwest.",90.0,Easy
163,Fa,21,Midterm,,Problem 9,Problem 9,"King Triton has boarded a Southwest flight. For in-flight
refreshments, Southwest serves four types of cookies – chocolate chip,
gingerbread, oatmeal, and peanut butter.
The flight attendant comes to King Triton with a box containing 10
cookies:

4 chocolate chip
3 gingerbread
2 oatmeal, and
1 peanut butter

The flight attendant tells King Triton to grab 2 cookies out of the
box without looking.
Fill in the blanks below to implement a simulation that estimates the
probability that both of King Triton’s selected cookies are the
same.
# 'cho' stands for chocolate chip, 'gin' stands for gingerbread,
# 'oat' stands for oatmeal, and 'pea' stands for peanut butter.

cookie_box = np.array(['cho', 'cho', 'cho', 'cho', 'gin', 
                       'gin', 'gin', 'oat', 'oat', 'pea'])

repetitions = 10000
prob_both_same = 0
for i in np.arange(repetitions):
    grab = np.random.choice(__(a)__)
    if __(b)__:
        prob_both_same = prob_both_same + 1
prob_both_same = __(c)__",,,
164,Fa,21,Midterm,,Problem 9,Problem 9.1,"What goes in blank (a)?

 cookie_box, repetitions, replace=False
 cookie_box, 2, replace=True
 cookie_box, 2, replace=False
 cookie_box, 2","cookie_box, 2, replace=False
We are told that King Triton grabs two cookies out of the box without
looking. Since this is a random choice, we use the function
np.random.choice to simulate this. The first input to this
function is a sequence of values to choose from. We already have an
array of values to choose from in the variable cookie_box.
Calling np.random.choice(cookie_box) would select one
cookie from the cookie box, but we want to select two, so we use an
optional second parameter to specify the number of items to randomly
select. Finally, we should consider whether we want to select with or
without replacement. Since cookie_box contains individual
cookies and King Triton is selecting two of them, he cannot choose the
same exact cookie twice. This means we should sample without
replacement, by specifying replace=False. Note that
omitting the replace parameter would use the default option
of sampling with replacement.",92.0,Easy
165,Fa,21,Midterm,,Problem 9,Problem 9.2,What goes in blank (b)?,"grab[0] == grab[1]
The idea of a simulation is to do some random process many times. We
can use the results to approximate a probability by counting up the
number of times some event occurred, and dividing that by the number of
times we did the random process. Here, the random process is selecting
two cookies from the cookie box, and we are doing this 10,000 times. The
approximate probability will be the number of times in which both
cookies are the same divided by 10,000. So we need to count up the
number of times that both randomly selected cookies are the same. We do
this by having an accumulator variable that starts out at 0 and gets
incremented, or increased by 1, every time both cookies are the same.
The code has such a variable, called prob_both_same, that
is initialized to 0 and gets incremented when some condition is met.
We need to fill in the condition, which is that both randomly
selected cookies are the same. We’ve already randomly selected the
cookies and stored the results in grab, which is an array
of length 2 that comes from the output of a call to
np.random.choice. To check if both elements of the
grab array are the same, we access the individual elements
using brackets with the position number, and compare using the
== symbol to check equality. Note that at the end of the
for loop, the variable prob_both_same will
contain a count of the number of trials out of 10,000 in which both of
King Triton’s cookies were the same flavor.",79.0,Easy
166,Fa,21,Midterm,,Problem 9,Problem 9.3,"What goes in blank (c)?

 prob_both_same / repetitions
 prob_both_same / 2
 np.mean(prob_both_same)
 prob_both_same.mean()","prob_both_same / repetitions
After the for loop, prob_both_same contains
the number of trials out of 10,000 in which both of King Triton’s
cookies were the same flavor. We’d like it to represent the approximate
probability of both cookies being the same flavor, so we need to divide
the current value by the total number of trials, 10,000. Since this
value is stored in the variable repetitions, we can divide
prob_both_same by repetitions.",93.0,Easy
167,Fa,21,Midterm,,Problem 10,Problem 10,"Note: This problem is out of scope; it
covers material no longer included in the course.
In response to the pandemic, some airlines chose to leave middle
seats empty, while others continued seating passengers in middle seats.
Let’s suppose Delta did not seat passengers in middle seats during the
pandemic, and United did seat passengers in middle seats during the
pandemic.
Delta wants to know whether customers were satisfied with them for
making this decision not to use middle seats. Suppose they have access
to a dataset of customer satisfaction surveys, taken annually for each
airline. How can Delta determine whether its new seating policy caused
an increase in customer satisfaction?

 Compare Delta’s average customer satisfaction before and after this
change went into effect.
 Compare Delta’s average customer satisfaction after the change went
into effect to United’s average customer satisfaction at the same point
in time.
 Compare the change in Delta’s average customer satisfaction to the
change in United’s average customer satisfaction, throughout the same
period of time, spanning the change.
 None of the above.","None of the above.
None of the options isolate the effect of the seating policy because
they do not use randomized controlled trials. Even measuring the change
in each airline’s average satisfaction rating as described in the third
option is insufficient because we don’t know whether any differences are
due to the changed seating policy or other changes. It’s possible that
many things changed around the time of the pandemic, and we have no way
of separating the effects of each of these changes. For example, maybe
United stopped serving snacks during the pandemic and Delta continued
serving snacks, at around the same time as the seating changes went into
effect. If we find a difference in average customer satisfaction between
the airlines, we have no way of knowing whether it’s because of the
differences in seating policies or snack policies (or something
else).",13.0,Hard
168,Fa,22,Midterm,,Problem 1,Problem 1,"Note: This problem is out of scope; it
covers material no longer included in the course.
TritonTire, a UCSD alumni-run car repair shop, has an electronic
database of all services it has ever provided, dating back to 1987. The
owner, Dasha, says:

Our collective records from all 50 states show that, over the years,
our repair shops have performed more and more repairs on EVs. After
discussing this with our employees, we believe this is because newer EVs
are not made to the same standards as the gasoline cars before them.

In at most two sentences, identify the largest
confounding factor that Dasha has not considered in her argument. Your
answer must fit in the box below.","The confounding factor that Dasha hasn’t
considered is the prevalence of EVs. There are a lot more EVs now than
ever before, so even if EVs are made to the same standards as gasoline
cars or better, we’d expect to see more repairs of EVs now.",68.0,Medium
169,Fa,22,Midterm,,Problem 2,Problem 2,,,,
170,Fa,22,Midterm,,Problem 2,Problem 2.1,"Which type of visualization should we use to visualize the
distribution of ""Range""?

 Bar chart
 Histogram
 Scatter plot
 Line plot","Histogram
""Range"" is a numerical (i.e. quantitative) variable, and
we use histograms to visualize the distribution of numerical
variables.

A bar chart couldn’t work here. Bar charts can show the distribution
of a categorical variable, but ""Range"" is not
categorical.
A scatter plot visualizes the relationship between two numerical
variables, but we are only dealing with one numerical variable here
(""Range"").
Similarly, a line plot visualizes the relationship between two
numerical variables, but we only have one here.",63.0,Medium
171,Fa,22,Midterm,,Problem 2,Problem 2.2,"Teslas, on average, tend to have higher ""Range""s than
BMWs. In which of the following visualizations would we be able to see
this pattern? Select all that apply.

 A bar chart that shows the distribution of ""Brand""
 A bar chart that shows the average ""Range"" for each
""Brand""
 An overlaid histogram showing the distribution of
""Range"" for each ""Brand""
 A scatter plot with ""TopSpeed"" on the x-axis and ""Range"" on the y-axis","A bar chart that shows the average ""Range"" for each
""Brand""
An overlaid histogram showing the distribution of
""Range"" for each ""Brand""

Let’s look at each option more closely.

Option 1: A bar chart showing the distribution
of ""Brand"" would only show us how many cars of each
""Brand"" there are. It would not tell us anything about the
average ""Range"" of each ""Brand"".
Option 2: A bar chart showing the average range
for each ""Brand"" would help us directly visualize how the
average range of each ""Brand"" compares to one
another.
Option 3: An overlaid histogram, although
perhaps a bit messy, would also give us a general idea of the average
range of each ""Brand"" by giving us the distribution of the
""Range"" of each brand. In the scenario mentioned in the
question, we’d expect to see that the Tesla distribution is further
right than the BMW distribution.
Option 4: A scatter plot of
""TopSpeed"" against ""Range"" would only
illustrate the relationship between ""TopSpeed"" and
""Range"", but would contain no information about the
""Brand"" of each EV.",91.0,Easy
172,Fa,22,Midterm,,Problem 2,Problem 2.3,"Gabriel thinks ""Seats"" is a categorical variable because
it can be used to categorize EVs by size. For instance, EVs with 4 seats
are small, EVs with 5 seats are medium, and EVs with 6 or more seats are
large.
Is Gabriel correct?

 Yes
 No

Justify your answer in one sentence. Your answer
must fit in the box below.","No
""Seats"" is a numerical variable, since it makes sense to
do arithmetic with the values. For instance, we can find the average
number of ""Seats"" that a group of cars has. Gabriel’s
argument could apply to any numerical variable; just because we can
place numerical variables into “bins” doesn’t make them categorical.",51.0,Medium
173,Fa,22,Midterm,,Problem 3,Problem 3,"Suppose we’ve run the following two lines of code. ​
first = evs.get(""Brand"").apply(max) 
second = evs.get(""Brand"").max()
Note:

The length of a value v is defined as
len(v), unless v is a DataFrame, in which case
its length is v.shape[0].
If s is a string, then max(s) also
evaluates to a string.",,,
174,Fa,22,Midterm,,Problem 3,Problem 3.1,"Fill in the blanks: first is a __(i)__ of length
__(ii)__.
(i):

 list
 array
 string
 DataFrame
 Series

(ii): _____","(i): Series
(ii): 32

The .apply method applies a function on every element of
a Series. Here, evs.get(""Brand"").apply(max) applies the
max function on every element of the ""Brand""
column of evs, producing a new Series with the same length
as evs.
While not necessary to answer the question, if s is a
string, then max(s) evaluates to the single character in
s that is last in the alphabet. For instance,
max(""zebra"") evaluates to ""z"". As such,
evs.get(""Brand"").apply(max) is a Series of 32 elements,
each of which is a single character, corresponding to the latest
character in the alphabet for each entry in
evs.get(""Brand"").",65.0,Medium
175,Fa,22,Midterm,,Problem 3,Problem 3.2,"Fill in the blanks: second is a __(i)__ of length
__(ii)__.
(i):

 list
 array
 string
 DataFrame
 Series

(ii): _____","(i): string
(ii): 5

The .max() method will find the “largest” element in the
Series it is called in, which in this case is
evs.get(""Brand""). The way that strings are ordered is
alphabetically, so evs.get(""Brand"").max() will be the last
value of ""Brand"" alphabetically. Since we were told that
the only values in the ""Brand"" column are
""Tesla"", ""BMW"", ""Audi"", and
""Nissan"", the “maximum” is ""Tesla"", which has
a length of 5.",54.0,Medium
176,Fa,22,Midterm,,Problem 4,Problem 4,"Suppose we’ve run the following line of code. ​
counts = evs.groupby(""Brand"").count()",,,
177,Fa,22,Midterm,,Problem 4,Problem 4.1,"What value does counts.get(""Range"").sum() evaluate
to?","32
counts is a DataFrame with one row per
""Brand"", since we grouped by ""Brand"". Since we
used the .count() aggregation method, the columns in
counts will all be the same – they will all contain the
number of rows in evs for each ""Brand""
(i.e. they will all contain the distribution of ""Brand"").
If we sum up the values in any one of the columns in
counts, then, the result will be the total number of rows
in evs, which we know to be 32. Thus,
counts.get(""Range"").sum() is 32.",56.0,Medium
178,Fa,22,Midterm,,Problem 4,Problem 4.2,What value does counts.index[3] evaluate to?,"""Tesla""
Since we grouped by ""Brand"" to create
counts, the index of counts will be
""Brand"", sorted alphabetically (this sorting happens
automatically when grouping). This means that counts.index
will be the array-like sequence
[""Audi"", ""BMW"", ""Nissan"", ""Tesla""], and
counts.index[3] is ""Tesla"".",33.0,Hard
179,Fa,22,Midterm,,Problem 5,Problem 5,"Consider the following incomplete assignment statement.
result = evs______.mean()
In each part, fill in the blank above so that result evaluates to the
specified quantity.",,,
180,Fa,22,Midterm,,Problem 5,Problem 5.1,"A DataFrame, indexed by ""Brand"", whose
""Seats"" column contains the average number of
""Seats"" per ""Brand"". (The DataFrame may have
other columns in it as well.)",".groupby(""Brand"")
When we group by a column, the resulting DataFrame contains one row
for every unique value in that column. The question specified that we
wanted some information per ""Brand"", which implies
that grouping by ""Brand"" is necessary.
After grouping, we need to use an aggregation method. Here, we wanted
the resulting DataFrame to be such that the ""Seats"" column
contained the average number of ""Seats"" per
""Brand""; this is accomplished by using
.mean(), which is already done for us.
Note: With the provided solution, the resulting DataFrame also has
other columns. For instance, it has a ""Range"" column that
contains the average ""Range"" for each ""Brand"".
That’s fine, since we were told that the resulting DataFrame may have
other columns in it as well. If we wanted to ensure that the only column
in the resulting DataFrame was ""Seats"", we could have used
.get([""Brand"", ""Seats""]) before grouping, though this was
not necessary.",76.0,Easy
181,Fa,22,Midterm,,Problem 5,Problem 5.2,"A number, corresponding to the average ""TopSpeed"" of all
EVs manufactured by Audi in evs","[evs.get(""Brand"") == ""Audi""].get(""TopSpeed"")
There are two parts to this problem:

Querying, to make sure that we only keep the rows corresponding
to Audis. This is accomplished by:

Using evs.get(""Brand"") == ""Audi"" to create a Boolean
Series, with Trues for the rows we want to keep and
Falses for the other rows.
Using Boolean indexing to keep only the rows in which the
aforementioned Series is True. This is accomplished by
evs[evs.get(""Brand"") == ""Audi""] (though the
evs part at the front was already provided).

Accessing the ""TopSpeed"" column. This is
accomplished by using .get(""TopSpeed"").

Then, evs[evs.get(""Brand"") == ""Audi""].get(""TopSpeed"") is
a Series contaning the ""TopSpeed""s of all Audis, and mean
of this Series is the result we’re looking for. The call to
.mean() was already provided for us.",77.0,Easy
182,Fa,22,Midterm,,Problem 5,Problem 5.3,"A number, corresponding to the average of the natural logarithm of
the ""TopSpeed"" of all EVs in evs. (Hint: The function
np.log computes the natural logarithm of a single
number.)",".get(""TopSpeed"").apply(np.log)
The .apply method is used to apply a function on every
element of a Series. The relevant Series here is the column containing
the ""TopSpeed"" of each EV,
i.e. evs.get(""TopSpeed"") (the evs part was
already provided to us).
After we get that Series, we need to use the function
np.log on every element of it. This is accomplished by
using .apply(np.log). Putting our steps so far together, we
have evs.get(""TopSpeed"").apply(np.log), which is a Series
containing the natural logarithm of the ""TopSpeed"" of all
EVs in evs.
The number we were asked for was the average of the natural logarithm
of the ""TopSpeed"" of all EVs in evs; all we
need to do now is use the .mean() method at the end, which
was already done for us.",61.0,Medium
183,Fa,22,Midterm,,Problem 6,Problem 6,"The DataFrame below shows the distribution of
""BodyStyle"" for all ""Brands"" in
evs, other than Nissan. We will call EVs made by a
""Brand"" other than Nissan “non-Nissan EVs”; there are 24
non-Nissan EVs in evs.

Use the information above to answer the following questions.",,,
184,Fa,22,Midterm,,Problem 6,Problem 6.1,"Suppose we randomly select one of the non-Nissan EVs and it is either
an SUV or a sedan. What is the most likely ""Brand"" of the
randomly selected non-Nissan EV?

 Tesla
 BMW
 Audi","Audi
Let’s compute the number of EVs that are either SUVs or sedans for
each non-Nissan Brand. (To do this, we’ll look at the right-most two
columns in the DataFrame provided.)

Number of Teslas that are SUVs or sedans: 4 + 3 = 7
Number of BMWs that are SUVs or sedans: 1
+ 1 = 2
Number of Audis that are SUVs or sedans: 8
+ 1 = 9

Since Audi is the ""Brand"" with the most total SUVs and
sedans, it is the most likely ""Brand"" to be selected.
Note: You could compute conditional probabilities for each
brand, if you’d like, by dividing the counts above by 18 (the total
number of SUVs and sedans). For instance, P(\text{EV is a BMW given that EV is an SUV or
sedan}) = \frac{2}{18}. The ""Brand"" with the highest
count (Audi, with 9 SUVs or sedans) is also the ""Brand""
with the highest conditional probability of being selected given that
the selected car is an SUV or sedan (Audi, with \frac{9}{18}).",88.0,Easy
185,Fa,22,Midterm,,Problem 6,Problem 6.2,"Suppose we randomly select two of the non-Nissan EVs without
replacement. The probability that both are BMWs is equal to \frac{1}{k} , where k is a positive integer.
What is k?

 8
 56
 64
 84
 92
 108","92
In the first selection, the probability of selecting a BMW is \frac{1+1+1}{24} = \frac{3}{24} (3 is the
total number of EVs that are BMW, and 24 is the total number of
non-Nissan EVs as given by the question).
In the second selection, since we select without
replacement, there are only 23 EVs we can select from. Given
that in the first selection we already selected 1 BMW, there are only 2
BMWs left among the 23 EVs left. Thus, the probability of getting a BMW
in the second selection is \frac{2}{23}.
Putting this all together, the probability that both selections are
BMWs is
\frac{3}{24}\cdot\frac{2}{23} =
\frac{6}{24} \cdot \frac{1}{23} =\frac{1}{4} \cdot \frac{1}{23} =
\frac{1}{92}
So, k = 92.",67.0,Medium
186,Fa,22,Midterm,,Problem 6,Problem 6.3,"Suppose we randomly select one of the non-Nissan EVs and it is an
SUV. What is the probability that it is made by Tesla? Give your answer
as a simplified fraction.","\frac{4}{13}
The question is asking for the proportion of SUVs that are made by
Tesla.
We first need to find the number of SUVs in the DataFrame provided,
which is 4 + 1 + 8 = 13. Of those 13
SUVs, 4 are made by Tesla. Thus, the proportion of SUVs made by Tesla is
\frac{4}{13}, so the probability that a
randomly selected SUV is made by Tesla is \frac{4}{13}.",66.0,Medium
187,Fa,22,Midterm,,Problem 7,Problem 7,"Below, we provide the same DataFrame as shown at the start of the
previous problem, which contains the distribution of “BodyStyle” for all
“Brands” in evs, other than Nissan.

Suppose we’ve run the following few lines of code.
tesla = evs[evs.get(""Brand"") == ""Tesla""]
bmw = evs[evs.get(""Brand"") == ""BMW""]
audi = evs[evs.get(""Brand"") == ""Audi""]

combo = tesla.merge(bmw, on=""BodyStyle"").merge(audi, on=""BodyStyle"")
How many rows does the DataFrame combo have?

 21
 24
 35
 65
 72
 96","35
Let’s attempt this problem step-by-step. We’ll first determine the
number of rows in tesla.merge(bmw, on=""BodyStyle""), and
then determine the number of rows in combo. For the
purposes of the solution, let’s use temp to refer to the
first merged DataFrame,
tesla.merge(bmw, on=""BodyStyle"").
Recall, when we merge two DataFrames, the resulting
DataFrame contains a single row for every match between the two columns,
and rows in either DataFrame without a match disappear. In this problem,
the column that we’re looking for matches in is
""BodyStyle"".
To determine the number of rows of temp, we need to
determine which rows of tesla have a
""BodyStyle"" that matches a row in bmw. From
the DataFrame provided, we can see that the only
""BodyStyle""s in both tesla and
bmw are SUV and sedan. When we merge tesla and
bmw on ""BodyStyle"":

The 4 SUV rows in tesla each match the 1 SUV row in
bmw. This will create 4 SUV rows in temp.
The 3 sedan rows in tesla each match the 1 sedan row in
bmw. This will create 3 sedan rows in
temp.

So, temp is a DataFrame with a total of 7 rows, with 4
rows for SUVs and 3 rows for sedans (in the ""BodyStyle"")
column. Now, when we merge temp and audi on
""BodyStyle"":

The 4 SUV rows in temp each match the 8 SUV rows in
audi. This will create 4 \cdot 8
= 32 SUV rows in combo.
The 3 sedan rows in temp each match the 1 sedan row in
audi. This will create 3 \cdot 1
= 3 sedan rows in combo.

Thus, the total number of rows in combo is 32 + 3 = 35.
Note: You may notice that 35 is the result of multiplying the
""SUV"" and ""Sedan"" columns in the DataFrame
provided, and adding up the results. This problem is similar to",,
188,Fa,22,Midterm,,Problem 5,Problem 5,"from the Fall 2021
Midterm.

Difficulty: ⭐️⭐️⭐️⭐️


The average score on this problem was 45%.",,45.0,Hard
189,Fa,22,Midterm,,Problem 8,Problem 8,"TritonTrucks is an EV startup run by UCSD alumni. Their signature EV,
the TritonTruck, has a subpar battery (the engineers didn’t pay
attention in their Chemistry courses).
A new TritonTruck’s battery needs to be replaced after 500 days,
unless it fails first, in which case it needs to be replaced
immediately. On any given day, the probability that a given
TritonTruck’s battery fails is 0.02, independent of all other days.
Fill in the blanks so that
average_days_until_replacement is an estimate of the
average number of days a new TritonTruck’s battery lasts without
needing to be replaced.
def days_until_replacement(daily_conditions):
    days = 0
    for i in __(a)__:
        if daily_conditions[i] == True:
            __(b)__
        else:
            return days
    return days
    
total = 0
repetitions = 10000
for i in np.arange(repetitions):
    # The first element of the first argument to np.random.choice is
    # chosen with probability 0.98
    daily_conditions = np.random.choice(__(c)__, 500, p=[0.98, 0.02])
    total = total + days_until_replacement(daily_conditions)
average_days_until_replacement = total / repetitions
What goes in blanks (a), (b), and (c)?","Blank (a): np.arange(len(daily_conditions))
Blank (b): days = days + 1
Blank (c): [True, False]

At a high-level, here’s how this code block works:

daily_conditions is an array of length 500, in each
each element is True with probability 0.98 and
False with probability 0.02. Each element of
daily_conditions is meant to represent whether or not the
given TritonTruck’s battery failed on that day. For instance, if the
first four elements of daily_conditions are
[True, True, False, True, ...], it means the battery was
fine the first two days, but failed on the third day.
The function days_until_replacement takes in
daily_conditions and returns the number of days until the
battery failed for the first time. In other words, it returns the number
of elements before the first False in
daily_conditions. In the example above, where the first
four elements of daily_conditions are
[True, True, False, True, ...],
days_until_replacement would return 2, since the battery
lasted 2 days until it needed to be replaced. It doesn’t matter what is
in daily_conditions after the first
False.

With that in mind, let’s fill in the pieces.

Blank (a): We need to loop over all elements in
daily_conditions. There are two ways to do this, in theory
– by looping over the elements themselves
(e.g. for cond in daily_conditions) or their positions
(e.g. for i in np.arange(len(daily_conditions))). However,
here we must loop over their positions, because the body of
days_until_replacement uses
daily_conditions[i], which only makes sense if
i is the position of an element in
daily_conditions. Since daily_conditions has
500 elements, the possible positions are 0, 1, 2, …, 499. Since
len(daily_conditions) is 500, both
np.arange(len(daily_conditions)) and
np.arange(500) yield the same correct result here.
Blank (b): days is the “counter” variable that is
being used to keep track of the number of days the battery lasted before
failing. If daily_conditions[i] is True, it
means that the battery lasted another day without failing, and so 1
needs to be added to days. As such, the correct answer here
is days = days + 1 (or days += 1). (If
daily_conditions[i] is False, then the battery
has failed, and so we return the number of days until the first
failure.)
Blank (c): This must be [True, False], as mentioned
above. There are other valid answers too, including
np.array([True, False]) and [1, 0].",56.0,Medium
190,Fa,22,Midterm,,Problem 9,Problem 9,"Histograms A and B below both display the distribution of the
""Seats"" column, using different bins. Each histogram
includes all 32 rows of evs.",,,
191,Fa,22,Midterm,,Problem 9,Problem 9.1,How many EVs in evs have exactly 6 seats?,"3
Here are two ways to solve this problem. In both solutions, we only
look at Histogram A, since only Histogram A contains a bin that
corresponds to EVs with exactly 6 ""Seats"". Recall, in
histograms, bins are inclusive of the left endpoint and exclusive of the
right endpoint, which means the [6, 7) bin represents EVs with >= 6
""Seats"" and < 7 ""Seats""; since the number
of ""Seats"" is a whole number, this corresponds to exactly 6
""Seats"".
Solution 1
Since the bin [6, 7) has a width of 1, its height is equal to its
area, which is equal to the proportion of values in that bin. There are
32 values total, so all proportions (and, thus, the height of the [6, 7)
bar) must be a multiple of \frac{1}{32}. The height is close to but just
under 0.01; this implies the height is \frac{3}{32}, since that is also close to but
just under 0.01 (\frac{4}{32} is way
above and \frac{2}{32} is way below).
So, we conclude that the number of EVs with 6 seats is 3.
Solution 2
The height of the [6, 7) bar is ever-so-slightly less than 0.1. If
it’s height was 0.1, it would imply that the proportion of values in the
[6, 7) bin was 0.1 \cdot (7 - 6) = 0.1,
which would imply that the number of values in the [6, 7) bin is 0.1 \cdot 32 = 3.2. However, since the number
of values in a bin must be an integer, the number of values in this bin
is 3 (which is slightly less than 3.2).",61.0,Medium
192,Fa,22,Midterm,,Problem 9,Problem 9.2,How many EVs in evs have exactly 5 seats?,"22
Now, we must look at Histogram B. In the previous part, we computed
that there are 3 EVs with exactly 6 ""Seats"" in
evs. Histogram B shows us the proportion, and thus number,
of EVs with 5 or 6 ""Seats"", through its [5, 7) bin
(remember, this bin corresponds to EVs with >= 5 ""Seats""
and < 7 ""Seats""). If we can find the number of EVs with
5 or 6 ""Seats"", we can subtract 3 from it to determine the
number of EVs with exactly 5 ""Seats"".
Since it’s not quite clear what the height of the [5, 7) bar is, we
can create a range for the height of the bar, and use that to
come up with a range for the area of the bar, and hence a range for the
number of values in the bin. We can then use the fact that the number of
values in a bin must be an integer to narrow down our answer.

The height of the [5, 7) bar is less than 0.4. This means that the
area of the [5, 7) bar is less than (7 - 5)
\cdot 0.4 = 0.8, which means that the proportion of EVs with 5 or
6 ""Seats"" is less than 0.8, and the number of EVs with 5 or
6 ""Seats"" is less than 0.8 \cdot
32 = 25.6.
The height of the [5, 7) bar is a bit above 0.375, which is the
midpoint between 0.35 and 0.4 on the y-axis. This means that the area of the [5,
7) bar is more than (7 - 5) \cdot 0.375 =
0.75, which means that the number of EVs with 5 or 6
""Seats"" is more than 0.75 \cdot
32 = 24.

We’ve found that the number of EVs with 5 or 6 ""Seats""
is more than 24 and less than 25.6. There is only one integer in this
range – 25 – so the number of EVs with 5 or 6 ""Seats"" is
25. Finally, the number of EVs with exactly 5 seats is 25 - 3 = 22.",35.0,Hard
193,Fa,22,Midterm,,Problem 9,Problem 9.3,"Histogram C also displays the distribution of the
""Seats"" column, but uses just a single bin, [4, 9]. What is
the height of the sole bar in Histogram C?","\frac{1}{5}
(or 0.2)
Recall, the total area of a (density) histogram is 1. Since Histogram
C only has one bar, the area of that one bar must be 1. we can use this
fact to find what its height must be.
\begin{align*}
\text{Area} &= \text{Width} \cdot \text{Height} \\
1 &= (9 - 4) \cdot \text{Height} \\
\frac{1}{5} &= \text{Height}
\end{align*}",68.0,Medium
194,Fa,23,Midterm,,Problem 1,Problem 1,,,,
195,Fa,23,Midterm,,Problem 1,Problem 1.1,"You’re interested in comparing the ""avg_housing_cost""
across different ""family_type"" groups for San Diego County,
CA specifically. Which type of visualization would be most
appropriate?


 Scatter plot
 Line plot
 Bar chart
 Histogram","Bar chart

""family_type"" is a categorical variable, and we use bar
charts to visualize the distribution of categorical variables.

A scatter plot visualizes the relationship between two numerical
variables, but we are only using one numerical variable here
(""avg_housing_cost"").
A line plot is used to visualize the trend between two numerical
variables, but we are only using one numerical variable.
A histogram is used to visualize the distribution of numerical
variables, but we want to see the distribution of the categorical
variable ""family_type"".",89.0,Easy
196,Fa,23,Midterm,,Problem 1,Problem 1.2,"Suppose we run the three lines of code below.
families = living_cost.groupby(""family_type"").median()
sorted_families = families.sort_values(by=""avg_housing_cost"")
result = sorted_families.get(""avg_childcare_cost"").iloc[0]
Which of the following does result evaluate to?

 The median ""avg_childcare_cost"" of the
""family_type"" with the lowest median
""avg_housing_cost"".
 The median ""avg_childcare_cost"" of the
""family_type"" with the highest median
""avg_housing_cost"".
 The median ""avg_housing_cost"" of the
""family_type"" with the lowest median
""avg_childcare_cost"".
 The median ""avg_housing_cost"" of the
""family_type"" with the highest median
""avg_childcare_cost"".","The median ""avg_childcare_cost""
of the ""family_type"" with the lowest median
""avg_housing_cost"".

When we grouped living_cost by
""family_type"", families is a DataFrame with
one row per ""family_type"". Using the .median()
aggregation method takes the median of all numerical columns per
""family_type"".
sorted_families is the families DataFrame,
but sorted in ascending order based on the
""avg_housing_cost"" column. The first row of
sorted_families is the ""family_type"" with the
lowest median ""avg_housing_cost"", and the last row of
sorted_families is the ""family_type"" with the
highest median ""avg_housing_cost"".
In the last line of code, we’re getting the
""avg_childcare_cost"" column from the
sorted_families DataFrame. We then use iloc to
get the first entry in the ""avg_childcare_cost"" column.
Since sorted_families is sorted in ascending order, this
means that we’re getting the lowest median in the column. Therefore,
result evaluates to the median
""avg_childcare_cost"" of the ""family_type"" with
the lowest median ""avg_housing_cost"".",82.0,Easy
197,Fa,23,Midterm,,Problem 1,Problem 1.3,"Suppose we define another_result as follows.
another_result = (living_cost.groupby(""state"").count()
                  .sort_values(by=""median_income"", ascending=False)
                  .get(""median_income"").index[0])
What does another_result represent?

 The state with the highest median income.
 The median income in the state with the highest median income.
 The state with the most counties.
 The median income in the state with the most counties.","The state with the most counties.

The living_cost DataFrame is being grouped by the
""state"" column, so there is now one row per
""state"". By using the .count() aggregation
method, the columns in the DataFrame will contain the number of rows
in living_count for each ""state"". All of the
columns will also be the same after using .count(), so they
will all contain the distribution of ""state"". Since
living_cost has data on every county in the US, the grouped
DataFrame represents the number of counties that each state has.
We then sort the DataFrame in descending order, so the state with the
most counties is at the top of the DataFrame. The last line of the
expression gets a column and uses .index to get the state
corresponding to the first entry, which happens to be the state with the
most counties and the value that gets assigned to
another_result.
Since all the columns are the same, it doesn’t matter which column we
pick to use in the .sort_values() method. In this case, we
used the ""median_income"" column, but picking any other
column will produce the same result.",65.0,Medium
198,Fa,23,Midterm,,Problem 1,Problem 1.4,"Which of the following DataFrames has exactly four columns?

 living_cost.groupby(""family_type"").min()
 living_cost.groupby(""family_type"").sum()
 living_cost.groupby(""family_type"").count()
 None of the above.","living_cost.groupby(""family_type"").sum()

Since we can’t take the sum of columns with categorical data, all of
the columns in living_cost that contain non-numerical data
are dropped after we use the .sum() aggregation method.
There are four columns in living_cost that have numerical
data (""is_metro"", ""avg_housing_cost"",
""avg_childcare_cost"", and ""median_income"").
Since Python can take the sum of these numerical columns, these four
columns are kept. Therefore, the resulting DataFrame has exactly four
columns.
Although ""is_metro"" contains Boolean values, Python can
still calculate the sum of this column. The Boolean value
True corresponds to 1 and False corresponds to
0.",35.0,Hard
199,Fa,23,Midterm,,Problem 1,Problem 1.5,"Suppose we define the Series three_columns to be the
concatenation of three columns of the living_cost DataFrame
as follows.
three_columns = (living_cost.get(""state"") + "" "" +
                 living_cost.get(""county"") + "" "" + 
                 living_cost.get(""family_type""))
For example, the first element of three_columns is the
string ""CA San Diego County 1a2c"" (refer back to the first
row of living_cost provided in the data overview).
What does the following expression evaluate to?
(living_cost.assign(geo_family=three_columns)
            .groupby(""geo_family"").count()
            .shape[0])

 10, the number of distinct
""family_type"" values.
 50, the number of states in the
US.
 500, the number of combinations of
states in the US and ""family_type"" values.
 3143, the number of counties in the
US.
 31430, the number of rows in the
living_cost DataFrame.","31430, the
number of rows in the living_cost DataFrame.

The first line of the expression creates a new column in
living_cost, called ""geo_family"" that
represents the concatenation of the values in
""three_columns"". When we group the DataFrame by
""geo_family"", we create a new DataFrame that contains a row
for every unique value in ""three_columns"".
""three_columns"" has various combinations of
""state"", ""country"", and
""family_type"". Since it’s given in the DataFrame
description that each of the 31430 rows of the DataFrame represents a
different combination of ""state"", ""country"",
and ""family_type"", this means that the grouped DataFrame
has 31430 unique combinations as well. Therefore, when we use
.shape[0] to get the number of rows in the grouped
DataFrame in the last line of the expression, we get the same value as
the number of rows in the living_cost DataFrame, 31430.",74.0,Medium
200,Fa,23,Midterm,,Problem 2,Problem 2,"Suppose we define the three variables below.
J = living_cost.get(""county"") == ""Benton County""
K = living_cost.get(""state"") == ""IN""
L = living_cost.get(""family_type"") == ""1a2c""
Feel free to use these variables in your solutions to the following
questions.",,,
201,Fa,23,Midterm,,Problem 2,Problem 2.1,"Fill in the blanks so that the expression below evaluates to the
average yearly childcare cost for families with one adult and two
children in Benton County, IN.
    __(a)__.__(b)__.iloc[0]

What goes in blank (a)?
What goes in blank (b)?","living_cost[J & K & L]

The first step is to query the rows in the DataFrame that meet our
specific criteria. In this case, we want the rows in the DataFrame where
the county is ""Benton County"", the state is
""IN"", and the family has 1 adult and 2 children.
J, K, and L specify these
criteria. When used to query the living_cost DataFrame, we
are able to obtain a DataFrame with only one row, corresponding this
family type in this specific county.",69.0,Medium
202,Fa,23,Midterm,,Problem 2,Problem 2.2,"Fill in the blanks so that the expression below evaluates to the
number of states with a county named Benton County.
    __(c)__.__(d)__ / 10

What goes in blank (c)?
What goes in blank (d)?","living_cost[J]

Since we want to find how many states have a county named
""Benton County"", we first want to obtain all the rows of
the DataFrame where the county is ""Benton County"". Variable
J specifies this condition, so we use it to query and
obtain a DataFrame with the rows in living_cost where the
county is ""Benton County.""",60.0,Medium
203,Fa,23,Midterm,,Problem 3,Problem 3,"Suppose we want to assign a new column named
""family_size"" to living_cost that contains the
total number of people in each family, stored as an int. We do so as
follows.
living_cost = living_cost.assign(
              family_size=living_cost.get(""family_type"").apply(num_people))
Which of the following options correctly define the function
num_people such that the line above adds the
""family_size"" column as desired? Select all that
apply.
Hint: You can access an individual character in a string
using the position number in square brackets. For example,
""midterm""[0] evaluates to ""m"" and
""midterm""[1] evaluates to ""i"".
# Option 1
def num_people(fam): 
    return int(fam[0]) + int(fam[2])
------------------------------------
# Option 2
def num_people(fam):
    return int(fam[0] + fam[2])
------------------------------------
# Option 3
def num_people(fam):
    x = int(fam[0] + fam[2])
    return int(x / 10) + x % 10
------------------------------------
# Option 4
def num_people(fam):
    x = fam.strip(""c"").split(""a"")
    return int(x[0]) + int(x[1])
------------------------------------
# Option 5
def num_people(fam):
    x = 0
    for i in fam:
        if i % 2 == 0:
            x = x + 1
    return x
------------------------------------
# Option 6
def num_people(fam):
    x = 0
    for i in np.arange(len(fam)):
        if i % 2 == 0:
            x = x + int(fam[i])
    return x

 Option 1
 Option 2
 Option 3
 Option 4
 Option 5
 Option 6
 None of the above.","Options 1, 3, 4, 6
Option 1: In order to get the number of people within a family, we
can look at the character at position 0 (for the number of adults) and
the character at position 2 (for the number of children). Converting
each character into an int and adding these ints yields the correct
results.
Option 2: This is similar to Option 1, however, the key difference is
that the separate strings are concatenated first, then converted into an
integer afterwards. Remember that the plus sign between two strings
concatenates the strings together, and does not add mathematically. For
example, on a family type of ""1a2c"", ""1"" and
""2"" will be extracted and concatenated together as
""12"", then converted to the int 12. This is returned
instead of the value 3 that we are looking for.
Option 3: This option is similar to Option 2, however, it includes an
extra step after concatenation. int(x/10) gets the value in
the tens place, taking advantage of the fact that the int()
function always rounds down. At the same time, x % 10 gets
the value in the ones place by calculating the remainder upon division
by ten. Looking at the example of ""1a2c"", the first line
will set x = 12 and then int(12/10) will yield
1 while 12 % 10 yields 2. Adding these together achieves
the correct answer of 3.
Option 4: This option is similar to Option 1, but includes the
initial step of removing ""c"" from the string and separating
by ""a"". After this, x is a list of two
elements, the first of which represents the number of adults in the
family, and the second of which represents the number of children in the
family. These are separately converted to ints then added up in the last
line.
Option 5: This option iterates through the input string, where
i represents each individual character in the string. For
example, on an input of ""1a2c"", i is first set
to 1, then a, then 2, then
c. However, calculating the remainder when we divide by two
(i % 2) only makes sense when i is a number,
and results in an error when i is a string.
Option 6: This is a similar approach to Option 5, except this time,
i represents each of the numbers 0, 1, 2, and 3, since
len(fam) is always 4. For each such i, which
we can think of as the position number, the code will check if the
position number is even (i % 2 == 0). This is only true for
position 0 and 2, which are the positions that contain the numbers of
adults and children in the family. When this condition is met, we add
the value at that position onto our running total, x, which
at the end, equals the total number of adults and children in the
family.",73.0,Medium
204,Fa,23,Midterm,,Problem 4,Problem 4,"For those who plan on having children, an important consideration
when deciding whether to live in an area is the cost of raising children
in that area. The DataFrame expensive, defined below,
contains all of the rows in living_cost where the
""avg_childcare_cost"" is at least $20,000.
expensive = living_cost[living_cost.get(""avg_childcare_cost"") 
                        >= 20000]
We’ll call a county an “expensive county"" if there is at
least one ""family_type"" in that county with an
""avg_childcare_cost"" of at least $20,000. Note that all
expensive counties appear in the expensive DataFrame, but
some may appear multiple times (if they have multiple
""family_type""s with an ""avg_childcare_cost"" of
at least $20,000).
Recall that the ""is_metro"" column contains Boolean
values indicating whether or not each county is part of a metropolitan
(urban) area. For all rows of living_cost (and, hence,
expensive) corresponding to the same geographic location,
the value of ""is_metro"" is the same. For instance, every
row corresponding to San Diego County has an ""is_metro""
value of True.
Fill in the blanks below so that the result is a DataFrame indexed by
""state"" where the ""is_metro"" column gives the
proportion of expensive counties in each state that are part of
a metropolitan area. For example, if New Jersey has five
expensive counties and four of them are metropolitan, the row
corresponding to a New Jersey should have a value of 0.8 in the
""is_metro"" column.
(expensive.groupby(____(a)____).max()
          .reset_index()
          .groupby(____(b)____).____(c)____)",,,
205,Fa,23,Midterm,,Problem 4,Problem 4.1,What goes in blank (a)?,"[""state"", ""county""] or
[""county"", ""state""]
We are told that all expensive counties appear in the
expensive DataFrame, but some may appear multiple times,
for several different ""family_type"" values. The question we
want to answer, however, is about the proportion of expensive counties
in each state that are part of a metropolitan area, which has nothing to
do with ""family_type"". In other words, we don’t want or
need multiple rows corresponding to the same US county.
To keep just one row for each US county, we can group by both
""state"" and ""county"" (in either order). Then
the resulting DataFrame will have one row for each unique combination of
""state"" and ""county"", or one row for each US
county. Notice that the .max() aggregation method keeps the
last alphabetical value from the ""is_metro"" column in each
US county. If there are multiple rows in expensive
corresponding to the same US county, we are told that they will all have
the same value in the ""is_metro"" column, so taking the
maximum just takes any one of these values, which are all the same. We
could have just as easily taken the minimum.
Notice the presence of .reset_index() in the provided
code. That is a clue that we may need to group by multiple columns in
this problem!",14.0,Hard
206,Fa,23,Midterm,,Problem 4,Problem 4.2,What goes in blank (b)?,"""state""
Now that we have one row for each US county that is considered
expensive, we want to proceed by calculating the proportion of expensive
counties within each state that are in a metropolitan area. Our goal is
to organize the counties by state and create a DataFrame indexed only by
""state"" so we want to group by ""state"" to
achieve this.",,
207,Fa,23,Midterm,,Problem 4,Problem 4.3,What goes in blank (c)?,"mean()
Recall that the ""is_metro"" column consists of Boolean
values, where True equals 1 and False equals
0. Notice that if we take the average of the ""is_metro""
column for all the counties in a given state, we’ll be computing the sum
of these 0s and 1s (or the number of True values) divided
by the total number of expensive counties in that state. This gives the
proportion of expensive counties in the state that are in a metropolitan
area. Thus, when we group the expensive counties according to what state
they are in, we can use the .mean() aggregation method to
calculate the proportion of expensive counties in each state that are in
a metropolitan area.",35.0,Hard
208,Fa,23,Midterm,,Problem 5,Problem 5,"The rows in living_cost with a
""family_type"" value of ""1a0c"" correspond to
families that consist of individuals living on their own. We’ll call
such families “solo families."" Below, we’ve visualized the distribution
of the ""median_income"" column, but only for rows
corresponding to solo families. Instead of visualizing median incomes in
dollars, we’ve visualized them in thousands of dollars.

Suppose we’re interested in splitting the [50, 80) bin into two separate bins — a [50, 70) bin and a [70, 80) bin.
Let h_1 be the height of the new bar
corresponding to the [50, 70) bin and
let h_2 be the height of the new bar
corresponding to the [70, 80) bin.",,,
209,Fa,23,Midterm,,Problem 5,Problem 5.1,"What are the minimum and maximum possible values of h_2? Give your answers as decimals
rounded to three decimal places.","Minimum: 0
In a histogram, we do not know how data are distributed within a bin.
This means that when we split the bin with range [50, 80) into two smaller bins, we have no
way of knowing how the data from the original bin will be distributed.
It is possible that all of the data in the [50, 80) bin fell between 50 and 70.
In this case, there would be no data in the [70, 80) bin, and as such, the height of this
new bar would be 0.",61.0,Medium
210,Fa,23,Midterm,,Problem 5,Problem 5.2,"Suppose that the number of counties in which the median income of
solo families is in the interval [50,
70) is r times the number of
counties in which the median income of solo families is in the interval
[70, 80). Given this fact, what is the
value of \frac{h_1}{h_2}, the ratio of
the heights of the two new bars?

 \frac{1}{r}
 \frac{2}{r}
 \frac{3}{r}
 \frac{r}{2}
 \frac{r}{3}
 2r
 3r","\frac{r}{2}
The key to solving this problem is recognizing that the number of
counties in a given interval is directly related to the area of that
interval’s bar in the histogram. This comes from the property of density
histograms that the area of a bar corresponds to the proportion of the
data contained within the bar.
Given that there are r times the
amount of data in the interval [50,
70), in comparison to the interval [70,
80), we know that the area of the bar corresponding to
the bin [50, 70) is r times the area of the bar corresponding to
the bin [70, 80).
Therefore, if A_1 represents the
area of the [50, 70) bar and A_2 represents the area of the [70, 80) bar, we have

A_1 = r \cdot A_2.

Then, since each bar is a rectangle, its area comes from the product
of its height and its base. We know the [50,
70) bar has a base of 20 and a
height of h_1, and the [70, 80) bar has a base of 10 and a height of h_2. Plugging this in gives

h_1 \cdot 20 = r \cdot h_2 \cdot 10.

From here, we can rearrange terms to get

\frac{h_1}{h_2} = \frac{r}{2}.",40.0,Hard
211,Fa,23,Midterm,,Problem 6,Problem 6,"Recall that living_cost has 31430 rows, one for each of the ten possible
""family_type"" values in each of the 3143 US counties.
Consider the function state_merge, defined below.
def state_merge(A, B):
    state_A = living_cost[living_cost.get(""state"") == A]
    state_B = living_cost[living_cost.get(""state"") == B]
    return state_A.merge(state_B, on=""family_type"").shape[0]
Suppose Montana (""MT"") has 5 counties, and suppose
state_merge(""MT"", ""NV"") evaluates to 1050. How
many counties does Nevada (""NV"") have? Give your answer as
an integer.",,36.0,Hard
212,Fa,23,Midterm,,Problem 7,Problem 7,"King Triton had four children, and each of his four children started
their own families. These four families organize a Triton family reunion
each year. The compositions of the four families are as follows:

Family W: ""1a4c""
Family X: ""2a1c""
Family Y: ""2a3c""
Family Z: ""1a1c""

Suppose we choose one of the fifteen people at the Triton family
reunion at random.",,,
213,Fa,23,Midterm,,Problem 7,Problem 7.1,"Given that the chosen individual is from a family with one child,
what is the probability that they are from Family X? Give your answer as
a simplified fraction.","\frac{3}{5}
Given that the chosen individual is from a family with one child, we
know that they must be from either Family X or Family Z. There are three
individuals in Family X, and there are a total of five individuals from
these two families. Thus, the probability of choosing any one of the
three individuals from Family X out of the five individuals from both
families is \frac{3}{5}.",43.0,Hard
214,Fa,23,Midterm,,Problem 7,Problem 7.2,"Consider the events A and B, defined below.

A: The chosen individual is an
adult.
B: The chosen individual is a
child.

True or False: Events A and B are independent.

 True
 False","False
If two events are independent, knowledge of one event happening does
not change the probability of the other event happening. In this case,
events A and B are not independent because knowledge of
one event gives complete knowledge of the other.
To see this, note that the probability of choosing a child randomly
out of the fifteen individuals is \frac{9}{15}. That is, P(B) = \frac{9}{15}.
Suppose now that we know that the chosen individual is an adult. In
this case, the probability that the chosen individual is a child is
0, because nobody is both a child and
an adult. That is, P(B \text{ given } A) =
0, which is not the same as P(B) =
\frac{9}{15}.
This problem illustrates the difference between mutually exclusive
events and independent events. In this case A and B are
mutually exclusive, because they cannot both happen. But that forces
them to be dependent events, because knowing that someone is an adult
completely determines the probability that they are a child (it’s
zero!)",33.0,Hard
215,Fa,23,Midterm,,Problem 7,Problem 7.3,"Consider the events C and D, defined below.

C: The chosen individual is a
child.
D: The chosen individual is from
family Y.

True or False: Events C and D are independent.

 True
 False","True
If two events are independent, the probability of one event happening
does not change when we know that the other event happens. In this case,
events C and D are indeed independent.
If we know that the chosen individual is a child, the probability
that they come from Family Y is \frac{3}{9}, which simplifies to \frac{1}{3}. That is P(D \text{ given } C) = \frac{1}{3}.
On the other hand, without any prior knowledge, when we select
someone randomly from all fifteen individuals, the probability they come
from Family Y is \frac{5}{15}, which
also simplifies to \frac{1}{3}. This
says P(D) = \frac{1}{3}.
In other words, knowledge of C is
irrelevant to the probability of D
occurring, which means C and D are independent.",35.0,Hard
216,Fa,23,Midterm,,Problem 7,Problem 7.4,"At the reunion, the Tritons play a game that involves placing the
four letters into a hat (W, X, Y, and Z, corresponding to the four
families). Then, five times, they draw a letter from
the hat, write it down on a piece of paper, and place it back into the
hat.
Let p = \frac{1}{4} in the questions
that follow.
What is the probability that Family W is selected all 5 times?

 p^5
 1 - p^5
 1 - (1 - p)^5
 (1 - p)^5
 p \cdot (1 - p)^4
 p^4 (1 - p)
 None of these.","p^5
The probability of selecting Family W in the first round is p, which is the same for the second round,
the third round, and so on. Each of the chosen letters is drawn
independently from the others because the result of one draw does not
affect the result of the next. We can apply the multiplication rule here
and multiply the probabilities of choosing Family W in each round. This
comes out to be p\cdot p\cdot p\cdot p\cdot
p, which is p^5.",91.0,Easy
217,Fa,23,Midterm,,Problem 7,Problem 7.5,"What is the probability that Family W is selected at least once?

 p^5
 1 - p^5
 1 - (1 - p)^5
 (1 - p)^5
 p \cdot (1 - p)^4
 p^4 (1 - p)
 None of these.","1 - (1 -
p)^5
Since there are too many ways that Family W can be selected to meet
the condition that it is selected at least once, it is easier if we
calculate the probability that Family W is never selected and subtract
that from 1. The probability that Family W is not selected in the first
round is 1-p, which is the same for the
second round, the third round, and so on. We want this to happen for all
five rounds, and since the events are independent, we can multiply their
probabilities all together. This comes out to be (1-p)^5, which represents the probability
that Family W is never selected. Finally, we subtract (1-p)^5 from 1 to find the probability that
Family W is selected at least once, giving the answer 1 - (1-p)^5.",62.0,Medium
218,Fa,23,Midterm,,Problem 7,Problem 7.6,"What is the probability that Family W is selected exactly once, as
the last family that is selected?

 p^5
 1 - p^5
 1 - (1 - p)^5
 (1 - p)^5
 p \cdot (1 - p)^4
 p^4 (1 - p)
 None of these.","p \cdot (1 -
p)^4
We want to find the probability of Family W being selected only as
the last draw, and not in the first four draws. The probability that
Family W is not selected in the first draw is (1-p), which is the same for the second,
third, and fourth draws. For the fifth draw, the probability of choosing
Family W is p. Since the draws are
independent, we can multiply these probabilities together, which comes
out to be (1-p)^4 \cdot p = p\cdot
(1-p)^4.",67.0,Medium
219,Fa,23,Midterm,,Problem 8,Problem 8,"After the family reunion, Family Y gets together with nine other
families to play a game. All ten families (which we’ll number 1 through
10) have a composition of ""2a3c"". Within each family, the
three children are labeled ""oldest"", ""middle"",
or ""youngest"".
In this game, the numbers 1 through 10, representing the ten
families, are placed into a hat. Then, five times, they
draw a number from the hat, write it down on a piece of paper, and place
it back into the hat. If a family’s number is written down on the paper
at least twice, then two of the three children in that family are
randomly selected to win a prize. The same child cannot be selected to
win a prize twice.
Chiachan is the middle child in Family 4. He writes a simulation,
which is partially provided on the next page. Fill in the blanks so that
after running the simulation,

np.count_nonzero(outcomes == ""Outcome Q"") / repetitions
gives an estimate of the probability that Chiachan wins a
prize.
np.count_nonzero(outcomes == ""Outcome R"") / repetitions
gives an estimate of the probability that both of Chiachan’s siblings
win a prize, but Chiachan does not.
np.count_nonzero(outcomes == ""Outcome S"") / repetitions
gives an estimate of the probability that nobody from Chiachan’s family
wins a prize.


ages = np.array([""oldest"", ""middle"", ""youngest""])
outcomes = np.array([])
repetitions = 10000
for i in np.arange(repetitions):
    fams = np.random.choice(np.arange(1, 11), 5, ____(a)____)
    if ____(b)____:
        children = np.random.choice(ages, 2, ____(c)____)
        if not ""middle"" in children:
            outcomes = np.append(outcomes, ____(d)____)
        else:
            outcomes = np.append(outcomes, ____(e)____)
    else:
        outcomes = np.append(outcomes, ____(f)____)",,,
220,Fa,23,Midterm,,Problem 8,Problem 8.1,"What goes in blank (a)?

 replace=True
 replace=False","replace=True
A family can be selected more than once, as indicated by “placing the
number back into the hat” in the problem statement. Therefore we use
replace=True to allow for the same family to get picked
more than once.",88.0,Easy
221,Fa,23,Midterm,,Problem 8,Problem 8.2,What goes in blank (b)?,"np.count_nonzero(fams == 4) >= 2 or equivalent
Notice that inside the body of the if statement, the
first line defines a variable children which selects two
children from among ages. We are told in the problem
statement that if a family’s number is written down on the paper at
least twice, then two of the three children in that family are randomly
selected to win a prize. Therefore, the condition that we want to check
in the if statement should correspond to Chiachan’s family
number (4) being written down on the paper at least twice.
When we compare the entire fams array to the value 4
using fams == 4, the result is an array of
True or False values, where each
True represents an instance of Chiachan’s family being
chosen. Then np.count_nonzero(fams == 4) evaluates to the
number of Trues, because in Python, True is 1
and False is 0. That is,
np.count_nonzero(fams == 4) represents the number of times
Chichan’s family is selected, and so our condition is
np.count_nonzero(fams == 4) >= 2.
There are many equivalent ways to write this same condition,
including np.count_nonzero(fams == 4) > 1 and
(fams == 4).sum() >= 2.",17.0,Hard
222,Fa,23,Midterm,,Problem 8,Problem 8.3,"What goes in blank (c)?

 replace=True
 replace=False","replace=False
A child cannot win a prize twice, so we remove them from the pool
after being selected.",86.0,Easy
223,Fa,23,Midterm,,Problem 8,Problem 8.4,"What goes in blank (d)?

 ""Outcome Q""
 ""Outcome R""
 ""Outcome S""","""Outcome R""
Chiachan is the middle child in the family, and recall that each
outcome corresponds to either Chiachan winning
(""Outcome Q""), Chiachan not winning but his siblings
winning (""Outcome R""), or nobody in his family winning
(""Outcome S"").
This condition checks the negation of the middle child being
selected, which evaluates to True when Chiachan’s siblings
win but he doesn’t, so we append ""Outcome R"" to the
outcomes array in this case.",76.0,Easy
224,Fa,23,Midterm,,Problem 8,Problem 8.5,"What goes in blank (e)?

 ""Outcome Q""
 ""Outcome R""
 ""Outcome S""","""Outcome Q""
Chiachan is the middle child in the family, and recall that each
outcome corresponds to either Chiachan winning
(""Outcome Q""), Chiachan not winning but his siblings
winning (""Outcome R""), or nobody in his family winning
(""Outcome S"").
This condition corresponds to the middle child being selected, so we
append ""Outcome Q"" to the outcomes array in
this case.",,
225,Fa,23,Midterm,,Problem 8,Problem 8.6,"What goes in blank (f)?

 ""Outcome Q""
 ""Outcome R""
 ""Outcome S""","""Outcome S""
Chiachan is the middle child in the family, and recall that each
outcome corresponds to either Chiachan winning
(""Outcome Q""), Chiachan not winning but his siblings
winning (""Outcome R""), or nobody in his family winning
(""Outcome S"").
This condition is that Chichan’s family was not selected two or more
times, which means nobody in his family will win a prize, so we append
""Outcome S"" to the outcomes array in this
case.",80.0,Easy
226,Fa,24,Midterm,,Problem 1,Problem 1,"Which of the following columns would be an appropriate index for the
treat DataFrame?

 ""address""
 ""candy""
 ""neighborhood""
 None of these.","None of these.
The index uniquely identifies each row of a DataFrame. As a result,
for a column to be a candidate for the index, it must not contain repeat
items. Since it is possible for an address to give out different types
of candy, values in ""address"" can show up multiple times.
Similarly, values in ""candy"" can also show up multiple
times as it will appear anytime a house gives it out. Finally, a
neighborhood has multiple houses, so if more than one of those houses
show up, that value in ""neighborhood"" will appear multiple
times. Since ""address"", ""candy"", and
""neighborhood"" can potentially have repeat values, none of
them can be the index for treat.",54.0,Medium
227,Fa,24,Midterm,,Problem 2,Problem 2,"Which of the following expressions evaluate to
""M&M""? Select all that apply.

 treat.get(""candy"").iloc[1]
 treat.sort_values(by=""candy"", ascending = False).get(""candy"").iloc[1]
 treat.sort_values(by=""candy"", ascending = False).get(""candy"").loc[1]
 treat.set_index(""candy"").index[-1]
 None of these.","treat.get(""candy"").iloc[1] and
treat.sort_values(by=""candy"", ascending = False).get(""candy"").loc[1]

Option 1:
treat.get(""candy"").iloc[1] gets the candy
column and then retrieves the value at index location 1,
which would be ""M&M"".
Option 2:
treat.sort_values(by=""candy"", ascending=False).get(""candy"").iloc[1]
sorts the candy column in descending order (alphabetically,
the last candy is at the top) and then retrieves the value at index
location 1 in the candy column. The entire
dataset is not shown, but in the given rows, the second-to-last candy
alphabetically is ""Skittles"", so we know that
""M&M"" will not be the second-to-last alphabetical candy
in the full dataset.
Option 3:
treat.sort_values(by=""candy"", ascending=False).get(""candy"").loc[1]
is very similar to the last option; however, this time,
.loc[1] is used instead of .iloc[1]. This
means that instead of looking at the row in position 1
(second row) of the sorted DataFrame, we are finding the row with an
index label of 1. When the rows are sorted by
candy in descending order, the index labels remain with
their original rows, so the ""M&M"" row is retrieved when
we search for the index label 1.
Option 4:
treat.set_index(""candy"").index[-1] sets the index to the
candy column and then retrieves the last element in the
index (candy). The entire dataset is not shown, but in the
given rows, the last value would be ""Skittles"" and not
""M&M"". The last value of the full dataset could be
""M&M"", but since we are not sure, this option is not
selected.",66.0,Medium
228,Fa,24,Midterm,,Problem 3,Problem 3,"Consider the code below.
street = treats.get(""address"").str.contains(""Street"")
sour = treats.get(""candy"").str.contains(""Sour"")",,,
229,Fa,24,Midterm,,Problem 3,Problem 3.1,"What is the data type of street?

 int
 bool
 str
 Series
 DataFrame","Series
.str.contains works in a series and returns a series of booleans.
Each entry is True if it contains a certain string or
False otherwise. So the answer is street has
the Series data type.",,
230,Fa,24,Midterm,,Problem 3,Problem 3.2,"What does the following expression evaluate to? Write your answer
exactly how the output would appear in Python.
np.count_nonzero(street & sour) > sour.sum()","False
np.count_nonzero(street & sour) counts the number of
rows that contains the word “Street” in the address column
AND also contains the word “Sour” in candy.
sour.sum() sums up all the trues and falses, effectively
making it a count of rows that contain the word “Sour” in
candy. Even if we don’t know the full dataframe, we should
be able to figure out that the number of rows that satisfy the condition
of both Street AND Sour should be lower than
or equal to the number of rows that satisfy Sour by itself.
Therefore, it’s impossible for
np.count_nonzero(street & sour) > sour.sum() to be
True so the answer is False.",59.0,Medium
231,Fa,24,Midterm,,Problem 4,Problem 4,"The ""address"" column contains quite a bit of
information. All houses are in ""San Diego, CA"", but the
street address and the zip code vary. Note that the “street address""
includes both the house number and street name, such as
""820 Opal Street"". All addresses are formatted in the same
way, for example,
""820 Opal Street, San Diego, CA, 92109"".",,,
232,Fa,24,Midterm,,Problem 4,Problem 4.1,"Fill in the blanks in the function address_part below.
The function has two inputs: a value in the index of treat
and a string part, which is either ""street"" or
""zip"". The function should return the appropriate part of
the address at the given index value, as a string. Example behavior is
given below.
>>> address_part(4, ""street"")
""8575 Jade Coast Drive""

>>> address_part(1, ""zip"")
""92109""
The function already has a return statement included. You should not
add the word return anywhere else!
def address_part(index_value, part):
    if part == ""street"":
        var = 0
    else:
        ___(a)___
    return treat.get(""address"").loc[___(b)___].___(c)___","(a): var = 3, var = -1 or alternate
solution var = 1
(b): index_value
(c): split("", "")[var] or alternate solution
split("", San Diego, CA, "")[var]",58.0,Medium
233,Fa,24,Midterm,,Problem 4,Problem 4.2,"Suppose we had a different function called zip_as_int
that took as input a single address, formatted exactly as the addresses
in treat, and returned the zip code as an int.
Write a Python expression using the zip_as_int function
that evaluates to a Series with the zip codes of all the addresses in
treat.","treat.get(""address"").apply(zip_as_int)",76.0,Easy
234,Fa,24,Midterm,,Problem 5,Problem 5,"Write a Python expression that evaluates to the address of the house
with the most pieces of candy available (the most
pieces, not the most varieties).
It’s okay if you need to write on multiple lines, but your code
should represent a single expression in Python.","treat.groupby(""address"").sum().sort_values(by=""how_many"", ascending = False).index[0]
or
treat.groupby(""addresss"").sum().sort_values(by=""how_many"").index[-1]
In the treat DataFrame, there are multiple rows for each
address, one for each candy they are giving out with their quantity.
Since we want the address with the most pieces of candy available, we
need to combine this information, so we start by grouping by address:
treat.groupby(“address”). Now, since we want to add the
number of candy available per address, we use the sum()
aggregate function. So now we have a DataFrame with one row per address
where there value in each column is the sum of all the values. To get
the address with the most pieces of candy available, we can simply sort
by the “how_many” column since this stores the total amount
of candy per house. Setting ascending=False means that the
address with the greatest amount of candy will be the first row. Since
the addresses are located in the index as a result of the
groupby, we can access this value by using
index[0].
Note: If you do not set ascending=False, then the
address with the most amount of candy available will be the last row
which you can access by index[-1].",67.0,Medium
235,Fa,24,Midterm,,Problem 6,Problem 6,"Suppose you visit a house that has 40 Twix, 50 M&Ms, and 10
KitKats in a bowl. You take three pieces of candy from this bowl.",,,
236,Fa,24,Midterm,,Problem 6,Problem 6.1,"What is the probability you get all Twix?

 \dfrac{40}{100} \cdot \dfrac{39}{100} \cdot
\dfrac{38}{100}
 \dfrac{40}{100} \cdot \dfrac{40}{99} \cdot
\dfrac{40}{98}
 \dfrac{40}{100} \cdot \dfrac{40}{100} \cdot
\dfrac{40}{100}
 \dfrac{40}{100} \cdot \dfrac{39}{99} \cdot
\dfrac{38}{98}","\dfrac{40}{100}
\cdot \dfrac{39}{99} \cdot \dfrac{38}{98}
We need to find the probability that we get all Twix among the three
candies selected from the bowl. Since we are selecting three times from
the same bowl, we know that we are selecting without
replacement.

First Selection:

There are 40 Twix and 40 + 50 + 10 = 100 candies in the bowl,
meaning the probability of selecting a Twix is \frac{40}{100}.

Second Selection:

Now that we have chosen one Twix there are 39 Twix and 99 candies
left, meaning that the probability of selecting a Twix now is \frac{39}{99}.

Third Selection:

After selecting two Twix there are 38 Twix and 98 candies left,
meaning the probability of selecting a Twix is \frac{38}{98}.


The total probability that we grab all Twix from the bowl is the
product of these probabilities: \frac{40}{100} \cdot \frac{39}{99} \cdot
\frac{38}{98}",94.0,Easy
237,Fa,24,Midterm,,Problem 6,Problem 6.2,"What is the probability you get no Twix? Leave your answer
completely unsimplified, similar to the answer choices
for part (a).","\dfrac{60}{100}
\cdot \dfrac{59}{99} \cdot \dfrac{58}{98}
We need to find the probability that we get no Twix among the three
candies selected from the bowl. We know that two candies are not Twix in
our bowl (M&Ms and Kitkats). Since we are selecting three times from
the same bowl, we know that we are selecting without
replacement.

First Selection:

There are 60 non-Twix candies in the bowl (50 M&Ms and 10
Kitkats) and 100 total candies. This means the probability of selecting
a non-Twix is \frac{60}{100}.

Second Selection:

Regardless of which non-Twix candy was chosen, there are now 59
non-Twix candies in the bowl (49 M&Ms and 10 Kitkats OR 50 M&Ms
and 9 Kitkats). Since there are 99 total candies in the bowl, the
probability of selecting a non-Twix is \frac{59}{99}.

Third Selection:

After selecting two non-Twix there are 58 non-Twix and 98 total
candies left meaning the probability of selecting a non-Twix is \frac{58}{98}.


The total probability that we grab no Twix from the bowl is the
product of these probabilities: \frac{60}{100} \cdot \frac{59}{99} \cdot
\frac{58}{98}",81.0,Easy
238,Fa,24,Midterm,,Problem 6,Problem 6.3,"Let a be your answer to part (a) and
let b be your answer to part (b). Write
a mathematical expression in terms of a
and/or b that evaluates to the
probability of getting some Twix and some non-Twix candy from this
house.","1 - a - b
or 1 - (a + b)
The case where we get some Twix and some non-Twix occurs can also be
thought of as the case when we DO NOT get either all
Twix OR all non-Twix. In 6.1 we calculated the
probability of getting all Twix as a
and in 6.2 we calculated the probability of getting all non-Twix as
b. Therefore the probability of getting
either all Twix OR all non-Twix is equal to a + b. However, we are looking for the
probability that this does not happen, meaning our
answer is 1 - (a + b).",30.0,Hard
239,Fa,24,Midterm,,Problem 7,Problem 7,"Suppose you visit another house and their candy bowl is composed of 2
Twix, 3 Rolos, 1 Snickers, 3 M&Ms, and 1 KitKat. You do the same as
before and take 3 candies from the bowl at random.
Fill in the blanks in the code below so that
prob_all_same evaluates to an estimate of the probability
that you get three of the same type of candy.
    candy_bowl = np.array([""Twix"", ""Twix"", ""Rolo"", ""Rolo"", ""Rolo"", ""Snickers"", ""M&M"", ""M&M"", ""M&M"", ""KitKat""])

    repetitions = 10000
    prob_all_same = 0
    for i in np.arange(repetitions):
        grab = np.random.choice(___(a)___)
        if ___(b)___:
            prob_all_same = prob_all_same + 1
    prob_all_same = ___(c)___",,,
240,Fa,24,Midterm,,Problem 7,Problem 7.1,"What goes in blank (a)?

 candy_bowl, len(candy_bowl), replace=False
 candy_bowl, 3, replace=False
 candy_bowl, 3, replace=True
 candy_bowl, repetitions, replace=True","candy_bowl, 3, replace=False
The question asks us to “take 3 candies from the bowl at random.” In
this part, we need to sample 3 candies at random using
np.random.choice. Now, we evaluate each option one by one
as follows:

candy_bowl, len(candy_bowl), replace=False: The code
tries to sample all candies without replacement. However, we are asked
to only sample three candies, not all.
candy_bowl, 3, replace=False: The code samples three
candies without replacement, which matches the description. This option
is correct.
candy_bowl, 3, replace=True: The code samples three
candies from the bowl with replacement. Under this setting, the same
candy can be selected multiple times in a single grab, which is not
realistic.
candy_bowl, repetitions, replace=True: This option
attempts to sample repetitions (10,000) candies in a single
grab. We are asked to sample three candies per iteration of the loop,
not thousands.",88.0,Easy
241,Fa,24,Midterm,,Problem 7,Problem 7.2,"What goes in blank (b)?

 grab[0] == ""Rolo"" and grab[1] == ""Rolo"" and grab[2] == ""Rolo""
 grab[0] == grab[1] and grab[0] == grab[2]
 grab[0] == grab[1] or grab[0] == grab[2]
 grab == ""Rolo"" | grab == ""M&M""","grab[0] == grab[1] and grab[0] == grab[2]
Here, we need condition that checks if all three candies selected in
the grab are the same. We now analyze each option as follows:

grab[0] == ""Rolo"" and grab[1] == ""Rolo"" and grab[2] == ""Rolo"":
This condition explicitly checks if all three candies are “Rolo”. While
it ensures that the three candies are the same, it only works for “Rolo”
and not for other types of candy in the bowl (e.g., “Twix,”
“M&M”).
grab[0] == grab[1] and grab[0] == grab[2]: This
condition checks if the first candy (grab[0]) is the same as the second
(grab[1]) and the third (grab[2]). If all three candies are the same
type (regardless of which type), this condition will evaluate to True.
Otherwise, the expression will evaluate to False, which is what we need.
The option is correct.
grab[0] == grab[1] or grab[0] == grab[2]: This
condition checks if the first candy (grab[0]) matches either the second
(grab[1]) or the third (grab[2]). It does not require all three candies
to be the same. For example, if grab = [“Twix”, “Twix”, “M&M”], this
condition would incorrectly evaluate to True.
grab == ""Rolo"" | grab == ""M&M"": This condition
is syntactically invalid. It tries to compare the grab list (which
contains three elements) with two strings (“Rolo” and “M&M”) using a
bitwise OR (|), not to mention that it does not check if three candies
are the same.",92.0,Easy
242,Fa,24,Midterm,,Problem 7,Problem 7.3,"What goes in blank (c)?

 prob_all_same.mean()
 prob_all_same / len(candy_bowl)
 prob_all_same / repetitions
 prob_all_same / 3","prob_all_same / repetitions
To calculate the estimated probability of drawing three candies of
the same type, we divide the total number of successes
(prob_all_same, which counts the instances where all three
candies are identical) by the total number of iterations
(repetitions).
The option prob_all_same.mean() is incorrect because
prob_all_same is an integer that accumulates the count of
successful trials, not an array or list that supports the
.mean() method. Similarly, dividing by
len(candy_bowl) or 3 is incorrect, as neither
represents the total number of iterations. Therefore, using these values
as the denominator would not provide an accurate probability
estimate.",86.0,Easy
243,Fa,24,Midterm,,Problem 8,Problem 8,"Select the correct way to fill in the blank such that the code below
evaluates to True.
treat.groupby(______).mean().shape[0] == treat.shape[0]

 ""address""
 ""candy""
 ""neighborhood""
 [""address"", ""candy""]
 [""candy"", ""neighborhood""]
 [""address"", ""neighborhood""]","[""address"", ""candy""]
.shape returns a tuple containing the number of rows and
number of columns of a DataFrame respectively. By indexing
.shape[0] we get the number of rows. In the above question,
we are comparing whether the number of rows of treat
grouped by its column(s) is equal to the number of rows of the original
treat itself. This is only possible when there is a unique
row for each value in the column or for each combination of columns.
Since it is possible for an address to give out different types of
candy, values in ""address"" can show up multiple times.
Similarly, values in ""candy"" can also show up multiple
times since more than one house may give out a specific candy. A
neighborhood has multiple houses, so if a neighborhood has more than one
house, ""neighborhood"" will appear multiple times.
% write for combinations here % Each address gives out a specific
candy only once, and hence [""address"", ""candy""] would have
a unique row for each combination. This would make the number of rows in
the grouped DataFrame equal to treat itself. Multiple
neighborhoods might be giving out the same candy or a single
neighborhood could be giving out multiple candies, so
[""candy"", ""neighborhood""] is not the answer. Finally, a
neighborhood can have multiple addresses, but each address could be
giving out more than one candy, which would mean this combination would
occur multiple times in treat, which means this would also
not be an answer. Since [""address"", ""candy""] is the only
combination that gives a unique row for each combination, the grouped
DataFrame would contain the same number of rows as treat
itself.",69.0,Medium
244,Fa,24,Midterm,,Problem 9,Problem 9,"Assume that all houses in treat give out the same size
candy, say fun-sized. Suppose we have an additional DataFrame,
trick, which is indexed by ""candy"" and has one
column, ""price"", containing the cost in dollars of a
single piece of fun-sized candy, as a
float.
Suppose that:

treat has 200 rows total, and includes 15 distinct
types of candies.
trick has 25 rows total: 15 for the candies that
appear in treat, plus 10 additional rows that correspond to
candies not represented in treat.

Consider the following line of code:
trick_or_treat = trick.merge(treat, left_index = True, right_on = ""candy"")
How many rows does trick_or_treat have?

 15
 25
 200
 215
 225
 3000
 5000","200
We are told that trick has 25 rows: 15 from candies that
are in treat and 10 additional candies. This means that
each candy in trick appears exactly once because 15+10= 25.
In addition, a general property when merging dataframes is that the
number of rows for one shared value between the dataframes is the
product of the number of occurences in either dataframe. For example, if
Twix occurs 5 times in treat, the number of times it occurs
in trick_or_treat is 5 * 1 = 5 (it occurs once in
trick). Using this logic, we can determine how many rows
are in trick_or_treat. Since each number of candies is
multipled by one and they sum up to 200, the number of rows will be
200.",39.0,Hard
245,Fa,24,Midterm,,Problem 10,Problem 10,"Recall from the last problem that the DataFrame
trick_or_treat includes a column called
""price"" with the cost in dollars of a single
piece of fun-sized candy, as a float.
Assume we have run the line of code tot = trick_or_treat
to reassign trick_or_treat to the shorter variable name
tot.
In this problem, we’ll use tot to calculate the total
amount of money that each house spent on Halloween candy. This number is
always less than \$80 for the houses in
our data set.",,,
246,Fa,24,Midterm,,Problem 10,Problem 10.1,"Fill in the blanks below so that the following block of code plots a
histogram that displays the distribution of the total amount of money
that houses spent on Halloween candy, in dollars.
total = (tot.assign(total_spent = ___(a)___)
            .groupby(___(b)___).___(c)___)
total.plot(kind = ""hist"",  y = ""total_spent"", density = True,
           bins = np.arange(0, 90, 10))
            ","(a): tot.get(""price"") * tot.get(""how_many"")
(b): “address”
(c): sum()

(a):
tot.get(""price"") * tot.get(""how_many"")

tot.get(""price"") retrieves the cost of a single piece
of candy.
tot.get(""how_many"") retrieves the number of pieces of
candy given out.
Multiplying these two columns calculates the total amount spent on
candy for each row in the dataset.
This step creates a new column total_spent that
represents the total money spent for each type of candy at a given
house.

(b): “address”

The data is grouped by the ""address"" column, which
uniquely identifies each house. This ensures that all records associated
with a single house are aggregated together.

(c): sum()

After grouping by ""address"", the .sum()
operation aggregates the total amount of money spent on candy for each
house. This sums up all total_spent values for records
belonging to the same house.

Final Output: The total DataFrame will have one row for
each house, with the column total_spent representing the
total money spent on Halloween candy. Finally, the
total.plot command creates a histogram of the
total_spent values to visualize the distribution of
spending across houses.",65.0,Medium
247,Fa,24,Midterm,,Problem 10,Problem 10.2,"Which two adjacent bins in the histogram represent about 50\% of the houses?

 [10, 20) and [20, 30)
 [20, 30) and [30, 40)
 [30, 40) and [40, 50)
 [40, 50) and [50, 60)
 [50, 60) and [60, 70)
 Not possible to determine.","[20, 30) and
[30, 40)

The histogram shows that the bins [20, 30) and
[30, 40) have the two tallest bars, with heights of 0.020
and 0.030, respectively.
Each bar’s height represents the density of data in that range
(proportion of houses divided by bin width). Since the bin width is 10,
we can multiply the height by 10 to calculate the proportion of data in
each bin:

[20, 30) contributes 0.020
\times 10 = 0.2 or 20\% of the
houses.
[30, 40) contributes 0.030
\times 10 = 0.3 or 30\% of the
houses.

Together, these two bins account for 20\%
+ 30\% = 50\% of the houses.",83.0,Easy
248,Fa,24,Midterm,,Problem 10,Problem 10.3,"Suppose we create a new histogram, using the same code as above but
with bins = np.arange(0, 90, 20) instead of
bins = np.arange(0, 90, 10). Approximate the height of the
tallest bar in this new histogram. If this is not possible, write “Not
possible to determine.""","0.025

With the new bin width of 20, the histogram combines adjacent bins
from the original histogram. The new bins become
[0, 20),[20, 40),[40, 60),[60, 80).
The bin [20, 40) merges the original bins
[20, 30) and [30, 40) and would be the bin
with the highest bar in the new histogram.
To find the total proportion of data in [20, 40):

From the original histogram:

[20, 30) contributes 0.020
\times 10 = 0.2 (20%).
[30, 40) contributes 0.030
\times 10 = 0.3 (30%).

Total for [20, 40) is 0.2 +
0.3 = 0.5 or 50\%.

The new bin width is 20, so the height of the bar is calculated as:
Height = \frac{\text{Proportion}}{\text{Bin
Width}} = \frac{0.5}{20} =
0.025

Therefore, the tallest bar in the new histogram has a height of
0.025.",38.0,Hard
249,Fa,24,Midterm,,Problem 10,Problem 10.4,"Suppose we create a new histogram, using the same code as above but
substituting bins = np.arange(0, 90, 5) for
bins = np.arange(0, 90, 10). Approximate the height of the
tallest bar in this new histogram. If this is not possible, write “Not
possible to determine.""","Not possible to determine.

In the original histogram, the bins are 10 units wide (e.g.,
[20, 30)). When switching to 5-unit bins (e.g.,
[20, 25), [25, 30)), we need to know the
distribution of data within the original 10-unit bins to calculate the
new bar heights.
The histogram does not provide this detailed information. For
example, we cannot determine whether the data in [20, 30)
is evenly distributed between [20, 25) and
[25, 30) or concentrated in one of the sub-bins.
Without this additional information, it is impossible to approximate
the height of the tallest bar accurately.",70.0,Medium
250,Fa,24,Midterm,,Problem 11,Problem 11,"As in the last problem, we’ll continue working with the
tot DataFrame that came from merging trick
with treat. The ""price"" column contains the
cost in dollars of a single piece of fun-sized candy,
as a float.
In this problem, we want to use tot to calculate the
average cost per piece of Halloween candy at each
house. For example, suppose one house has 30 Twix, which cost \$0.20 each, and 20 Laffy Taffy, which cost
\$0.10 each. Then this house spent
\$8.00 on 50 pieces of candy, for an
average cost of \$0.16 per piece.
Which of the following correctly sets ac to a DataFrame
indexed by ""address"" with a column called
""avg_cost"" that contains the average cost per piece of
Halloween candy at each address? Select all that apply.
Way 1:
ac = tot.groupby(""address"").sum()
ac = ac.assign(avg_cost = ac.get(""price"") / 
                          ac.get(""how_many"")).get([""avg_cost""])
Way 2:
ac = tot.assign(x = tot.get(""price"") / tot.get(""how_many""))
ac = ac.groupby(""address"").sum()
ac = ac.assign(avg_cost = ac.get(""x"").mean()).get([""avg_cost""])
Way 3:
ac = tot.assign(x = tot.get(""price"") / tot.get(""how_many""))
ac = ac.groupby(""address"").sum()
ac = ac.assign(avg_cost = ac.get(""x"") / 
                          ac.get(""how_many"")).get([""avg_cost""])
Way 4:
ac = tot.assign(x = tot.get(""how_many"") * tot.get(""price""))
ac = ac.groupby(""address"").sum()
ac = ac.assign(avg_cost = ac.get(""x"").mean()).get([""avg_cost""])
Way 5:
ac = tot.assign(x = tot.get(""how_many"") * tot.get(""price""))
ac = ac.groupby(""address"").sum()
ac = ac.assign(avg_cost = ac.get(""x"") / 
                          ac.get(""how_many"")).get([""avg_cost""])

 Way 1
 Way 2
 Way 3
 Way 4
 Way 5","Option 5
We need the average cost per piece at each house.
The correct formula would be: (total spent on candy) / (total pieces
of candy)
Let’s go through each Way and assess if it is valid or not.
Way 1: When we sum the “price” column directly,
we’re summing the per-piece prices, not the total spent. This gives
wrong totals. For example, if a house has 30 pieces at $0.20 and 20 at
$0.10, summing prices gives $0.30 instead of $8.00.
Way 2: This first calculates price/quantity for each
candy type, then takes the mean of these ratios. This is mathematically
incorrect for finding average cost per piece.

For Twix: $0.20/30 = $0.00667 per piece
For Laffy Taffy: $0.10/20 = $0.005 per piece
Takes mean: ($0.00667 + $0.005)/2 = $0.00583
This is wrong because it’s taking mean of ratios instead of ratio of
totals

Way 3: Similar to Way 2, but even more problematic
as it divides by quantity twice.

For Twix: $0.20/30 = $0.00667
For Laffy Taffy: $0.10/20 = $0.005
Sums these: $0.00667 + $0.005 = $0.01167
Divides by total quantity again: $0.01167/50 = $0.000233

Way 4: Correctly calculates total spent (x =
quantity * price) but then takes the mean of the totals instead of
dividing by total quantity.

For Twix: 30 × $0.20 = $6.00
For Laffy Taffy: 20 × $0.10 = $2.00
Takes mean of these totals: ($6.00 + $2.00)/2 = $4.00 (wrong)
This is wrong because it takes mean of totals instead of dividing by
total quantity

Way 5: This is correct because:

First calculates total spent on each candy type (quantity * price
per piece)
Groups by address and sums both the total spent and total
quantities
Finally divides total spent by total pieces to get average cost per
piece

Using our example:

30 Twix at $0.20 = $6.00
20 Laffy Taffy at $0.10 = $2.00
Total spent = $8.00
Total pieces = 50
Average = $8.00/50 = $0.16 per piece, the correct answer.",71.0,Medium
251,Fa,24,Midterm,,Problem 12,Problem 12,,,,
252,Fa,24,Midterm,,Problem 12,Problem 12.1,"What would be the best type of plot to visualize the distribution of
""neighborhood"" among the houses represented in
treat?

 scatter plot
 line plot
 bar chart
 histogram",bar chart,76.0,Easy
253,Fa,24,Midterm,,Problem 12,Problem 12.2,"Suppose we had access to historical data about the price of fun-sized
candies over time. If we wanted to compare the prices of Milky Way and
Skittles over time, which would be the best type of visualization to
plot?

 overlaid scatter plot
 overlaid line plot
 overlaid bar chart
 overlaid histogram",overlaid line plot,90.0,Easy
254,Fa,24,Midterm,,Problem 13,Problem 13,"Extra Credit
Define the variable double as follows.
double = treat.groupby(""candy"").count().groupby(""address"").count()
Now, suppose you know that
double.loc[1].get(""how_many"") evaluates to
5.
Which of the following is a valid interpretation of this information?
Select all that apply.

 There are five houses that are each giving out only one type of
candy.
 There are five types of candy that are each being given out by only
one house.
 There is only one house that is giving out five types of candy.
 There is only one type of candy that is being given out by five
houses.
 None of these.","Option 2
Let’s approach this solution by breaking down the line of code into
two intermediate steps, so that we can parse them one at a time: -
intermediate_one = treat.groupby(""candy"").count() -
double = intermediate_one.groupby(""address"").count()
Step 1:
intermediate_one = treat.groupby(""candy"").count()
The first of our two operations groups the treat
DataFrame by the ""candy"" column, and aggregates using the
.count() method. This creates an output DataFrame that is
indexed by ""candy"", where the values in each column
represent the number of times each candy appeared in the
treat DataFrame.
Remember, in our original DataFrame, each row represents one type of
candy being given out by one house. So, each row in
intermediate_one will contain the number of houses
giving out each candy. For example, if the values in the
columns in the row with row label Milky Way were all 3, it would mean that there are 3 houses giving out Milky Ways.
Step 2:
double = intermediate_one.groupby(""address"").count()
The second of our two operations groups the
intermediate_one DataFrame by the ""address""
column, and aggregates using the .count() method. This
creates an output DataFrame that is indexed by ""address"",
where the values in each column represent the number of times that each
value in the address column appeared in the
intermediate_one DataFrame. However, these are more
difficult to interpret, so let’s break down what this means in the
context of our problem.
The values in the intermediate_one DataFrame represent
how many houses are giving out a specific type of candy (this is the
result of our first operation). So, when we group by these values, the
resulting groups will be defined by all candies that are given out by
the same number of houses. For example, if the values in the columns
with row label 5 were all 2, it would mean that there are 2 types of candy that are being given out by
5 houses. More concretely, this would
mean that the value 5 showed up 2 times in the intermediate_one
DataFrame, which means there must have been 2 candies that were being given out by 5 houses (see above).
Combining these two results, we can interpret the output of our
original line of code:
double = treat.groupby(""candy"").count().groupby(""address"").count()
outputs a DataFrame where the value in each row represents the number of
different candies that are being given out by the same number of
houses.
Now, we can easily interpret this line of code:
double.loc[1].get(""how_many"") evaluates to
5.
This means that there are 5
different types of candies that are being given out by only 1 house. This corresponds to Option
2 and only Option 2 in our
answer choices, so Option 2 is the correct answer.",15.0,Hard
255,Wi,21,Midterm,,Problem 1,Problem 1,"Note: This problem is out of scope; it
covers material no longer included in the course.
Which of the following questions could not be answered by running a
randomized controlled experiment?

 Does eating citrus fruits increase the risk of heart disease?
 Do exams with integrity pledges have fewer reported cases of academic
dishonesty?
 Does rewarding students for good grades improve high school
graduation rates?
 Does drug abuse lead to a shorter life span?","Does drug abuse lead to a shorter life
span?
It would be unethical to try to run a randomized controlled
experiment to address the question of whether drug abuse leads to a
shorter life span, as this would involve splitting participants into
groups and telling one group to abuse drugs. This is problematic because
we know drug abuse brings about a host of problems, so we could not
ethically ask people to harm themselves.
Notice that the first proposed study, about the impacts of citrus
fruits on heart disease, does not involve the same kind of ethical
dilemma because we’re not forcing people to do something known to be
harmful. A randomized controlled experiment would involve splitting
participants into two groups and asking one group to eat citrus fruits,
and measuring the heart health of both groups. Since there are no known
harmful effects of eating citrus fruits, there is no ethical issue.
Similarly, we could run a randomized controlled trial by giving an
exam where some students had to sign an integrity pledge and others
didn’t, tracking the number of reported dishonesty cases in each group.
Likewise, we could reward some students for good grades and not others,
and keep track of high school graduation rates in each group. Neither of
these studies would involve knowingly harming people and could
reasonably be carried out.",85.0,Easy
256,Wi,21,Midterm,,Problem 2,Problem 2,"You are given a DataFrame called sports, indexed by
'Sport' containing one column,
'PlayersPerTeam'. The first few rows of the DataFrame are
shown below:



Sport
PlayersPerTeam","sports.index[1]
We are told that the DataFrame is indexed by 'Sport' and
'basketball' is one of the elements of the index. To access
an element of the index, we use .index to extract the index
and square brackets to extract an element at a certain position.
Therefore, sports.index[1] will evaluate to
'basketball'.
The first two answer choices attempt to use .loc or
.iloc directly on a DataFrame. We typically use
.loc or .iloc on a Series that results from
using .get on some column. Although we don’t typically do
it this way, it is possible to use .loc or
.iloc directly on a DataFrame, but doing so would produce
an entire row of the DataFrame. Since we want just one word,
'basketball', the first two answer choices must be
incorrect.
The last answer choice is incorrect because we can’t use
.get with the index, only with a column. The index is never
considered a column.",88.0,Easy
257,Wi,21,Midterm,,Problem 3,Problem 3,"The following is a quote from The New York Times’ The
Morning newsletter.
As Dr. Ashish Jha, the dean of the Brown University School of
Public Health, told me this weekend: “I don’t actually care about
infections. I care about hospitalizations and deaths and long-term
complications.” 
By those measures, all five of the vaccines — from Pfizer,
Moderna, AstraZeneca, Novavax and Johnson & Johnson — look extremely
good. Of the roughly 75,000 people who have received one of the five in
a research trial, not a single person has died from Covid, and only a
few people appear to have been hospitalized. None have remained
hospitalized 28 days after receiving a shot.
To put that in perspective, it helps to think about what Covid has
done so far to a representative group of 75,000 American
adults: It has killed roughly 150 of them and sent several
hundred more to the hospital. The vaccines reduce those numbers to zero
and nearly zero, based on the research trials.
Zero isn’t even the most relevant benchmark. A typical U.S. flu
season kills between five and 15 out of every 75,000 adults and
hospitalizes more than 100 of them.
Why does the article use a representative group of 75,000 American
adults?

 Convention. Rates are often given per 75,000 people.
 Comparison. It allows for quick comparison against the group of
people who got the vaccine in a trial.
 Comprehension. Readers should have a sense of the scale of 75,000
people.
 Arbitrary. There is no particular reason to use a group of this
size.","Comparison. It allows for quick comparison
against the group of people who got the vaccine in a trial.
The purpose of the article is to compare Covid outcomes among two
groups of people: the 75,000 people who got the vaccine in a research
trial and a representative group of 75,000 American adults. Since 75,000
people got the vaccine in a research trial, we need to compare
statistics like number of deaths and hospitalizations to another group
of the same size for the comparison to be meaningful.
There is no convention about using 75,000 for rates. This number is
used because that’s how many people got the vaccine in a research trial.
If a different number of people had been vaccinated in a research trial,
the article would have taken that number of adults in their
representative comparison group.
75,000 is quite a large number and most people probably don’t have a
sense of the scale of 75,000 people. If the goal were comprehension, it
would have made more sense to use a smaller number like 100 people.
The number 75,000 is not arbitrary. It was chosen as the size of the
representative group specifically to equal the number of people who got
the vaccine in a research trial.",91.0,Easy
258,Wi,21,Midterm,,Problem 4,Problem 4,"Suppose you are given a DataFrame of employees for a given company.
The DataFrame, called employees, is indexed by
'employee_id' (string) with a column called
'years' (int) that contains the number of years each
employee has worked for the company.",,,
259,Wi,21,Midterm,,Problem 4,Problem 4.1,"Suppose that the code
employees.sort_values(by='years', ascending=False).index[0]
outputs '2476'.
True or False: The number of years that employee 2476 has worked for
the company is greater than the number of years that any other employee
has worked for the company.

 True
 False","False
This is false because there could be other employees who worked at
the company equally long as employee 2476.
The code says that when the employees DataFrame is
sorted in descending order of 'years', employee 2476 is in
the first row. There might, however, be a tie among several employees
for their value of 'years'. In that case, employee 2476 may
wind up in the first row of the sorted DataFrame, but we cannot say that
the number of years employee 2476 has worked for the company is greater
than the number of years that any other employee has worked for the
company.
If the statement had said greater than or equal to instead
of greater than, the statement would have been true.",29.0,Hard
260,Wi,21,Midterm,,Problem 4,Problem 4.2,"What will be the output of the following code?
employees.assign(start=2021-employees.get('years'))
employees.sort_values(by='start').index.iloc[-1]

 the employee id of an employee who has worked there for the most
years
 the employee id of an employee who has worked there for the fewest
years
 an error message complaining about iloc[-1]
 an error message complaining about something else","an error message complaining about
something else
The problem is that the first line of code does not actually add a
new column to the employees DataFrame because the
expression is not saved. So the second line tries to sort by a column,
'start', that doesn’t exist in the employees
DataFrame and runs into an error when it can’t find a column by that
name.
This code also has a problem with iloc[-1], since
iloc cannot be used on the index, but since the problem
with the missing 'start' column is encountered first, that
will be the error message displayed.",27.0,Hard
261,Wi,21,Midterm,,Problem 5,Problem 5,"Suppose df is a DataFrame and b is any
boolean array whose length is the same as the number of rows of
df.
True or False: For any such boolean array b,
df[b].shape[0] is less than or equal to
df.shape[0].

 True
 False","True
The brackets in df[b] perform a query, or filter, to
keep only the rows of df for which b has a
True entry. Typically, b will come from some
condition, such as the entry in a certain column of df
equaling a certain value. Regardless, df[b] contains a
subset of the rows of df, and .shape[0] counts
the number of rows, so df[b].shape[0] must be less than or
equal to df.shape[0].",86.0,Easy
262,Wi,21,Midterm,,Problem 6,Problem 6,"You are given a DataFrame called books that contains
columns 'author' (string), 'title' (string),
'num_chapters' (int), and 'publication_year'
(int).
Suppose that after doing books.groupby('Author').max(),
one row says",,,
263,Wi,21,Midterm,,Problem 6,Problem 6.1,"Based on this data, can you conclude that Charles Dickens is the
alphabetically last of all author names in this dataset?

 Yes
 No","No
When we group by 'Author', all books by the same author
get aggregated together into a single row. The aggregation function is
applied separately to each other column besides the column we’re
grouping by. Since we’re grouping by 'Author' here, the
'Author' column never has the max() function
applied to it. Instead, each unique value in the 'Author'
column becomes a value in the index of the grouped DataFrame. We are
told that the Charles Dickens row is just one row of the output, but we
don’t know anything about the other rows of the output, or the other
authors. We can’t say anything about where Charles Dickens falls when
authors are ordered alphabetically (but it’s probably not last!)",94.0,Easy
264,Wi,21,Midterm,,Problem 6,Problem 6.2,"Based on this data, can you conclude that Charles Dickens wrote
Oliver Twist?

 Yes
 No","Yes
Grouping by 'Author' collapses all books written by the
same author into a single row. Since we’re applying the
max() function to aggregate these books, we can conclude
that Oliver Twist is alphabetically last among all books in the
books DataFrame written by Charles Dickens. So Charles
Dickens did write Oliver Twist based on this data.",95.0,Easy
265,Wi,21,Midterm,,Problem 6,Problem 6.3,"Based on this data, can you conclude that Oliver Twist has
53 chapters?

 Yes
 No","No
The key to this problem is that groupby applies the
aggregation function, max() in this case, independently to
each column. The output should be interpreted as follows:

Among all books in books written by Charles Dickens,
Oliver Twist is the title that is alphabetically last.
Among all books in books written by Charles Dickens, 53
is the greatest number of chapters.
Among all books in books written by Charles Dickens,
1838 is the latest year of publication.

However, the book titled Oliver Twist, the book with 53
chapters, and the book published in 1838 are not necessarily all the
same book. We cannot conclude, based on this data, that Oliver
Twist has 53 chapters.",74.0,Medium
266,Wi,21,Midterm,,Problem 6,Problem 6.4,"Based on this data, can you conclude that Charles Dickens wrote a
book with 53 chapters that was published in 1838?

 Yes
 No","No
As explained in the previous question, the max()
function is applied separately to each column, so the book written by
Charles Dickens with 53 chapters may not be the same book as the book
written by Charles Dickens published in 1838.",73.0,Medium
267,Wi,21,Midterm,,Problem 7,Problem 7,"Give an example of a dataset and a question you would want to answer
about that dataset which you would answer by grouping with subgroups
(using multiple columns in the groupby command). Explain
how you would use the groupby command to answer your
question.
Creative responses that are different than ones we’ve already seen in
this class will earn the most credit.","There are many possible correct answers.
Below are some student responses that earned full credit, lightly edited
for clarity.

 Consider the dataset of Olympic medals (Bronze, Silver, Gold)
that a country won for a specific sport, with columns
'sport', 'country',
'medals'.
 Question: In which sport did the US win the most medals?
We can group by country and then subgroup by sport. We can then
use a combination of reset_index() and
sort_values(by = 'medals') and then use .get
and .iloc[-1] to get the our answer to the
question.

 Given a data set of cell phone purchase volume at every
electronics store, we might want to find the difference in popularity of
Samsung phones and iPhones in every state. I would use the
groupby command to first group by state, followed by phone
brand, and then aggregate with the sum() method. The
resulting table would show the total iPhone and Samsung phone sales
separately for each state which I could then use to calculate the
difference in proportion of each brand’s sales volumes.

 You are given a table called cars with columns:
'brands' (Toyota, Honda, etc.), 'model'
(Prius, Accord, etc.), 'price' of the car, and
'fuel_type' (gas, hybrid, electric). Since you are
environmentally friendly you only want cars that are electric, but you
want to find the cheapest one. Find the brand that has the cheapest
average price for an electric car.
You want to groupby on both 'brands' and
'fuel_type' and use the aggregate command
mean() to find the average price per fuel type for each
brand. Then you would find only the electric fuel types and sort values
to find the cheapest.",81.0,Easy
268,Wi,21,Midterm,,Problem 8,Problem 8,"Which of the following best describes the input and output types of
the .apply Series method?

 input: string, output: Series
 input: Series, output: function
 input: function, output: Series
 input: function, output: function","input: function, output: Series
It helps to think of an example of how we typically use
.apply. Consider a DataFrame called books and
a function called year_to_century that converts a year to
the century it belongs to. We might use .apply as
follows:
books.assign(publication_century = books.get('publication_year').apply(year_to_century))
.apply is called a Series method because we use it on a
Series. In this case that Series is
books.get('publication_year'). .apply takes
one input, which is the name of the function we wish to apply to each
element of the Series. In the example above, that function is
year_to_century. The result is a Series containing the
centuries for each book in the books DataFrame, which we
can then assign back as a new column to the DataFrame. So
.apply therefore takes as input a function and outputs a
Series.",98.0,Easy
269,Wi,21,Midterm,,Problem 9,Problem 9,"You are given a DataFrame called restaurants that
contains information on a variety of local restaurants’ daily number of
customers and daily income. There is a row for each restaurant for each
date in a given five-year time period.
The columns of restaurants are 'name'
(string), 'year' (int), 'month' (int),
'day' (int), 'num_diners' (int), and
'income' (float).
Assume that in our data set, there are not two different restaurants
that go by the same 'name' (chain restaurants, for
example).",,,
270,Wi,21,Midterm,,Problem 9,Problem 9.1,"What type of visualization would be best to display the data in a way
that helps to answer the question “Do more customers bring in more
income?”

 scatterplot
 line plot
 bar chart
 histogram","scatterplot
The number of customers is given by 'num_diners' which
is an integer, and 'income' is a float. Since both are
numerical variables, neither of which represents time, it is most
appropriate to use a scatterplot.",87.0,Easy
271,Wi,21,Midterm,,Problem 9,Problem 9.2,"What type of visualization would be best to display the data in a way
that helps to answer the question “Have restaurants’ daily incomes been
declining over time?”

 scatterplot
 line plot
 bar chart
 histogram","line plot
Since we want to plot a trend of a numerical quantity
('income') over time, it is best to use a line plot.",95.0,Easy
272,Wi,21,Midterm,,Problem 10,Problem 10,"You have a DataFrame called prices that contains
information about food prices at 18 different grocery stores. There is
column called 'broccoli' that contains the price in dollars
for one pound of broccoli at each grocery store. There is also a column
called 'ice_cream' that contains the price in dollars for a
pint of store-brand ice cream.",,,
273,Wi,21,Midterm,,Problem 10,Problem 10.1,"What should type(prices.get('broccoli').iloc[0])
output?

 int
 float
 array
 Series","float
This code extracts the first entry of the 'broccoli'
column. Since this column contains prices in dollars for a pound of
broccoli, it makes sense to represent such a price using a float,
because the price of a pound of broccoli is not necessarily an
integer.",92.0,Easy
274,Wi,21,Midterm,,Problem 10,Problem 10.2,"Using the code,
prices.plot(kind='hist', y='broccoli', bins=np.arange(0.8, 2.11, 0.1), density=True)
we produced the histogram below:

How many grocery stores sold broccoli for a price greater than or
equal to $1.30 per pound, but less than $1.40 per pound (the tallest
bar)?","4 grocery stores
We are given that the bins start at 0.8 and have a width of 0.1,
which means one of the bins has endpoints 1.3 and 1.4. This bin (the
tallest bar) includes all grocery stores that sold broccoli for a price
greater than or equal to $1.30 per pound, but less than $1.40 per
pound.
This bar has a width of 0.1 and we’d estimate the height to be around
2.2, though we can’t say exactly. Multiplying these values, the area of
the bar is about 0.22, which means about 22 percent of the grocery
stores fall into this bin. There are 18 grocery stores in total, as we
are told in the introduction to this question. We can compute using a
calculator that 22 percent of 18 is 3.96. Since the actual number of
grocery stores this represents must be a whole number, this bin must
represent 4 grocery stores.
The reason for the slight discrepancy between 3.96 and 4 is that we
used 2.2 for the height of the bar, a number that we determined by eye.
We don’t know the exact height of the bar. It is reassuring to do the
calculation and get a value that’s very close to an integer, since we
know the final answer must be an integer.",71.0,Medium
275,Wi,21,Midterm,,Problem 10,Problem 10.3,"Suppose we now plot the same data with different bins, using the
following line of code:
prices.plot(kind='hist', y='broccoli', bins=[0.8, 1, 1.1, 1.5, 1.8, 1.9, 2.5], density=True)
What would be the height on the y-axis for the bin corresponding to
the interval [\$1.10, \$1.50)? Input
your answer below.","1.25
First, we need to figure out how many grocery stores the bin [\$1.10, \$1.50) contains. We already know
from the previous subpart that there are four grocery stores in the bin
[\$1.30, \$1.40). We could do similar
calculations to find the number of grocery stores in each of these
bins:

[\$1.10, \$1.20)
[\$1.20, \$1.30)
[\$1.40, \$1.50)

However, it’s much simpler and faster to use the fact that when the
bins are all equally wide, the height of a bar is proportional to the
number of data values it contains. So looking at the histogram in the
previous subpart, since we know the [\$1.30,
\$1.40) bin contains 4 grocery stores, then the [\$1.10, \$1.20) bin must contain 1 grocery
store, since it’s only a quarter as tall. Again, we’re taking advantage
of the fact that there must be an integer number of grocery stores in
each bin when we say it’s 1/4 as tall. Our only options are 1/4, 1/2, or
3/4 as tall, and among those choices, it’s clear.
Therefore, by looking at the relative heights of the bars, we can
quickly determine the number of grocery stores in each bin:

[\$1.10, \$1.20): 1 grocery
store
[\$1.20, \$1.30): 3 grocery
stores
[\$1.30, \$1.40): 4 grocery
stores
[\$1.40, \$1.50): 1 grocery
store

Adding these numbers together, this means there are 9 grocery stores
whose broccoli prices fall in the interval [\$1.10, \$1.50). In the new histogram, these
9 grocery stores will be represented by a bar of width 1.50-1.10 = 0.4. The area of the bar should
be \frac{9}{18} = 0.5. Therefore the
height must be \frac{0.5}{0.4} =
1.25.",33.0,Hard
276,Wi,21,Midterm,,Problem 10,Problem 10.4,"You are interested in finding out the number of stores in which a
pint of ice cream was cheaper than a pound of broccoli. Will you be able
to determine the answer to this question by looking at the plot produced
by the code below?
prices.get(['broccoli', 'ice_cream']).plot(kind='barh')

 Yes
 No","Yes
When we use .plot without specifying a y
column, it uses every column in the DataFrame as a y column
and creates an overlaid plot. Since we first use get with
the list ['broccoli', 'ice_cream'], this keeps the
'broccoli' and 'ice_cream' columns from
prices, so our bar chart will overlay broccoli prices with
ice cream prices. Notice that this get is unnecessary
because prices only has these two columns, so it would have
been the same to just use prices directly. The resulting
bar chart will look something like this:

Each grocery store has its broccoli price represented by the length
of the blue bar and its ice cream price represented by the length of the
red bar. We can therefore answer the question by simply counting the
number of red bars that are shorter than their corresponding blue
bars.",78.0,Easy
277,Wi,21,Midterm,,Problem 10,Problem 10.5,"You are interested in finding out the number of stores in which a
pint of ice cream was cheaper than a pound of broccoli. Will you be able
to determine the answer to this question by looking at the plot produced
by the code below?
prices.get(['broccoli', 'ice_cream']).plot(kind='hist')

 Yes
 No","No
This will create an overlaid histogram of broccoli prices and ice
cream prices. So we will be able to see the distribution of broccoli
prices together with the distribution of ice cream prices, but we won’t
be able to pair up particular broccoli prices with ice cream prices at
the same store. This means we won’t be able to answer the question. The
overlaid histogram would look something like this:

This tells us that broadly, ice cream tends to be more expensive than
broccoli, but we can’t say anything about the number of stores where ice
cream is cheaper than broccoli.",81.0,Easy
278,Wi,21,Midterm,,Problem 10,Problem 10.6,"Some code and the scatterplot that produced it is shown below:
(prices.get(['broccoli', 'ice_cream']).plot(kind='scatter', x='broccoli', y='ice_cream'))

Can you use this plot to figure out the number of stores in which a
pint of ice cream was cheaper than a pound of broccoli?
If so, say how many such stores there are and explain how you came to
that conclusion.
If not, explain why this scatterplot cannot be used to answer the
question.","Yes, and there are 2 such stores.
In this scatterplot, each grocery store is represented as one dot.
The x-coordinate of that dot tells the
price of broccoli at that store, and the y-coordinate tells the price of ice cream. If
a grocery store’s ice cream price is cheaper than its broccoli price,
the dot in the scatterplot will have y<x. To identify such dots in the
scatterplot, imagine drawing the line y=x. Any dot below this line corresponds to a
point with y<x, which is a grocery
store where ice cream is cheaper than broccoli. As we can see, there are
two such stores.",78.0,Easy
279,Wi,21,Midterm,,Problem 11,Problem 11,"Note: This problem is out of scope; it
covers material no longer included in the course.
You have a DataFrame called flights containing
information on various plane tickets sold between US cities. The columns
are 'route_length', which stores distance between the
arrival and departure airports, in miles, and 'price',
which stores the cost of the airline ticket, in dollars. You notice that
longer flights tend to cost more, as expected.



route_length
price","306 dollars
Galton’s method for making predictions is to take “nearby” x-values and average their corresponding
y-values. For example, to predict the
height of a child born to 70-inch parents, he averages the heights of
children born to families where the parents are close to 70 inches tall.
Using that same strategy here, we first need to identify which flights
are considered “nearby” to a route that is 2800 miles. We are told that
“nearby” means within 60 miles, so we are looking for flights between
2740 and 2860 miles in length. There are three such flights (the second,
third, and fourth rows of the original data):



route_length
price




2750
249


2850
349


2850
319



 Now, we simply need to average these three prices to make our
prediction. Since \frac{249+349+319}{3} =
305.67 and we are told to round to the nearest dollar, our
prediction is 306 dollars.",59.0,Medium
280,Wi,21,Midterm,,Problem 12,Problem 12,"You generate a three-digit number by randomly choosing each digit to
be a number 0 through 9, inclusive. Each digit is equally likely to be
chosen.",,,
281,Wi,21,Midterm,,Problem 12,Problem 12.1,"What is the probability you produce the number 027?
Give your answer as a decimal number between 0 and 1 with no
rounding.","0.001
There is a \frac{1}{10} chance that
we get 0 as the first random number, a \frac{1}{10} chance that we get 2 as the
second random number, and a \frac{1}{10} chance that we get 7 as the
third random number. The probability of all of these events happening is
\frac{1}{10}*\frac{1}{10}*\frac{1}{10} =
0.001.
Another way to do this problem is to think about the possible
outcomes. Any number from 000 to 999 is possible and all are equally
likely. Since there are 1000 possible outcomes and the number 027 is
just one of the possible outcomes, the probability of getting this
outcome is \frac{1}{1000} = 0.001.",92.0,Easy
282,Wi,21,Midterm,,Problem 12,Problem 12.2,"What is the probability you produce a number with an odd digit in the
middle position? For example, 250. Give your answer as
a decimal number between 0 and 1 with no rounding.","0.5
Because the values of the left and right positions are not important
to us, think of the middle position only. When selecting a random number
to go here, we are choosing randomly from the numbers 0 through 9. Since
5 of these numbers are odd (1, 3, 5, 7, 9), the probability of getting
an odd number is \frac{5}{10} =
0.5.",78.0,Easy
283,Wi,21,Midterm,,Problem 12,Problem 12.3,"What is the probability you produce a number with a
7 in it somewhere? Give your answer as a decimal number
between 0 and 1 with no rounding.","0.271
It’s easier to calculate the probability that the number has no 7 in
it, and then subtract this probability from 1. To solve this problem
directly, we’d have to consider cases where 7 appeared multiple times,
which would be more complicated.
The probability that the resulting number has no 7 is \frac{9}{10}*\frac{9}{10}*\frac{9}{10} =
0.729 because in each of the three positions, there is a \frac{9}{10} chance of selecting something
other than a 7. Therefore, the probability that the number has a 7 is
1 - 0.729 = 0.271.",69.0,Medium
284,Wi,21,Midterm,,Problem 13,Problem 13,"Describe in your own words the difference between a probability
distribution and an empirical distribution. Give an example of what each
distribution might look like for a certain experiment. Choose an
experiment that we have not already seen in this class.","There are many possible correct answers.
Below are some student responses that earned full credit, lightly edited
for clarity.

Probability distributions are theoretical distributions
distributed over all possible values of an experiment. Meanwhile,
empirical distributions are distributions of the real observed data. An
example of this would be choosing a certain suit from a deck of cards.
The probability distribution would be uniform, with a 1/4 chance of
choosing each suit. Meanwhile, the empirical distribution of choosing
suits from a deck of cards in 50 pulls manually and graphing the
observed data would show us different chances. 

A probability distribution is the distribution describing the
theoretical probability of each potential value occurring in an
experiment, while the empirical distribution describes the proportion of
each of the values in the experiment after running it, including all
observed values. In other words, the probability distribution is what we
expect to happen, and the empirical distribution is what actually
happens.
For example: My friends and I often go to a food court to eat, and
we randomly pick a restaurant every time. There is 1 McDonald’s, 1
Subway, and 2 Panda Express restaurants in the food court.
The probability distribution is as follows:

P(McDonald’s) = 0.25
P(Subway) = 0.25
P(Panda Express) = 0.5

After going to the food court 100 times, we look at the empirical
distribution to see which restaurants we eat at most often. it is as
follows:

(McDonald’s) = 0.21
(Subway) = 0.22
(Panda Express) = 0.57


Probability distribution is a theoretical representation of
certain outcomes in an event whereas an empirical distribution is the
observational representation of the same outcomes in an event produced
from an experiment.
An example would be if I had 10 pairs of shoes in my closet: The
probability distribution would suggest that each pair of shoes has an
equal chance of getting picked on any given day. On the other hand, an
empirical distribution would be drawn by recording which pair got picked
on a given day in N trials.",82.0,Easy
285,Wi,21,Midterm,,Problem 14,Problem 14,"results = np.array([])
for i in np.arange(10):
    result = np.random.choice(np.arange(1000), replace=False)
    results = np.append(results, result)
After this code executes, results contains:

 a simple random sample of size 9, chosen from a set of size 999 with
replacement
 a simple random sample of size 9, chosen from a set of size 999
without replacement
 a simple random sample of size 10, chosen from a set of size 1000
with replacement
 a simple random sample of size 10, chosen from a set of size 1000
without replacement","a simple random sample of size 10, chosen
from a set of size 1000 with replacement
Let’s see what the code is doing. The first line initializes an empty
array called results. The for loop runs 10 times. Each
time, it creates a value called result by some process
we’ll inspect shortly and appends this value to the end of the
results array. At the end of the code snippet,
results will be an array containing 10 elements.
Now, let’s look at the process by which each element
result is generated. Each result is a random
element chosen from np.arange(1000) which is the numbers
from 0 to 999, inclusive. That’s 1000 possible numbers. Each time
np.random.choice is called, just one value is chosen from
this set of 1000 possible numbers.
When we sample just one element from a set of values, sampling with
replacement is the same as sampling without replacement, because
sampling with or without replacement concerns whether subsequent draws
can be the same as previous ones. When we’re just sampling one element,
it really doesn’t matter whether our process involves putting that
element back, as we’re not going to draw again!
Therefore, result is just one random number chosen from
the 1000 possible numbers. Each time the for loop executes,
result gets set to a random number chosen from the 1000
possible numbers. It is possible (though unlikely) that the random
result of the first execution of the loop matches the
result of the second execution of the loop. More generally,
there can be repeated values in the results array since
each entry of this array is independently drawn from the same set of
possibilities. Since repetitions are possible, this means the sample is
drawn with replacement.
Therefore, the results array contains a sample of size
10 chosen from a set of size 1000 with replacement. This is called a
“simple random sample” because each possible sample of 10 values is
equally likely, which comes from the fact that
np.random.choice chooses each possible value with equal
probability by default.",11.0,Hard
286,Wi,21,Midterm,,Problem 15,Problem 15,"Suppose we take a uniform random sample with replacement from a
population, and use the sample mean as an estimate for the population
mean. Which of the following is correct?

 If we take a larger sample, our sample mean will be closer to the
population mean.
 If we take a smaller sample, our sample mean will be closer to the
population mean.
 If we take a larger sample, our sample mean is more likely to be
close to the population mean than if we take a smaller sample.
 If we take a smaller sample, our sample mean is more likely to be
close to the population mean than if we take a larger sample.","If we take a larger sample, our sample mean
is more likely to be close to the population mean than if we take a
smaller sample.
Larger samples tend to give better estimates of the population mean
than smaller samples. That’s because large samples are more like the
population than small samples. We can see this in the extreme. Imagine a
sample of 1 element from a population. The sample might vary a lot,
depending on the distribution of the population. On the other extreme,
if we sample the whole population, our sample mean will be exactly the
same as the population mean.
Notice that the correct answer choice uses the words “is more likely
to be close to” as opposed to “will be closer to.” We’re talking about a
general phenomenon here: larger samples tend to give better estimates of
the population mean than smaller samples. We cannot say that if we take
a larger sample our sample mean “will be closer to” the population mean,
since it’s always possible to get lucky with a small sample and unlucky
with a large sample. That is, one particular small sample may happen to
have a mean very close to the population mean, and one particular large
sample may happen to have a mean that’s not so close to the population
mean. This can happen, it’s just not likely to.",100.0,Easy
287,Wi,22,Midterm,,Problem 1,Problem 1,"Below, identify the data type of the result of each of the following
expressions, or select “error” if you believe the expression results in
an error.",,,
288,Wi,22,Midterm,,Problem 1,Problem 1.1,"sky.sort_values('height')

 int or float
 Boolean
 string
 array
 Series
 DataFrame
 error","DataFrame
sky is a DataFrame. All the sort_values
method does is change the order of the rows in the Series/DataFrame it
is called on, it does not change the data structure. As such,
sky.sort_values('height') is also a DataFrame.",87.0,Easy
289,Wi,22,Midterm,,Problem 1,Problem 1.2,"sky.sort_values('height').get('material').loc[0]

 int or float
 Boolean
 string
 array
 Series
 DataFrame
 error","error
sky.sort_values('height') is a DataFrame, and
sky.sort_values('height').get('material') is a Series
corresponding to the 'material' column, sorted by
'height' in increasing order. So far, there are no
errors.
Remember, the .loc accessor is used to access
elements in a Series based on their index.
sky.sort_values('height').get('material').loc[0] is asking
for the element in the
sky.sort_values('height').get('material') Series with index
0. However, the index of sky is made up of building names.
Since there is no building named 0, .loc[0]
causes an error.",79.0,Easy
290,Wi,22,Midterm,,Problem 1,Problem 1.3,"sky.sort_values('height').get('material').iloc[0]

 int or float
 Boolean
 string
 array
 Series
 DataFrame
 error","string
As we mentioned above,
sky.sort_values('height').get('material') is a Series
containing values from the 'material' column (but sorted).
Remember, there is no element in this Series with an index of 0, so
sky.sort_values('height').get('material').loc[0] errors.
However, .iloc[0] works differently than
.loc[0]; .iloc[0] will give us the first
element in a Series (independent of what’s in the index). So,
sky.sort_values('height').get('material').iloc[0] gives us
back a value from the 'material' column, which is made up
of strings, so it gives us a string. (Specifically, it gives us the
'material' type of the skyscraper with the smallest
'height'.)",89.0,Easy
291,Wi,22,Midterm,,Problem 1,Problem 1.4,"sky.get('city').apply(len)

 int or float
 Boolean
 string
 array
 Series
 DataFrame
 error","Series
The .apply method takes in a function and evaluates that
function on every element in a Series. Here,
sky.get('city').apply(len) is using the function
len on every element in the Series
sky.get('city'). The result is also a Series, containing
the lengths of the names of each 'city'.",79.0,Easy
292,Wi,22,Midterm,,Problem 1,Problem 1.5,"sky.get('city').apply(max)

 int or float
 Boolean
 string
 array
 Series
 DataFrame
 error","Series
This is a tricky problem!
The function that apply takes in must work on individual
elements in a Series, i.e. it must work with just a single argument. We
saw this in the above subpart, where
sky.get('city').apply(len) applied len on each
'city' name.
Here, we are trying to apply the max function on each
'city' name. The max of a single item does not
work in Python, because taking the max requires comparing
two or more elements. Try it out - in a notebook, run the expression
max(5), and you’ll see an error. So, if we tried to use
.apply(max) on a Series of numbers, we’d run into an
error.
However, we are using .apply(max) on a
Series of strings, and it turns out that Python does
allow us to take the max of a string! The max
of a string in Python is defined as the last character in the string
alphabetically, so max('hello') evaluates to
'o'. This means that
sky.get('city').apply(max) does actually run without error;
it evaluates to a Series containing the last element in the name of each
'city'.
(This subpart was trickier than we intended – we ended up giving
credit to both “error” and “Series”.)",89.0,Easy
293,Wi,22,Midterm,,Problem 1,Problem 1.6,"sky.get('floors').max()

 int or float
 Boolean
 string
 array
 Series
 DataFrame
 error","int or float
The Series sky.get('floors') is made up of integers, and
sky.get('floors').max() evaluates to the largest number in
the Series, which is also an integer.",91.0,Easy
294,Wi,22,Midterm,,Problem 1,Problem 1.7,"sky.groupby('material').max()

 int or float
 Boolean
 string
 array
 Series
 DataFrame
 error","DataFrame
When grouping and using an aggregation method, the result is always a
DataFrame. The DataFrame sky.groupby('material').max()
contains all of the columns in sky, minus
'material', which has been moved to the index. It contains
one row for each unique 'material'.
Note that no columns were “dropped”, as may happen when using
.mean(), because .max() can work on Series’ of
any type. You can take the max of strings, while you cannot take the
mean of strings.",78.0,Easy
295,Wi,22,Midterm,,Problem 1,Problem 1.8,"sky.index[0]

 int or float
 Boolean
 string
 array
 Series
 DataFrame
 error","string
sky.index contains the values
'Bayard-Condict Building',
'The Yacht Club at Portofino',
'City Investing Building', etc. sky.index[0]
is then 'Bayard-Condict Building', which is a string.",91.0,Easy
296,Wi,22,Midterm,,Problem 2,Problem 2,"In this question, we’ll write code to learn more about the
skyscrapers in the beautiful city of San Diego. (Unrelated fun fact –
since the San Diego Airport is so close to downtown, buildings in
downtown San Diego legally cannot be taller than 152 meters.)",,,
297,Wi,22,Midterm,,Problem 2,Problem 2.1,"Below, fill in the blank to create a DataFrame, named
san_tall, consisting of just the skyscrapers in San Diego
that are over 100 meters tall.
condition = ______
san_tall = sky[(sky.get('city') == 'San Diego') & condition]
What goes in the blank?","sky.get('height') > 100
We need to query for all of the skyscrapers that satisfy two
conditions – the 'city' must be 'San Diego'
and the 'height' must be above 100. The first condition was
already implemented for us, so we just need to construct a Boolean
Series that implements the second condition.
Here, we want all of the rows where 'height' is above
100, so we get the 'height' column and compare
it to 100 like so: sky.get('height') > 100.",95.0,Easy
298,Wi,22,Midterm,,Problem 2,Problem 2.2,"Suppose san_tall from the previous part was created
correctly. Fill in the blanks so that height_many_floors
evaluates to the height (in meters) of the skyscraper
with the most floors, amongst all skyscrapers in San
Diego that are over 100 meters tall.
height_many_floors = san_tall.______.iloc[0]
What goes in the blank?","sort_values('floors', ascending=False).get('height')
The end of the line given to us is .iloc[0]. We know
that .iloc[0] extracts the first element in whatever Series
it is called on, so what comes before .iloc[0] must be a
Series where the first element is the 'height' of the
skyscraper with the most floors, among all skyscrapers in San Diego that
are over 100 meters tall. The DataFrame we are working with,
san_tall, already only has skyscrapers in San Diego that
are over 100 meters tall.
This means that in the blank, all we need to do is:

Sort skyscrapers by 'floors' in decreasing order (so
that the first row is the skyscraper with the most
'floors').
Extract the 'height' column.

As such, a complete answer is
height_many_floors = san_tall.sort_values('floors', ascending=False).get('height').iloc[0].",74.0,Medium
299,Wi,22,Midterm,,Problem 2,Problem 2.3,"height_many_floors, the value you computed in the
previous part (2.2) was a number.
True or False: Assuming that the DataFrame
san_tall contains all skyscrapers in San Diego,
height_many_floors is the height (in meters) of the
tallest skyscraper in San Diego.

 True
 False","False
height_many_floors is the height of the skyscraper with
the most 'floors'. However, this is not necessarily the
tallest skyscraper (i.e. the skyscraper with the largest
'height')! Consider the following scenario:

Building A: 15 floors, height of 150 feet
Building B: 20 floors, height of 100 feet

height_many_floors would be 100, but it is not the
'height' of the taller building.",84.0,Easy
300,Wi,22,Midterm,,Problem 3,Problem 3,"Note that each part of Question 3 depends on previous parts of
Question 3.
In this question, we’ll take a closer look at the
'material' column of sky.",,,
301,Wi,22,Midterm,,Problem 3,Problem 3.1,"Below, fill in the blank to complete the implementation of the
function majority_concrete, which takes in the name of a
city and returns True if the majority of the
skyscrapers in that city are made of concrete, and False
otherwise. We define “majority” to mean “at least
50%”.
def majority_concrete(city):
    all_city = sky[sky.get('city') == city]
    concrete_city = all_city[all_city('material') == 'concrete']
    proportion = ______
    return proportion >= 0.5
What goes in the blank?","concrete_city.shape[0] / all_city.shape[0]
Let’s first understand the code that is already provided for us. Note
that city is a string corresponding to the name of a
city.
all_city contains only the rows for the passed in
city. Note that all_city.shape[0] or
len(all_city) is the number of rows in
all_city, i.e. it is the number of skyscrapers in
city. Then, concrete_city contains only the
rows in all_city corresponding to 'concrete'
skyscrapers, i.e. it contains only the rows corresponding to
'concrete' skyscrapers in city. Note that
concrete_city.shape[0] or len(concrete_city)
is the number of skyscrapers in city that are made of
'concrete'.
We want to return True only if at least 50% of the
skyscrapers in city are made of concrete. The last line in
the function, return proportion >= 0.5, is already
provided for us, so all we need to do is compute the proportion of
skyscrapers in city that are made of concrete. This is
concrete_city.shape[0] / all_city.shape[0].
Another possible answer is
len(concrete_city) / len(all_city).",85.0,Easy
302,Wi,22,Midterm,,Problem 3,Problem 3.2,"Below, we create a DataFrame named by_city.
by_city = sky.groupby('city').count().reset_index()
Below, fill in the blanks to add a column to by_city,
called 'is_majority', that contains the value
True for each city where the majority of skyscrapers are
concrete, and False for all other cities. You may need to
use the function you defined in the previous subpart.
by_city = by_city.assign(is_majority = ______)
What goes in the blank?","by_city.get('city').apply(majority_concrete)
We are told to add a column to by_city. Recall, the way
that .assign works is that the name of the new column comes
before the = symbol, and a Series (or array) containing the
values for the new column comes after the = symbol. As
such, a Series needs to go in the blank.
majority_concrete takes in the name of a single
city and returns either True or
False accordingly. All we need to do here then is use the
majority_concrete function on every element in the
'city' column. After accessing the 'city'
column using by_city.get('city'), we need to use the
.apply method using the argument
majority_concrete. Putting it all together yields
by_city.get('city').apply(majority_concrete), which is a
Series.
Note: Here, by_city.get('city') only works because
.reset_index() was used in the line where
by_city was defined. If we did not reset the index,
'city' would not be a column!",86.0,Easy
303,Wi,22,Midterm,,Problem 3,Problem 3.3,"by_city now has a column named
'is_majority' as described in the previous subpart. Now,
suppose we create another DataFrame, mystery, below:
mystery = by_city.groupby('is_majority').count()
What is the largest possible value that mystery.shape[0]
could evaluate to?","2
Recall, the 'is_majority' column we created in the
previous subpart contains only two possible values – True
and False. When we group by_city by
'is_majority', we create two “groups” – one for
True and one for False. As such, no matter
what aggregation method we use (here we happened to use
.count()), the resulting DataFrame will only have 2 rows
(again, one for True and one for False).
Note: The question asked for the “largest possible value” that
mystery.shape[0], because it is possible that
mystery only has 1 row. This can only happen in two
cases:

It is true in all cities that the majority of
skyscrapers are made of 'concrete'.
It is true in no cities that the majority of
skyscrapers are made of 'concrete'.",76.0,Easy
304,Wi,22,Midterm,,Problem 3,Problem 3.4,"Suppose
mystery.get('city').iloc[0] == mystery.get('city').iloc[1]
evaluates to True.
True or False: In exactly half of the cities in
sky, it is true that a majority of skyscrapers are made of
'concrete'. (Tip: Walk through
the manipulations performed in the previous three subparts to get an
idea of what mystery looks like and contains.)

 True
 False","True
In the solution to the previous subpart, we noted that
mystery contains at most 2 rows, one corresponding to
cities where 'is_majority' is True and one
corresponding to cities where 'is_majority' is
False. Furthermore, recall that we used the
.count() aggregation method, which means that the entries
in each column of mystery contain the
number of cities where 'is_majority' is
True and the number of cities where
'is_majority' is False.
If
mystery.get('city').iloc[0] == mystery.get('city').iloc[1],
it must be the case that the number of cities where
'is_majority' is True and False
are equal. This must mean that in exactly half of the cities in
sky, it is true that the majority of skyscrapers are made
of 'concrete'.",69.0,Medium
305,Wi,22,Midterm,,Problem 4,Problem 4,"Suppose we have access to another DataFrame, new_york,
that contains the latitude and longitude of every single skyscraper in
New York City that is also in sky. The first few rows of
new_york are shown below.

Below, we define a new DataFrame, sky_with_location,
that merges together both sky and
new_york.
sky_with_location = sky.merge(new_york, left_index=True, right_on='name')
Given that:

sky has s rows,
new_york has n rows,
and
building names are spelled and formatted the exact same way in both
sky and new_york, i.e. that there are no typos
in either DataFrame,

select the true statement below.

 sky_with_location has exactly s rows.
 sky_with_location has exactly n rows.
 sky_with_location has exactly s - n rows.
 sky_with_location has exactly s + n rows.
 sky_with_location has exactly s \times n rows.","sky_with_location has exactly
n rows.
Here, we are merging sky and new_york on
skyscraper names (stored in the index in sky and in the
'name' column in new_york). The resulting
DataFrame, sky_with_location, will have one row for each
“match” between skyscrapers in sky and
new_york. Since skyscraper names are presumably unique,
sky_with_location will have one row for each skyscraper
that is in both sky and new_york.
The skyscrapers that are in both sky and
new_york are just the skyscrapers in new_york,
since all of the non-New York skyscrapers in sky won’t be
in new_york. As such, sky_with_location has
the same number of rows as new_york. new_york
has n rows, so
sky_with_location also has n rows.",64.0,Medium
306,Wi,22,Midterm,,Problem 5,Problem 5,"Recall, the interval [a, b) refers
to numbers greater than or equal to a
and less than b, and the interval [a, b] refers to numbers greater than or
equal to a and less than or equal to
b.
Suppose we created a DataFrame, medium_sky, containing
only the skyscrapers in sky whose number of floors are in
the interval [20, 60]. Below, we’ve
drawn a histogram of the number of floors of all skyscrapers in
medium_sky.",,,
307,Wi,22,Midterm,,Problem 5,Problem 5.1,"Suppose that there are 160 skyscrapers whose number of floors are in
the interval [30, 35).
Given this information and the histogram above, how many skyscrapers
are there in medium_sky?","800
Recall, in a histogram,
\text{proportion of values in bin} =
\text{area of bar} = \text{width of bin} \cdot \text{height of
bar}
Also, note that:
\text{\# of values satisfying condition} =
\text{proportion of values satisfying condition} \cdot \text{total \# of
values}
Here, we’re given the entire histogram, so we can find the proportion
of values in the [30, 35) bin. We are
also given the number of values in the [30,
35) bin (160). This means we can use the second equation above to
find the total number of skyscrapers in medium_sky.
The first step is finding the area of the [30, 35) bin’s bar. Its width is 35-30 = 5, and its height is 0.04, so its area is 5 \cdot 0.04 = 0.2. Then,
\begin{aligned}
        \text{\# of values satisfying condition} &= \text{proportion
of values satisfying condition} \cdot \text{total \# of values} \\
        160 &= 0.2 \cdot \text{total \# of values} \\
        \implies \text{total \# of values} &= \frac{160}{0.2} = 160
\cdot 5 = 800
\end{aligned}",62.0,Medium
308,Wi,22,Midterm,,Problem 5,Problem 5.2,"Again, suppose that there are 160 skyscrapers whose number of floors
are in the interval [30, 35).
Now suppose that there is a typo in the medium_sky
DataFrame, and 20 skyscrapers were accidentally listed as having 53
floors each when instead they actually only have 35 floors each. The
histogram drawn above contains the incorrect version of the data.
Suppose we re-draw the above histogram using the correct data. What
will be the new heights of both the [35,
40) bar and [50, 55) bar? Select
the closest answer.

 The [35, 40) bar’s height becomes
0.0325, and the [50, 55) bar’s height
becomes 0.0105.
 The [35, 40) bar’s height becomes
0.035, and the [50, 55) bar’s height
becomes 0.008.
 The [35, 40) bar’s height becomes
0.0375, and the [50, 55) bar’s height
becomes 0.0055.
 The [35, 40) bar’s height becomes
0.04, and the [50, 55) bar’s height
becomes 0.003.","The [35,
40) bar’s height becomes 0.035, and the [50, 55) bar’s height becomes 0.008.
The current height of the [35, 40)
bar is 0.03, and the current height of the [50, 55) bar is 0.013 (approximately; its
height appears to be slightly more than halfway between 0.01 and 0.015).
We need to decrease the height of the [50,
55) bar and increase the height of the [35, 40) bar. The combined area of both bars
must stay the same, since the proportion of values in their bins
(together) is not changing. This means that the amount we need to
decrease the [50, 55) bar’s height by
is the same as the amount we need to increase the [35, 40) bar’s height by. Note that this
relationship is true in all 4 answer choices.
In the question, we were told that 20 skyscrapers were incorrectly
binned. There are 800 skyscrapers total, so the proportion of
skyscrapers that were incorrectly binned is \frac{20}{800} = 0.025. This means that the
area of the [35, 40) bar needs to
increase by 0.025 and the area of the [50,
55) bar needs to decrease by 0.025. Recall, each bar has width 5.
That means that the “rectangular section” we will add to the [35, 40) bar and remove from the [50, 55) bar has height
\text{height} =
\frac{\text{area}}{\text{width}} = \frac{0.025}{5} = 0.005
Thus, the height of the [35, 40) bar
becomes 0.03 + 0.005 = 0.035 and the
height of the [50, 55) bar becomes
0.013 - 0.005 = 0.008.",74.0,Medium
309,Wi,22,Midterm,,Problem 6,Problem 6,"Consider the following line plot, which depicts the number of
skyscrapers built per year.",,,
310,Wi,22,Midterm,,Problem 6,Problem 6.1,"We created the line plot above using the following line of code:
sky.groupby('year').count().plot(kind='line', y='height');
Which of the following could we replace 'height' with in
the line of code above, such that the resulting line of code creates the
same line plot? Select all that apply.

 'name'
 'material'
 'city'
 'floors'
 'year'
 None of the above",,74.0,Medium
311,Wi,22,Midterm,,Problem 6,Problem 6.2,"Note: This problem is out of scope; it
covers material no longer included in the course.
Now let’s look at the number of skyscrapers built each year since
1975 in New York City 🗽.

Which of the following is a valid conclusion we can make using this
graph alone?

 No city in the dataset had more skyscrapers built in 2015 than New
York City.
 The decrease in the number of skyscrapers built in 2012 over previous
years was due to the 2008 economic recession, and the reason the
decrease is seen in 2012 rather than 2008 is because skyscrapers usually
take 4 years to be built.
 The decrease in the number of skyscrapers built in 2012 over previous
years was due to something other than the 2008 economic recession.
 The COVID-19 pandemic is the reason that so few skyscrapers were
built in 2020.
 None of the above.","None of the above.
Let’s look at each answer choice.

“No city in the dataset had more skyscrapers built in 2015 than New
York City.” is not necessarily true, because the graph provided only
shows information for New York City. For all we know, 10000 skyscrapers
were built in San Diego in 2015.
“The decrease in the number of skyscrapers built in 2012 over
previous years was due to the 2008 economic recession, and the reason
the decrease is seen in 2012 rather than 2008 is because skyscrapers
usually take 4 years to be built.” is not necessarily true, and we have
no information that suggests that it is.
“The decrease in the number of skyscrapers built in 2012 over
previous years was due to something other than the 2008 economic
recession.” is also not necessarily true – it could be the case
that the recession was the reason, but the graph doesn’t tell us whether
or not it is.
“The COVID-19 pandemic is the reason that so few skyscrapers were
built in 2020.”, for similar reasons, is also not necessarily true.

Tip: This is a typical “cause-and-effect” problem
that you’ll see in DSC 10 exams quite often. In order to establish that
some treatment had an effect, we need to run a randomized controlled
trial, or have some other guarantee that there is no difference between
the naturally-observed control and treatment groups.",90.0,Easy
312,Wi,22,Midterm,,Problem 6,Problem 6.3,"In which of the following scenarios would it make sense to draw a
overlaid histogram?

 To visualize the number of skyscrapers of each material type,
separately for New York City and Chicago.
 To visualize the distribution of the number of floors per skyscraper,
separately for New York City and Chicago.
 To visualize the average height of skyscrapers built per year,
separately for New York City and Chicago.
 To visualize the relationship between the number of floors and height
for all skyscrapers.","To visualize the distribution of the number
of floors per skyscraper, separately for New York City and Chicago.
Recall, we use a histogram to visualize the distribution of a
numerical variable. Here, we have a numerical variable (number of
floors) that is split across two categories (New York City and Chicago),
so we need to draw two histograms, or an overlaid histogram.
In the three incorrect answer choices, another visualization type is
more appropriate. Given the descriptions here, see if you can draw what
each of these three visualizations should look like.

To visualize the number of skyscrapers of each material type,
separately for New York City and Chicago, we’d need to draw an overlaid
bar chart. For each material type, there would be two bars – one for New
York City, and one for Chicago. The length of each bar would correspond
to the number of skyscrapers of that material type in that city. We
could color the New York City and Chicago bars in different colors.
To visualize the average height of skyscrapers built per year,
separately for New York City and Chicago, we’d need to draw an overlaid
line chart. There would be two lines, one for New York City and one for
Chicago (this would look similar to the plot in the previous subpart,
but with another line).
To visualize the relationship between the number of floors and
height for all skyscrapers, we’d need to draw a scatter plot. This is
because scatter plots are the correct tool to use to visualize the
relationship between two numerical variables, and both “number of
floors” and “height” are numerical variables.",62.0,Medium
313,Wi,22,Midterm,,Problem 7,Problem 7,"Billina Records, a new record company focused on creating new TikTok
audios, has its offices on the 23rd floor of a skyscraper with 75 floors
(numbered 1 through 75). The owners of the building promised that 10
different random floors will be selected to be renovated.",,,
314,Wi,22,Midterm,,Problem 7,Problem 7.1,"Below, fill in the blanks to complete a simulation that will estimate
the probability that Billina Records’ floor will be renovated.
total = 0
repetitions = 10000
for i in np.arange(repetitions):
    choices = np.random.choice(__(a)__, 10, __(b)__)
    if __(c)__:
        total = total + 1
prob_renovate = total / repetitions
What goes in blank (a)?

 np.arange(1, 75)
 np.arange(10, 75)
 np.arange(0, 76)
 np.arange(1, 76)

What goes in blank (b)?

 replace=True
 replace=False

What goes in blank (c)?

 choices == 23
 choices is 23
 np.count_nonzero(choices == 23) > 0
 np.count_nonzero(choices) == 23
 choices.str.contains(23)","np.arange(1, 76),
replace=False,
np.count_nonzero(choices == 23) > 0
Here, the idea is to randomly choose 10 different
floors repeatedly, and each time, check if floor 23 was selected.
Blank (a): The first argument to np.random.choice needs
to be an array/list containing the options we want to choose from,
i.e. an array/list containing the values 1, 2, 3, 4, …, 75, since those
are the numbers of the floors. np.arange(a, b) returns an
array of integers spaced out by 1 starting from a and
ending at b-1. As such, the correct call to
np.arange is np.arange(1, 76).
Blank (b): Since we want to select 10 different floors, we need to
specify replace=False (the default behavior is
replace=True).
Blank (c): The if condition needs to check if 23 was one
of the 10 numbers that were selected, i.e. if 23 is in
choices. It needs to evaluate to a single Boolean value,
i.e. True (if 23 was selected) or False (if 23
was not selected). Let’s go through each incorrect option to see why
it’s wrong:

Option 1, choices == 23, does not evaluate to a single
Boolean value; rather, it evaluates to an array of length 10, containing
multiple Trues and Falses.
Option 2, choices is 23, does not evaluate to what we
want – it checks to see if the array choices is the same
Python object as the number 23, which it is not (and will never be,
since an array cannot be a single number).
Option 4, np.count_nonzero(choices) == 23, does
evaluate to a single Boolean, however it is not quite correct.
np.count_nonzero(choices) will always evaluate to 10, since
choices is made up of 10 integers randomly selected from 1,
2, 3, 4, …, 75, none of which are 0. As such,
np.count_nonzero(choices) == 23 is the same as
10 == 23, which is always False, regardless of whether or
not 23 is in choices.
Option 5, choices.str.contains(23), errors, since
choices is not a Series (and .str can only
follow a Series). If choices were a Series, this would
still error, since the argument to .str.contains must be a
string, not an int.

By process of elimination, Option 3,
np.count_nonzero(choices == 23) > 0, must be the correct
answer. Let’s look at it piece-by-piece:

As we saw in Option 1, choices == 23 is a Boolean array
that contains True each time the selected floor was floor
23 and False otherwise. (Since we’re sampling without
replacement, floor 23 can only be selected at most once, and so
choices == 23 can only contain the value True
at most once.)
np.count_nonzero(choices == 23) evaluates to the number
of Trues in choices == 23. If it is positive
(i.e. 1), it means that floor 23 was selected. If it is 0, it means
floor 23 was not selected.
Thus, np.count_nonzero(choices == 23) > 0 evaluates
to True if (and only if) floor 23 was selected.",75.0,Easy
315,Wi,22,Midterm,,Problem 7,Problem 7.2,"In the previous subpart of this question, your answer to blank (c)
contained the number 23, and the simulated probability was stored in the
variable prob_renovate.
Suppose, in blank (c), we change the number 23 to the number 46, and
we store the new simulated probability in the variable name
other_prob. (prob_renovate is unchanged from
the previous part.)
With these changes, which of the following is the most accurate
representation of the relationship between other_prob and
prob_renovate?

 other_prob will be roughly half of
prob_renovate
 other_prob will be roughly equal to
prob_renovate
 other_prob will be roughly double
prob_renovate","other_prob will be roughly
equal to prob_renovate
The calculation we did in the previous subpart was not specific to
the number 23. That is, we could have replaced 23 with any integer
between 1 and 75 inclusive and the simulation would have been just as
valid. The probability we estimated is the probability that any
one floor was randomly selected; there is nothing special about
23.
(We say “roughly equal” because the result may turn out slightly
different due to randomness.)",89.0,Easy
316,Wi,22,Midterm,,Problem 8,Problem 8,"While they are not skyscrapers, New Sixth College at UCSD has four
relatively tall residential buildings, which we’ll call Building A,
Building B, Building C, and Building D. Suppose each building has 10
floors.
Sixth College administration decides to ease the General Education
requirements for a few randomly selected students. Here’s their
strategy:

Wave 1: Select, at random, one floor from each
building.
Wave 2: Select, at random, one of the four floors
that was selected in Wave 1.

Everyone on one of the four floors selected in Wave 1 has the CAT 1
requirement waived. Everyone on the one floor selected in Wave 2 has
both the CAT 1 and CAT 2 requirements waived.",,,
317,Wi,22,Midterm,,Problem 8,Problem 8.1,"Billy lives on the 8th floor of Building C. What’s the probability
that Billy has both the CAT 1 and CAT 2 requirements waived? Give your
answer as a proportion between 0 and 1, rounded to 3 decimal places.","0.025
In order for the 8th floor of Building C to be selected, two things
need to happen:

In Wave 1, the 8th floor of Building C needs to be selected amongst
all 10 floors in Building C – this happens with probability \frac{1}{10}, since each floor in Building C
is equally likely to be selected.
In Wave 2, the 8th floor of Building C needs to be selected amongst
the 4 floors selected in Wave 1 – this happens with probability \frac{1}{4}, since each floor in Wave 1 is
equally likely to be selected.

Since both events need to occur, and both events are
independent (think of selecting in each wave as drawing names from a
hat), the probability that both occur is the product of the
probabilities that they occur individually:
\frac{1}{10} \cdot \frac{1}{4} =
\frac{1}{40} = 0.025",80.0,Easy
318,Wi,22,Midterm,,Problem 8,Problem 8.2,"What’s the probability that at least one of the top
(10th) floors of all four buildings are selected in Wave 1?
Give your answer as a proportion between 0 and 1, rounded to 3
decimal places.","0.344
Whenever we are asked to compute the probability of “at least one”
occurrence of some event, it is almost always easiest to compute the
complement of (i.e. “1 minus”) the probability that
there are no occurrences of the event. That is the case here; as such,
we need to compute the probability that none of the 10th floors are
selected across all four buildings.
To compute the probability that none of the 10th floors are selected
across all four buildings, we first need to find the probability that
the 10th floor is not selected in just a single building. This is 1 - \frac{1}{10} = \frac{9}{10}. Then, since
the selections in each building are independent of other buildings, the
probability that none of the 10th floors are selected across all four
buildings is \left( \frac{9}{10}
\right)^4.
Lastly, the probability we are asked for is the complement of the
probability we just computed. So, the probability that at least one 10th
floor is selected across all four buildings is
1 - \left( 1 - \frac{1}{10} \right)^4 = 1
- \left( \frac{9}{10} \right)^4 = 1 - 0.6561 = 0.3439
This rounds to 0.344.",64.0,Medium
319,Wi,23,Midterm,,Problem 1,Problem 1,,,,
320,Wi,23,Midterm,,Problem 1,Problem 1.1,"Which column would be an appropriate index for storms?

 'Name'
 'Date'
 'Time'
 'Latitude'
 None of these","None of these.
An index should be unique. In this case 'Name',
'Date', 'Time', and 'Latitude'
have repetative values, which does not make them
unique. Remember 'Name' will be reused every six years,
multiple hurricanes could happen on the same date, time, or
latitude.",69.0,Medium
321,Wi,23,Midterm,,Problem 2,Problem 2,,,,
322,Wi,23,Midterm,,Problem 2,Problem 2.1,"The ""Date"" column stores the year, month, and day
together in a single 8-digit int. The first four digits
represent the year, the next two represent the month, and the last two
represent the day. For example, 19500812 corresponds to
August 12, 1950. The get month function below takes one
such 8-digit int, and only returns the
month as an int. For example,
get month(19500812) evaluates to 8.
def get_month(date):
    return int((date % 10000) / 100)
Similarily, the get year and get day
functions below should each take as input an 8-digit int
representing a date, and return an int representing the
year and day, respectively. Choose the correct code to fill in the
blanks.
def get_year(date):
    return ____(a)____
def get_day(date):
    return ____(b)____
What goes in blank (a)?

 date / 10000
 int(date / 10000)
 int(date % 10000)
 int((date % 10000) / 10000)","int(date/10000)
The problem is asking us to find the code for blank (a) and
get_year is asking us to find the year. Let’s use
19500812 as an example, so we need to convert
19500812 to 1950. If we plug in Option 2
for (a) we will get 1950. This is because \frac{19500812}{10000} = 1950.0812, and when
int() is applied to 1950.0812 then it will drop the
decimal, which returns 1950.
Option A: date / 10000 If we plugged in
Option 1 into blank (a) we would get: \frac{19500812}{10000} = 1950.0812. This is a
float and is not equal to 1950.
Option C: int(date % 10000) If we
plugged in Option 3 into blank (a) we would get: 812. Remember,
% is the operation to find the remainder. We can manually
find the remainder by doing \frac{19500812}{10000} = 1950.0812, then
looking at the decimal, and noticing 812 cannot be divided by 10000
evenly. This is an int, but not the year.
Option D: int((date % 10000) / 10000)
If we plugged in Option 4 into blank (a) we would get: 0.0812. We get
this number by once again looking at the remainder of \frac{19500812}{10000} = 1950.0812, which is
812, and then dividing 812 by 10000. This is a float and is not equal to
1950.",67.0,Medium
323,Wi,23,Midterm,,Problem 2,Problem 2.2,"What goes in blank (b)?

 int(date / 100)
 int(date / 1000000)
 int((date % 100) / 10000)
 date % 100","date % 100
The problem is asking us to find the code for blank (b) and
get_day is asking us to get the day. Let’s use
19500812 as the example again, so we need to convert
19500812 to 12. Remember, % is the operation
to find the remainder. If we plug in Option 4 for (b)
we will get 12. This is because \frac{19500812}{100} = 195008.12 and by
looking at the decimal place we notice 12 cannot be divided by 100
evenly, making the remainder 12.
Option 1: int(date / 100) If we plugged
in Option 1 into blank (b) we would get 195008. This is because Python
would do: \frac{19500812}{100} =
195008.12 then would drop the decimal due to the
int() function. This is not the day.
Option 2: int(date / 1000000) If we
plugged in Option 2 into blank (b) we would get 19. This is because
Python would do: \frac{19500812}{1000000} =
19.500812 then would drop the decimal due to the
int() function. This is a day, but not the one we
are looking for.
Option 3: int((date % 100) / 10000) If
we plugged in Option 3 into blank (b) we would get 0. This is because
Python works from the inside out, so it will first evaluate the
remainder: \frac{19500812}{100} =
195008.12, by looking at the decimal place we notice 12 cannot be
divided by 100 evenly, making the remainder 12. Python then does \frac{12}{10000} = 0.0012. Remember that
int() drops the decimal, so by plugging a date into this
code it will return 0, which is not a day.",56.0,Medium
324,Wi,23,Midterm,,Problem 3,Problem 3,,,,
325,Wi,23,Midterm,,Problem 3,Problem 3.1,"The DataFrame amelia was created by the code below, and
is shown in its entirety.
amelia = (storms[(storms.get(""Name"") == ""AMELIA"") &
         (storms.get(""Year"") == 1978)]
         .get([""Year"", ""Month"", ""Day"", ""Time"",
         ""Status"", ""Latitude"", ""Longitude""]))


Use the space provided to show the DataFrame that results from
        amelia.groupby(""Status"").max()
The column labels should go in the top row, and the row labels
(index) should go in the leftmost row. You may not need to use all the
rows and columns provided.","Status
Year
Month
Day
Time
Latitude
Longtitude




TD
1978
8
31
6pm
29.3N
99.2W


TS
1978
7
31
6am
28.0N
98.2W



Remember that calling .groupby('column name') sets the
column name to the index. This means the 'Status' column
will become the new bolded index, which is found in the leftmost column
of the data frame. Another thing to know is Python will organize strings
in the index in alphabetical order, so for example
TS and TD both start with T, but
D comes sooner in the alphabet than S, which
makes row TD come before row
TS.
Next it is important to look at the .max(), which tells
us that we want the maximum element of each column that correspond to
the unique 'Status'. Recall how .max()
interacts with strings: .max() will organize strings in the
columns in descending order, so the last alphabetically/numerically.",59.0,Medium
326,Wi,23,Midterm,,Problem 4,Problem 4,,,,
327,Wi,23,Midterm,,Problem 4,Problem 4.1,"Suppose there are n different storms included in
storms. Say we create a new DataFrame from
storms by adding a column called 'Duration'
that contains the number of minutes since the first data entry for that
storm, as an int. The first few rows of this new DataFrame
are shown below.


Next we sort this DataFrame in ascending order of
'Duration' and save the result as
storms_by_duration. Which of the following statements must
be true? Select all that apply.

 The first n rows of storms_by_duration will
all correspond to different storms, because they will contain the first
reading from each storm in the data set.
 The last n rows of storms_by_duration will
all correspond to different storms, because they will contain the last
reading from each storm in the data set.
 storms_by_duration will contain exactly n rows.
 len(storms_by_duration.take(np.arange(n)).get(""Name"").unique())
will evaluate to n.","“The first n rows of
storms_by_duration will all correspond to different storms,
because they will contain the first reading from each storm in the data
set.”
Let’s first analyze the directions. According to the directions, we
added the column 'Duration', so we know how long each storm
lasted. Then we sorted the DataFrame in ascending order, which will put
the storms with the shortest duration at the top.
Each row will be tied to a unique storm because each storm can only
have one minimum. This means storms_by_duration’s first
n rows will contain the shortest duration for each unique
storm, which corresponds to the first option.
Option 2: This is incorrect because even though the
DataFrame is sorted in ascending order it is possible for a storm to
have multiple close values in 'Duration', which does not
guarantee unique storms in the last n rows. For example if
you had the storm 'alice', which one time had a duration of
60 and the longest duration of 62. The values will be sorted such that
60 will come before 62, but they are within the last n
values of the DataFrame, causing 'alice' to appear
twice.
Option 3: This is incorrect because there can be
more than n rows. It is possible that a storm appears
multiple times. For example the storm Anna occured three
different times on August 21, 1965 without sorting.
Option 4: This is incorrect. The code written will
take the first n rows of the table, get the names, and find
the number of unique named storms. Names are not unique, so it is
possible for the storms to share the same name. This can be seen in the
DataFrame example above.",66.0,Medium
328,Wi,23,Midterm,,Problem 5,Problem 5,,,,
329,Wi,23,Midterm,,Problem 5,Problem 5.1,"Latitude measures distance from the equator in the North or South
direction. Longitude measures distance from the prime meridian in the
East or West direction.

Since all of the United States lies north of the equator and west of
the prime meridian, the last character of each string in the
""Latitude"" column of storms is
""N"", and the last character of each string in the
""Longitude"" column is ""W"". This means that we
can refer to the latitude and longitude of US locations by their
numerical values alone, without the directions ""N"" and
""W"". The map below shows the latitude and longitude of the
continental United States in numerical values only.


Complete the function lat_long_numerical that takes as
input a string representing a value from either the
""Latitude"" or ""Longitude"" column of
storms, and returns that latitude or longitude as a
float. For example, lat_long_numerical(34.1N)
should return 34.1.

    def lat_long_numerical(lat_long):
        return ________
Hint: The string method .strip() takes as input
a string of characters and removes all instances of those characters at
the beginning and end of the string. For example,
""www.dsc10.com"".strip(""cmowz."") evaluates to
""dsc10"".
What goes in the blank? To earn credit, your answer
must use the string method: .strip().","float(lat_long.strip(""NW""))
According to the hint, .strip() takes as input of a
string of characters and removes all instances of those characters at
the beginning or the end of the string. The input we are given,
lat_long is the latitude and longitude, so we able to take
it and use .strip() to remove the ""N"" and
""W"". However, it is important to mention the number we now
have is still a string, so we put float() around it to
convert the string to a float.",70.0,Medium
330,Wi,23,Midterm,,Problem 5,Problem 5.2,"Assume that lat_long_numerical has been correctly
implemented. Which of the following correctly replaces the strings in
the 'Latitude' and 'Longitude' columns of
storms with float values? Select all that
apply.
Option 1:
    lat_num = storms.get('Latitude').apply(lat_long_numerical)
    long_num = storms.get('Longitude').apply(lat_long_numerical)
    storms = storms.drop(columns = ['Latitude', 'Longitude'])
    storms = storms.assign(Latitude = lat_num, Longitude = long_num)
Option 2:
    lat_num = storms.get('Latitude').apply(lat_long_numerical)
    long_num = storms.get('Longitude').apply(lat_long_numerical)
    storms = storms.assign(Latitude = lat_num, Longitude = long_num)
    storms = storms.drop(columns = ['Latitude', 'Longitude'])
Option 3:
    lat_num = storms.get('Latitude').apply(lat_long_numerical)
    long_num = storms.get('Longitude').apply(lat_long_numerical)
    storms = storms.assign(Latitude = lat_num, Longitude = long_num)

 Option 1
 Option 2
 Option 3","Option 1 and Option 3
Option 1 is correct because it applies the function
lat_long_numerical to the 'Latitude' and
'Longitude' columns. It then drops the old
'Latitude' and 'Longitude' columns. Then
creates new 'Latitude' and 'Longitude' columns
that contain the float versions.
Option 3 is correct because it applies the function
lat_long_numerical to the 'Latitude' and
'Longitude' columns. It then re-assigns the
'Latitude' and 'Longitude' columns to the
float versions.
Option 2 is incorrect because it re-assigns the
'Latitude' and 'Longitude' columns to the
float versions and then drops the 'Latitude' and
'Longitude' columns from the DataFrame.",86.0,Easy
331,Wi,23,Midterm,,Problem 6,Problem 6,,,,
332,Wi,23,Midterm,,Problem 6,Problem 6.1,"Storm forecasters are very interested in the direction in which a
tropical storm moves. This direction of movement can be determined by
looking at two consecutive rows in storms that correspond
to the same storm.
The function direction takes as input four values: the
latitude and longitude of a data entry for one storm, and the latitude
and longitude of the next data entry for that same storm. The function
should return the direction in which the storm moved in the period of
time between the two data entries. The return value should be one of the
following strings:

""NE"" for Northeastern movement,
""NW"" for Northwestern movement,
""SW"" for Southwestern movement, or
""SE"" for Southeastern movement



For example, direction(23.1, 75.1, 23.4, 75.7) should
return ""NW"". If the storm happened to move
directly North, South, East, or West, or if the storm did not
move at all, the function may return any one of ""NE"",
""NW"", ""SW"", or ""SE"". Fill in the
blanks in the function direction below. It may help to
refer to the images on Page 5.
def direction(old_lat, old_long, new_lat, new_long):
    if old_lat > new_lat and old_long > new_long:
        return ____(a)____
    elif old_lat < new_lat and old_long < new_long:
        return ____(b)____
    elif old_lat > new_lat:
        return ____(c)____
    else:
        return ____(d)____
What goes in blank (a)?

 ""NE""
 ""NW""
 ""SW""
 ""SE""","""SE""
According to the given example that
direction(23.1, 75.1, 23.4, 75.7) should return
""NW"", we learn that having an old_lat <
new_lat (in the example: 23.1
< 23.4) causes for the storm to move North and that having a
old_long < new_long (in the example: 75.1 < 75.7) causes for the storm to move
West. This tells us if old_lat > new_lat
then the storm will move South, the opposite direction of North, and if
old_long > new_long then the storm will
move East, the opposite direction of West.
The if statement is looking for the direction where
(old_lat > new_lat and
old_long > new_long), so from above we can
tell it is moving Southeast. Remember both conditions must be satisfied
for this if statement to execute.",73.0,Medium
333,Wi,23,Midterm,,Problem 6,Problem 6.2,"What goes in blank (b)?

 ""NE""
 ""NW""
 ""SW""
 ""SE""","""NW""
Recall from the previous section the logic that:

North: old_lat < new_lat
South: old_lat > new_lat
East: old_long > new_long
West: old_long < new_long

This means we are looking for the directions that satisfy the
elif statement: old_lat < new_lat and
old_long < new_long. Looking at our logic the statement
is satisfied by the direction Northwest.",79.0,Easy
334,Wi,23,Midterm,,Problem 6,Problem 6.3,"What goes in blank (c)?

 ""NE""
 ""NW""
 ""SW""
 ""SE""","""SW""
We know the answers cannot be 'SE' or 'NW'
because the if statement and elif statement above the one we are
currently working in will catch those. This tells us we are either
working with 'SW' or 'NE'. From the logic we
established in the previous subparts we know when 'old_lat'
> 'new_lat' we know the storm is going in the Southern
direction. This means the only possible answer is 'SW'.",65.0,Medium
335,Wi,23,Midterm,,Problem 6,Problem 6.4,"What goes in blank (d)?

 ""NE""
 ""NW""
 ""SW""
 ""SE""","""NE""
The only option we have left is 'NE'. Remember than
Python if statements will run every single if, and if none
of them are triggered then it will move to the elif
statements, and if none of those are triggered then it will finally run
the else statement. That means whatever direction not used
in parts a, b, and c needs to be used here.",61.0,Medium
336,Wi,23,Midterm,,Problem 7,Problem 7,,,,
337,Wi,23,Midterm,,Problem 7,Problem 7.1,"The most famous Hurricane Katrina took place in August, 2005. The
DataFrame katrina_05 contains just the rows of
storms corresponding to this hurricane, which we’ll call
Katrina’05.
Fill in the blanks in the code below so that
direction_array evaluates to an array of directions (each
of which is""NE"", ""NW"", ""SW"", or
""SE"") representing the movement of Katrina ’05 between each
pair of consecutive data entries in katrina_05.
    direction_array = np.array([])
    for i in np.arange(1, ____(a)____):
        w = katrina_05.get(""Latitude"").____(b)____
        x = katrina_05.get(""Longitude"").____(c)____
        y = katrina_05.get(""Latitude"").____(d)____
        z = katrina_05.get(""Longitude"").____(e)____
        direction_array = np.append(direction_array, direction(w, x, y, z))
What goes in blank (a)?","katrina_05.shape[0]
In this line of code we want to go through the entire Katrina’05
DataFrame. We are provided the inclusive start, but we need to find the
exclusive stop, which would be the number of rows in the DataFrame.
katrina_05.shape[0] takes the DataFrame, finds the shape,
which is represented by: (rows, columns), and then isolates
the rows in the first index.",46.0,Hard
338,Wi,23,Midterm,,Problem 7,Problem 7.2,What goes in blank (b)?,"iloc[i-1]
In this line of code we want to find the latitude of the row we are
in. The whole line of code:
katarina_05.get(""Latitude"").iloc[i-1] isolates the column
'Latitude' and uses iloc, which is a purely
integer-location based indexing function, to select the element at
position i-1. The reason we are doing i-1 is
because the for loop started an np.array at 1. For example,
if we wanted the latitude at index 0 we would need to do
iloc[1-1] to get the equivalent iloc[0].",37.0,Hard
339,Wi,23,Midterm,,Problem 7,Problem 7.3,What goes in blank (c)?,"iloc[i-1]
Similarily to part b, this line of code will find the longitude of
the row we are in. The whole line of code:
katarina_05.get(""Longitude"").iloc[i-1] isolates the column
'Longitude' and uses iloc, which is a purely
integer-location based indexing function, to select the element at
position i-1. The reason we are doing i-1 is
because the for loop started an np.array at 1.",36.0,Hard
340,Wi,23,Midterm,,Problem 7,Problem 7.4,What goes in blank (d)?,"iloc[i]
Now we are trying to find the “next” latitude, which will be the next
coordinate point that the storm Katrina moved to. This
means we want to find the latitude after the one we
found in part b. The whole line of code:
katarina_05.get(""Latitude"").iloc[i] isolates the
""Latitude"" column, and uses iloc to choose the
element at position i. Recall, the for loop starts at 1, so
it will always be the “next” element comparatively to part b, where we
substracted 1.",37.0,Hard
341,Wi,23,Midterm,,Problem 7,Problem 7.5,What goes in blank (e)?,"iloc[i]
Similarily to part d, we are trying to find the “next” longitude,
which will be the next coordinate point that the storm
Katarina moved to. This means we want to find the longitude
after the one we found in part c. The whole line of
code: katarina_05.get(""Longitude"").iloc[i] isolates the
column 'Longitude' and uses iloc to choose the
element at position i. Recall, the for loop starts at 1, so
it will always be the “next” element comparitively to part c, where we
substracted 1.",37.0,Hard
342,Wi,23,Midterm,,Problem 8,Problem 8,,,,
343,Wi,23,Midterm,,Problem 8,Problem 8.1,"Now we want to use direction_array to find the number of
times that Katrina ’05 changed directions, or moved in a different
direction than it was moving immediately prior. For example, if
direction_array contained values ""NW"",
""NE"", ""NE"", ""NE"",
""NW"", we would say that there were two direction changes
(once from ""NW"" to ""NE"", and another from
""NE"" to ""NW""). Fill in the blanks so that
direction_changes evaluates to the number of times that
Katrina ’05 changed directions.
    direction_changes = 0
    for j in ____(a)____:
        if ____(b)____:
            direction_changes = direction_changes + 1
What goes in blank (a)?","np.arange(1, len(direction_array))
We want the for loop to execute the length of the given
direction_array because we compare all of the directions
inside of it. The line of code:
np.arange(1, len(direction_array)) will create an array
that is as large as the direction_array, which allows us to
use j to access different indexes inside of
direction_array.",36.0,Hard
344,Wi,23,Midterm,,Problem 8,Problem 8.2,What goes in blank (b)?,"direction_array[j] != direction_array[j-1]
We want to increase direction_changes whenever there is
a switch between directions. This means we want to see if
direction_array[j] is not equal to
direction_array[j-1].
It is important to look at how the for loop is running
because it starts at 1 and ends at len(direction_array) - 1
(this is because np.arange has an exclusive stop). Let’s
say we wanted to compare the direction at index 5 and index 4. We can
easily get the element at index 5 by doing
direction_array[j] and then get the element at index 4 by
doing direction_array[j-1]. We can then check if they are
not equal to each other by using the !=
operation.
When the if statement activates it will then update the
direction_changes",64.0,Medium
345,Wi,23,Midterm,,Problem 8,Problem 8.3,"There are 34 rows in katrina_05. Based on this
information alone, what is the maxi- mum possible value of
direction_changes?","32
Note: The maximum amount of direction changes would mean that every
other direction would be different from each other.
It is also important to remember what we are feeding
into direction_changes. We are feeding in the output from
direction_array from",,
346,Wi,23,Midterm,,Problem 7,Problem 7.,"Due to the
for-loops throughout this section we can see the maximum
amount of changes by 2. Once in direction_array and once in
direction_changes.
For a more visual explanation, let’s imagine katrina_05
has these values:
[(7, 3), (6, 4), (9, 2), (8, 7), (3, 5)]
We would then feed this into",,,
347,Wi,23,Midterm,,Problem 7,Problem 7,"’s direction_array
to get an array that looks like this:
[""NE"", ""SW"", ""SE"", ""NW""]
Finally, we will feed this array into direction_changes.
We can see that there are three changes: one from ""NE"" to
""SW"", one from ""SW"" to ""SE"", and
one from ""SE"" to ""NW"".
This means that the maximum amount of direction_changes
is 34-1-1 = 32. We subtract 1 for each
step in the process because we lose a value due to the
for-loops.

Difficulty: ⭐️⭐️⭐️⭐️


The average score on this problem was 36%.",,36.0,Hard
348,Wi,23,Midterm,,Problem 9,Problem 9,,,,
349,Wi,23,Midterm,,Problem 9,Problem 9.1,"The DataFrame directors contains historical information about the
director of the National Hurricane Center (NHC). A preview of directors
is shown below.


We would like to merge storms with
directors to produce a DataFrame with the same information
as storms plus one additional column with the name of the
director who was leading the NHC at the time of each storm. However,
when we try to merge with the command shown below, Python fails to
produce the desired DataFrame.
directors.merge(storms, left_on=""Tenure"", right_on=""Year"")
Which of the following is a problem with our attempted merge?
Select all that apply.

 We cannot merge these two DataFrames because they have two completely
different sets of column names.
 We want to add information about the directors to storms, so we need
to use storms as our left DataFrame. The command should start with
storms.merge(directors).
 The directors DataFrame does not contain enough
information to determine who was the director of the NHC at the time of
each storm.
 The ""Tenure"" column of directors contains a different
data type than the ""Year"" column of storms.","Option 3 and Option 4
Recall that
left_df.merge(right_df, left_on='column_a', right_on='column_b')
merges left_df to the right_df and specifies
which columns from the DataFrame to use as keys by using
left_on= and right_on=. This means that
column_a becomes the key for the left DataFrame and
column_b becomes the key for the right DataFrame. That
means the column names do not need to be the same. The important part of
this is 'column_a' and 'column_b' should be
the same data type and contain the same information for the merge to be
successful.
Option 4 is correct because the years are formatted differenntly in
storms and in directors. In
storms the column ""Year"" contains an int,
which is the year, whereas in ""Tenure"" the column contains
a string to represent a span of years. When we try to merge there is no
overlap between values in these columns. There will actually be an error
because we are trying to merge two columns of different types.
Option 3 is correct because the merge will fail to happen due to the
error we see caused by the columns containing values with
no overlap.
Option 1: Is incorrect because you can merge
DataFrames with different column names using left_on and
right_on.
Option 2: Is incorrect because regardless of the
left or right DataFrames if done correctly
they will merge together. This means the order of the DataFrames does
not make an impact.",61.0,Medium
350,Wi,23,Midterm,,Problem 10,Problem 10,,,,
351,Wi,23,Midterm,,Problem 10,Problem 10.1,"Recall that all the named storms in storms occurred
between 1965 and 2015, a fifty-year time period.
Below is a histogram and the code that produced it. Use it to answer
the questions that follow.
    (storms.groupby([""Name"", ""Year""]).count()
        .reset_index()
        .plot(kind=""hist"", y=""Year"",
            density=True, ec=""w"",
            bins=np.arange(1965, 2020, 5)));


Approximately (a) percent of named storms in this
fifty-year time period occurred in 1995 or later. Give your answer to
the nearest multiple of five.","55
We can find the percentage of named storms by using the fact: a
histograms’ area is always normalized to 1. This means we can calculate
the area of the rectangle, also known as width * height, to the left of
1995. The height, which is the frequency, is about 0.015 and the width
is the difference between 1995 and 1965. This gives us: 1 - (1995 - 1965) * 0.015 = 0.55. Next to
convert this to a percentage we multiply by 100, giving us: 0.55 * 100 = 55\%.",52.0,Medium
352,Wi,23,Midterm,,Problem 10,Problem 10.2,"True or False? The line plot generated by the code below will have no
downward-sloping segments after 1995.
    (storms.groupby([""Name"", ""Year""]).count()
        .reset_index()
        .groupby(""Year"").count()
        .plot(kind=""line"", y=""Name"");

 True
 False","False
The previous histogram shows upward-sloping segments for each 5-year
period after 1995; however, we are now ploting a line graph that shows a
continuous timeline. Thus, there may be downward-sloping that happens
within any 5-year periods.",51.0,Medium
353,Wi,23,Midterm,,Problem 11,Problem 11,,,,
354,Wi,23,Midterm,,Problem 11,Problem 11.1,"The code below defines a variable called
month formed.
month_formed = (storms.groupby([""Name"", ""Year""]).min()
                      .reset_index()
                      .groupby(""Month"").count()
                      .get(""Name""))
What is the data type of month formed?

 int
 str
 Series
 Dataframe
 None of these","Series
It’s helpful to analyze the code piece by piece. The first part is
doing .groupby([""Name"", ""Year""]).min(), which will index
both ""Name"" and ""Year"" and find the minimum
values in the DataFrame. We are still working with a DataFrame at this
point. The next part .reset_index() makes
""Name"" and ""Year"" columns again. Again, this
is a DataFrame. The next part .groupby(""Month"").count()
makes ""Month"" the index and gets the count for each element
in the DataFrame. Finally, .get(""Name"") isolates the
""Name"" column and returns to month_formed a
series.",81.0,Easy
355,Wi,23,Midterm,,Problem 11,Problem 11.2,"Which of the following expressions evaluates to the proportion of
storms in our data set that were formed in August?

 month_formed.loc[8]/month_formed.sum()
 month_formed.iloc[7]/month_formed.sum()
 month_formed[month_formed.index == 8].shape[0]/month_formed.sum()
 month_formed[month_formed.get(""Month"") == 8].shape[0]/month_formed.sum()","month_formed.loc[8]/month_formed.sum()
Option 1: Recall that August is the eigth month, so
using .loc[8] will find the label 8 in
month_formed, which will be counts or the
number of storms formed in August. Dividing the number of storms formed
in August by the total number of storms formed will give us the
proportion of storms that formed in August.
Option 2: It is important to realize that the months
have become the index of month_formed, but that doesn’t
necessarily mean that the index starts in January or that there have
been storm during a month before August. For example if there were no
storms in March then there would be no 3 in the index. Recall
.iloc[7] is indexing for whatever is in position 7, but
because the index is not guaranteed we cannot be certain the
.iloc[7] will return August.
Option 3: The code:
month_formed[month_formed.index == 8].shape[0] will return
1. Finding the index at month 8 will give us August, but doing
.shape[0] gives us the number of rows in August, which
should only be 1 because of groupby. This means that Option
3’s line of code will not give us the number of storms that formed in
August, which makes it impossible to find the propotion.
Option 4: Remember that months_formed’s
index is ""Month"". This means that there is no column
""Month"", so the code will error, meaning it cannot give us
proportions of storms that formed in August.",35.0,Hard
356,Wi,23,Midterm,,Problem 12,Problem 12,,,,
357,Wi,23,Midterm,,Problem 12,Problem 12.1,"Hurricane forecasters use complex models to simulate hurricanes.
Suppose a forecaster simulates 10,000 hurricanes and keeps track of the
state where each hurricane made landfall in an array called
landfalls. Each element of landfalls is a
string, which is either the full name of a US state or the string
""None"" if the storm did not hit land in the simulation.
The forecaster wants to use the results of their simulation to
estimate the probability that a given storm hits Georgia. Write one line
of Python code that approximates this probability, using the data in
landfalls.","np.count_nonzero(landfalls == ""Georgia"") / 10000
The probability would be the number of times the storm hit Georgia
over the total number of simulated hurricanes, which is 10000. Since
each element of landfills is a string, we can use
np.count_nonzero() to give us all of the times the storm
hits Georgia in the simulation. Recall np.count_nonzero()
determines whether the elements are “truthy”, so if the String was equal
to ""Georgia"" then it would be counted, and otherwise would
be ignored. Then to calculate the probability we simply divide the
number of times the storm hit Georgia by 10000.",34.0,Hard
358,Wi,23,Midterm,,Problem 12,Problem 12.2,"Oh no, a hurricane is forming! Experts predict that the storm has a
45% chance of hitting Florida, a 25% chance of hitting Georgia, a 5%
chance of hitting South Carolina, and a 25% chance of not making
landfall at all.
Fast forward: the storm made landfall. Assuming the expert
predictions were correct, what is the probability that the storm hit
Georgia? Give your answer as a fully simplified
fraction between 0 and 1.
Hint: The answer is not \frac{1}{4}, or 25%.","Originally, we were unsure if the hurricane would make landfall. Now
that it has our total percentage has changed: 100\% - 25\% = 75\%. Remember probability
should always add up to 100%. So our new total is 75%. We can calculate
the probability that a storm hits Georgia by doing: \frac{25\%}{75\%} = \frac{1}{3}.",36.0,Hard
359,Wi,24,Midterm,,Problem 1,Problem 1,"Each of the following expressions evaluates to an integer. Determine
the value of that integer, if possible, or circle “not enough
information.""
Important: Before proceeding, make sure to read the
page called Clue: The Murder Mystery Game.",,,
360,Wi,24,Midterm,,Problem 1,Problem 1.1,"(clue.get(""Cardholder"") == ""Janine"").sum() ","6
This code counts the number of times that Janine appears in the
Cardholder column. This is because
clue.get(""Cardholder"") == ""Janine"" will return a Series of
True and False values of length 22 where
True corresponds to a card belonging to Janine. Since 6
cards were dealt to her, the expression evaluates to 6.",78.0,Easy
361,Wi,24,Midterm,,Problem 1,Problem 1.2,"np.count_nonzero(clue.get(""Category"").str.contains(""p"")) ","13
This code counts the number of cells that contain that letter
""p"" in the Category column.
clue.get(""Category"").str.contains(""p"") will return a Series
that contains True if ""p"" is part of the entry
in the ""Category"" column and False otherwise.
The words ""suspect"" and ""weapons"" both contain
the letter ""p"" and since there are 6 and 7 of each
respectively, the expression evaluates to 13.",75.0,Easy
362,Wi,24,Midterm,,Problem 1,Problem 1.3,"clue[(clue.get(""Category"") == ""suspect"") & (clue.get(""Cardholder"") == ""Janine"")].shape[0] ","not enough information
This code first filters only for rows that contain both
""suspect"" as the category and ""Janine"" as the
cardholder and returns the number of rows of that DataFrame with
.shape[0]. However, from the information given, we do not
know how many ""suspect"" cards Janine has.",83.0,Easy
363,Wi,24,Midterm,,Problem 1,Problem 1.4,"len(clue.take(np.arange(5, 20, 3)).index) ","5
np.arange(5, 20, 3) is the arary
np.array([5, 8, 11, 14, 17]). Recall that
.take will filter the DataFrame to contain only certain
rows, in this case rows 5, 8, 11, 14, and 17. Next, .index
extracts the index of the DataFrame, so the length of the index is the
same as the number of rows contained in the DataFrame. There are 5
rows.",69.0,Medium
364,Wi,24,Midterm,,Problem 1,Problem 1.5,"len(clue[clue.get(""Category"") >= ""this""].index) ","7
Similarly to the previous problem, we are getting the number of rows
of the DataFrame clue after filtering it.
clue.get(""Category"") >= ""this"" returns a Boolean Series
where True is returned when a string in ""Category"" is
greater than alphabetically than ""this"". This only happens
when the string is ""weapon"", which occurs 7 times.",29.0,Hard
365,Wi,24,Midterm,,Problem 1,Problem 1.6,"clue.groupby(""Cardholder"").count().get(""Category"").sum() ","22
groupby(""Cardholder"").count() will return a DataFrame
indexed by ""Cardholder"" where each column contains the
number of cards that each ""Cardholder"" has. Then we sum the
values in the ""Category"" column, which evaluates to 22
because the sum of the total number of cards each cardholder has is the
total number of cards in play!",52.0,Medium
366,Wi,24,Midterm,,Problem 2,Problem 2,"Since Janine’s knowledge of who holds each card will change
throughout the game, the clue DataFrame needs to be updated
by setting particular entries.
Suppose more generally that we want to write a function that changes
the value of an entry in a DataFrame. The function should work for any
DataFrame, not just clue.
What parameters would such a function require? Say what each
parameter represents.","We would need four parameters:

df, the DataFrame to change.
row, the row label or row number of the entry to
change.
col, the column label of the entry to
change.
val, the value that we want to store at that
location.",43.0,Hard
367,Wi,24,Midterm,,Problem 3,Problem 3,"An important part of the game is knowing when you’ve narrowed it down
to just one suspect with one weapon in one room. Then you can make your
accusation and win the game!
Suppose the DataFrames grouped and filtered
are defined as follows.
    grouped = (clue.reset_index()
                   .groupby([""Category"", ""Cardholder""])
                   .count()
                   .reset_index())
    filtered = grouped[grouped.get(""Cardholder"") == ""Unknown""]",,,
368,Wi,24,Midterm,,Problem 3,Problem 3.1,"Fill in the blank below so that ""Ready to accuse"" is
printed when Janine has enough information to make an accusation and win
the game.
    if filtered.get(""Card"").______ == 3:
        print(""Ready to accuse"")
What goes in the blank?

 count()
 sum()
 max()
 min()
 shape[0]","sum()
It is helpful to first visualize how both the grouped
(left) and filtered (right) DataFrames could look:




grouped DataFrame contains the number of cards for a certain ""Category""/""Cardholder"" combination.



filtered DataFrame contains the number of cards that are ""Unknown"" by ""room"", ""suspect"", and ""weapon"".



Now, let’s think about the scenario presented. We want a method that
will return 3 from filtered.get(""Card"").___. We do not use
count() because that is an aggregation function that
appears after a .groupby, and there is no grouping
here.
According to the instructions, we want to know when we narrowed it
down to just one suspect with one weapon in one room.
This means for filtered DataFrame, each row should have 1
in the ""Card"" column when you are already to accuse.
sum() works because when you have only 1 unknown card for
each of the three categories, that means you have a sum of 3 unknown
cards in total. You can make an accusation now!",50.0,Medium
369,Wi,24,Midterm,,Problem 3,Problem 3.2,"Now, let’s look at a different way to do the same thing. Fill in the
blank below so that ""Ready to accuse"" is printed when
Janine has enough information to make an accusation and win the
game.
    if filtered.get(""Card"").______ == 1:
        print(""Ready to accuse"")
What goes in the blank?

 count()
 sum()
 max()
 min()
 shape[0]","max()
This problem follows the same logic as the first except we only want
to accuse when filtered.get(""Card"").___ == 1. As we saw in
the previous part, we only want to accuse when all the numbers in the
""Card"" column are 1, as this represents one unknown in each
category. This means the largest number in the ""Card""
column must be 1, so we can fill in the blank with
max().",40.0,Hard
370,Wi,24,Midterm,,Problem 4,Problem 4,"When someone is ready to make an accusation, they make a statement
such as:
“It was Miss Scarlett with the dagger in the study""
While the suspect, weapon, and room may be different, an accusation
will always have this form:
“It was ______ with the ______ in the ______""
Suppose the array words is defined as follows (note the
spaces).
    words = np.array([""It was "", "" with the "", "" in the ""])
Suppose another array called answers has been defined.
answers contains three elements: the name of the suspect,
weapon, and room that we would like to use in our accusation, in that
order. Using words and answers, complete the
for-loop below so that accusation is a string,
formatted as above, that represents our accusation.
    accusation = """"
    for i in ___(a)___:
        accusation = ___(b)___",,,
371,Wi,24,Midterm,,Problem 4,Problem 4.1,What goes in blank (a)?,"[0, 1, 2]
answers could potentially look like this array
np.array(['Mr. Green', 'knife', 'kitchen']). We want
accusation to be the following: “It was Mr. Green
with the knife in the kitchen” where the
underline represent the string from the words array and the
nonunderlined parts represent the string from the answers
array. In the for loop, we want to iterate through words and answers
simultaneously, so we can use [0, 1, 2] to represent the
indices of each array we will be iterating through.",52.0,Medium
372,Wi,24,Midterm,,Problem 4,Problem 4.2,What goes in blank (b)?,"accusation + words[i] + answers[i]
We are performing string concatenation here. Using the example from
above, we want to add to the string accusation in order of
accusation, words, answer. After
all, we want “It was” before “Mr. Green”.",56.0,Medium
373,Wi,24,Midterm,,Problem 5,Problem 5,"Recall that the game Clue comes with 22 cards, one for each
of the 6 suspects, 7 weapons, and 9 rooms. One suspect card, one weapon
card, and one room card are chosen randomly, without being looked at,
and placed aside in an envelope. The remaining 19 cards (5 suspects, 6
weapons, 8 rooms) are randomly shuffled and dealt out, splitting them as
evenly as possible among the players. Suppose in a three-player game,
Janine gets 6 cards, which are dealt one at a time.
Answer the probability questions that follow. Leave your answers
unsimplified.",,,
374,Wi,24,Midterm,,Problem 5,Problem 5.1,"Cards are dealt one at a time. What is the probability that the first
card Janine is dealt is a weapon card?","\frac{6}{19}
The probability of getting a weapon card is just the number of weapon
cards divided by the total number of cards. There are 6 weapon cards and
19 cards total, so the probability has to be \frac{6}{19}. Note that it does not matter
how the cards were dealt. Though each card is dealt one at a time to
each player, Janine will always end up with a randomly selected 6 cards,
out of the 19 cards available.",80.0,Easy
375,Wi,24,Midterm,,Problem 5,Problem 5.2,"What is the probability that all 6 of Janine’s cards are weapon
cards?","\frac{6}{19} \cdot
\frac{5}{18} \cdot \frac{4}{17} \cdot \frac{3}{16} \cdot \frac{2}{15}
\cdot \frac{1}{14}
We can calculate the answer using the multiplication rule. The
probability of getting Janine getting all the weapon cards is the
probability of getting a dealt a weapon card first multiplied by the
probability of getting a weapon card second multiplied by continuing
probabilities of getting a weapon card until probability of getting a
weapon card on the sixth draw. The denominator of each subsequent
probability decreases by 1 because we remove one card from the total
number of cards on each draw. The numerator also decreases by 1 because
we remove a weapon card from the total number of available weapon cards
on each draw.",,
376,Wi,24,Midterm,,Problem 5,Problem 5.3,"Determine the probability that exactly one of the first two cards
Janine is dealt is a weapon card. This probability can be expressed in
the form \frac{k \cdot (k + 1)}{m \cdot (m +
1)} where k and m are integers. What are the
values of k and m?
Hint: There is no need for any sort of calculation
that you can’t do easily in your head, such as long division or
multiplication.","k = 12,
m = 18
m has to be 18 because the
denominator is the number of cards available during the first and second
draw. We have 19 cards on the first draw and 18 on the second draw, so
the only way to get that is for m =
18.
The probability that exactly one of the cards of your first two draws
is a weapon card can be broken down into two cases: getting a weapon
card first and then a non-weapon card, or getting a non-weapon card
first and then a weapon card. We add the probabilities of the two cases
together in order to calculate the overall probability, since the cases
are mutually exclusive, meaning they cannot both happen at the same
time.
Consider first the probability of getting a weapon card followed by a
non-weapon card. This probability is \frac{6}{19} \cdot \frac{13}{18}. Similarly,
the probability of getting a non-weapon card first, then a weapon card,
is \frac{13}{19} \cdot \frac{6}{18}.
The sum of these is \frac{6 \cdot 13}{19 \cdot
18} + \frac{13 \cdot 6}{19 \cdot
18}.
Since we want the numerator to look like k
\cdot (k+1), we want to combine the terms in the numerator. Since
the fractions in the sum are the same, we can represent the probability
as 2 \cdot \frac{6}{19} \cdot
\frac{13}{18}. Since 2\cdot 6 =
12, we can express the numerator as 12
\cdot 13, so k = 12.",31.0,Hard
377,Wi,24,Midterm,,Problem 6,Problem 6,"Which of the following probabilities could most easily be
approximated by writing a simulation in Python? Select the best
answer.

 The probability that Janine wins the game.
 The probability that a three-player game takes less than 30 minutes
to play.
 The probability that Janine has three or more suspect cards.
 The probability that Janine visits the kitchen at some point in the
game.","The probability that Janine has three or
more suspect cards.
Let’s explain each choice and why it would be easy or difficult to
simulate in Python. The first choice is difficult
because these simulations depend on Janine’s strategies and decisions in
the game. There is no way to simulate people’s choices. We can only
simulate randomness. For the second choice, we are not
given information on how long each part of the gameplay takes, so we
would not be able to simulate the length of a game. The third
choice is very plausible to do because when cards are dealt out
to Janine, this is a random process which we can simulate in code, where
we keep track of whether she has three of more suspect cards. The
fourth choice follows the same reasoning as the first
choice. There is no way to simulate Janine’s moves in the game, as it
depends on the decisions she makes while playing.",83.0,Easy
378,Wi,24,Midterm,,Problem 7,Problem 7,"Part of the gameplay of Clue involves moving around the
gameboard. The gameboard has 9 rooms, arranged on a grid, and players
roll dice to determine how many spaces they can move.
The DataFrame dist contains a row and a column for each
of the 9 rooms. The entry in row r and
column c represents the shortest
distance between rooms r and c on the Clue gameboard, or the
smallest dice roll that would be required to move between rooms r and c.
Since you don’t need to move at all to get from a room to the same room,
the entries on the diagonal are all 0.
dist is indexed by ""Room"", and the room
names appear exactly as they appear in the index of the
clue DataFrame. These same values are also the column
labels in dist.",,,
379,Wi,24,Midterm,,Problem 7,Problem 7.1,"Two of the following expressions are equivalent, meaning they
evaluate to the same value without erroring. Select these two
expressions.

 dist.get(""kitchen"").loc[""library""]
 dist.get(""kitchen"").iloc[""library""]
 dist.get(""library"").loc[""kitchen""]
 dist.get(""library"").iloc[""kitchen""]

Explain in one sentence why these two expressions
are the same.","dist.get(""kitchen"").loc[""library""] and
dist.get(""library"").loc[""kitchen""]
dist.get(""kitchen"").iloc[""library""] and
dist.get(""library"").iloc[""kitchen""] are both wrong because
they uses iloc inappropriately. iloc[] takes
in an integer number representing the location of column, row, or cell
you would like to extract and it does not take a column or index
name.
dist.get(""kitchen"").loc[""library""] and
dist.get(""library"").loc[""kitchen""] lead to the same answer
because the DataFrame has a unique property! The entry at r, c is the
same as the entry at c, r because both are the distances for the same
two rooms. The distance from the kitchen to library is the same as the
distance from the library to kichen.",84.0,Easy
380,Wi,24,Midterm,,Problem 7,Problem 7.2,"On the Clue gameboard, there are two “secret passages."" Each
secret passage connects two rooms. Players can immediately move through
secret passages without rolling, so in dist we record the
distance as 0 between two rooms that are connected with a secret
passage.
Suppose we run the following code.
    nonzero = 0
    for col in dist.columns:
        nonzero = nonzero + np.count_nonzero(dist.get(col))
Determine the value of nonzero after the above code is
run.","nonzero = 68
The nonzero variable represents the entries in the
DataFrame where the distance between two rooms is not 0. There are 81
entries in the DataFrame because there are 9 rooms and 9 \cdot 9 = 81. Since the diagonal of the
DataFrame is 0 (due to the distance from a room to itself being 0), we
know there are at most 72 = 81 - 9
nonzero entries in the DataFrame.
We are also told that there are 2 secret passages, each of which
connects 2 different rooms, meaning the distance between these rooms is
0. Each secret passage will cause 2 entries in the DataFrame to have a
distance of 0. For instance, if the secret passage was between the
kitchen and dining room, then the distance from the kitchen to the
dining room would be 0, but also the distance from the dining room to
the kitchen would be 0. Since there are 2 secret passages and each gives
rise to 2 entries that are 0, this is 4 additional entries that are 0.
This means there are 68 nonzero entries in the DataFrame, coming from
81 - 9 - 4 = 68.",28.0,Hard
381,Wi,24,Midterm,,Problem 7,Problem 7.3,"Fill in blanks so that the expression below evaluates to a DataFrame
with all the same information as dist, plus one
extra column called ""Cardholder"" containing
Janine’s knowledge of who holds each room card.
    dist.merge(___(a)___, ___(b)___, ___(c)___)

What goes in blank (a)?
What goes in blank (b)?
What goes in blank (c)?","(a): clue.get([""Cardholder""])
(b): left_index=True
(c): right_index=True

Since we want to create a DataFrame that looks like dist
with an extra column of ""Cardholder"", we want to extract
just that column from clue to merge with dist.
We do this with clue.get([""Cardholder""]). This is necessary
because when we merge two DataFrames, we get all columns from either
DataFrame in the end result.
When deciding what columns to merge on, we need to look for columns
from each DataFrame that share common values. In this case, the common
values in the two DataFrames are not in columns, but in the index, so we
use left_index=True and right_index=True.",28.0,Hard
382,Wi,24,Midterm,,Problem 7,Problem 7.4,"Suppose we generate a scatter plot as follows.
    dist.plot(kind=""scatter"", x=""kitchen"", y=""study"");
Suppose the scatterplot has a point at (4, 6). What can we conclude
about the Clue gameboard?

 The kitchen is 4 spaces away from the study.
 The kitchen is 6 spaces away from the study.
 Another room besides the kitchen is 4 spaces away from the study.
 Another room besides the kitchen is 6 spaces away from the study.","Another room besides the kitchen is 6 spaces
away from the study.
Let’s explain each choice and why it is correct or incorrect. The
scatterplot shows how far a room is from the kitchen (as shown by values
on the x-axis) and how far a room is from the study (as shown by the
values on the y-axis). Each room is represented by a point. This means
there is a room that is 4 units away from the kitchen and 6 units away
from the study. This room can’t be the kitchen or study itself, since a
room must be distance 0 from itself. Therefore, we conclude, based on
the y-coordinate, that there is a room besides the kitchen that is 6
units away from the study.",47.0,Hard
383,Wi,24,Midterm,,Problem 8,Problem 8,"The histogram below shows the distribution of game times in minutes
for both two-player and three-player games of Clue, with each
distribution representing 1000 games played.",,,
384,Wi,24,Midterm,,Problem 8,Problem 8.1,"How many more three-player games than two-player
games took at least 50 minutes to play? Give your answer as an
integer, rounded to the nearest multiple of 10.","80
First, calculate the number of three-player games that took at least
50 minutes. We can calculate this number by multiplying the area of that
particular histogram bar (from 50 to 60) by the total number of three
player games(1000 games) total. This results in (60-50) \cdot 0.014 \cdot 1000 = 140. We
repeat the same process to find the number of two-player games that took
at least 50 minutes, which is (60-50) \cdot
0.006 \cdot 1000 = 60. Then, we find the difference of these
numbers, which is 140 - 60 = 80.
An easier way to calculate this is to measure the difference
directly. We could do this by finding the area of the highlighted region
below and then multiplying by the number of games. This represents the
difference between the number of three-player games and the number of
two player games. This, way we need to do just one calculation to get
the same answer: (60 - 50) \cdot (0.014 -
0.006) \cdot 1000 = 80.",61.0,Medium
385,Wi,24,Midterm,,Problem 8,Problem 8.2,"Calculate the approximate area of overlap of the two histograms. Give
your answer as a proportion between 0 and 1, rounded to two
decimal places.","0.74
To find the area of overlap of the two histograms, we can directly
calculate the area of overlap in each bin and add them up as shown
below. However, this requires a lot of calculation, and is not
advised.

From 10-20: (20-10) \cdot 0.006 =
0.06
From 20-30: (30-20) \cdot 0.018 =
0.18
From 30-40: (40-30) \cdot 0.028 =
0.28
From 40-50: (50-40) \cdot 0.016 =
0.16
From 50-60: (60-50) \cdot 0.006 =
0.06

The summation of the overlap here is 0.74!
A much more efficient way to do this problem is to find the area of
overlap by taking the total area of one distribution (which is 1) and
subtracting the area in that distribution that does not overlap with the
other. In the picture below, the only area in the two-player
distribution that does not overlap with three-player distribution is
highlighted. Notice there are only two regions to find the area of, so
this is much easier. The calculation comes out the same: 1 - ((20 - 10) \cdot (0.022-0.006) + (30 - 20) \cdot
(0.028 - 0.018) = 0.74.",56.0,Medium
386,Sp,22,Midterm,,Problem 1,Problem 1,"Which of the following is a valid reason not to set
the index of sungod to 'Artist'?
Select all correct answers.

 Two different artists have the same name.
 An artist performed at Sun God in more than one year.
 Several different artists performed at Sun God in the same year.
 Many different artists share the same value of
'Appearance_Order'.
 None of the above.","Two different artists have the same name.,
An artist performed at Sun God in more than one year.
For this question, it is crucial to know that an index should not
contain duplicate values, so we need to consider reasons why
'Artist' might contain two values that are the same. Let’s
go through the answer choices in order.
For the first option, if two different artists had the same name,
this would lead to duplicate values in the 'Artist' column.
Therefore, this is a valid reson not to index sungod by
'Artist'.
For the second option, if one artist performed at Sun God in more
than one year, their name would appear multiple times in the
'Artist' column, once for each year they performed. This
would also be a valid reason not to index sungod by
'Artist'.
For the third option, if several different artists performed at Sun
God in the same year, that would not necessarily create duplicates in
the 'Artist' column, unless of course two of the artists
had the same name, which we’ve already addressed in the first answer
choice. This is not a valid reason to avoid indexing sungod
by 'Artist'.
For the last answer choice, if many different artists share the same
value of 'Appearance_Order', this would not create
duplicates in the 'Artist' column. Therefore, this is also
not a valid reason to avoid indexing sungod by
'Artist'.",83.0,Easy
387,Sp,22,Midterm,,Problem 2,Problem 2,"On the graph paper below, draw the histogram that would be produced
by this code.
(
sungod.take(np.arange(5))
      .plot(kind='hist', density=True, 
      bins=np.arange(0, 7, 2), y='Appearance_Order');
)
In your drawing, make sure to label the height of each bar in the
histogram on the vertical axis. You can scale the axes however you like,
and the two axes don’t need to be on the same scale.","To draw the histogram, we first need to bin the data and figure out
how many data values fall into each bin. The code includes
bins=np.arange(0, 7, 2) which means the bin endpoints are
0, 2, 4, 6. This gives us three bins:
[0, 2), [2,
4), and [4, 6]. Remember that
all bins, except for the last one, include the left endpoint but not the
right. The last bin includes both endpoints.
Now that we know what the bins are, we can count up the number of
values in each bin. We need to look at the
'Appearance_Order' column of
sungod.take(np.arange(5)), or the first five rows of
sungod. The values there are 1,
4, 3, 1, 3. The two 1s fall into
the first bin [0, 2). The two 3s fall into the second bin [2, 4), and the one 4 falls into the last bin [4, 6]. This means the proportion of values
in each bin are \frac{2}{5}, \frac{2}{5},
\frac{1}{5} from left to right.
To figure out the height of each bar in the histogram, we use the
fact that the area of a bar in a density histogram should equal the
proportion of values in that bin. The area of a rectangle is height
times width, so height is area divided by width.
For the bin [0, 2), the area is
\frac{2}{5} = 0.4 and the width is
2, so the height is \frac{0.4}{2} = 0.2.
For the bin [2, 4), the area is
\frac{2}{5} = 0.4 and the width is
2, so the height is \frac{0.4}{2} = 0.2.
For the bin [4, 6], the area is
\frac{1}{5} = 0.2 and the width is
2, so the height is \frac{0.2}{2} = 0.1.
Since the bins are all the same width, the fact that there an equal
number of values in the first two bins and half as many in the third bin
means the first two bars should be equally tall and the third should be
half as tall. We can use this to draw the rest of the histogram quickly
once we’ve drawn the first bar.",45.0,Hard
388,Sp,22,Midterm,,Problem 3,Problem 3,"Suppose in a new cell, we type the following.
    sungod.sort_values(by='Year')
After we run that cell, we type the following in a second cell.
    sungod.get('Artist').iloc[0]
What is the output when we run the second cell? Note that the first
Sun God festival was held in 1983.

 'Blues Traveler'
 The artist who appeared on stage first in 1983.
 An artist who appeared in 1983, but not necessarily the one who
appeared first.
 Not enough information to tell.","'Blues Traveler'
In the first cell, although we seem to be sorting sungod
by 'Year', we aren’t actually changing the DataFrame
sungod at all because we don’t save the sorted DataFrame.
Remember that DataFrame methods don’t actually change the underlying
DataFrame unless you explicitly make that happen by saving the output as
the name of the DataFrame. So the first 'Artist' name will
still be 'Blues Traveler'.
Suppose we had saved the sorted DataFrame as in the code below.

    sungod = sungod.sort_values(by='Year')   
    sungod.get('Artist').iloc[0]

In this case, the output would be the name of an artist who appeared
in 1983, but not necessarily the one who appeared first. There will be
several artists associated with the year 1983, and we don’t know which
of them will be first in the sorted DataFrame.",12.0,Hard
389,Sp,22,Midterm,,Problem 4,Problem 4,"Write one line of code below to create a DataFrame called
openers containing the artists that appeared first on stage
at a past Sun God festival. The DataFrame openers should
have all the same columns as sungod.","openers = sungod[sungod.get('Appearance_Order')==1]
Since we want only certain rows of sungod, we need to
query. The condition to satisfy is that the
'Appearance_Order' column should have a value of 1 to
indicate that this artist performed first in a certain year’s
festival.",84.0,Easy
390,Sp,22,Midterm,,Problem 5,Problem 5,"What was the largest number of artists that ever performed in a
single Sun God festival? Select all expressions that evaluate to
the correct answer.

 sungod.groupby('Appearance_Order').count().get('Year').max()
 sungod.groupby('Year').count().get('Artist').max()
 sungod.get('Appearance_Order').max()
 sungod.groupby('Year').max().get('Year').max()
 None of the above.","sungod.groupby('Year').count().get('Artist').max(),
sungod.get('Appearance_Order').max()
Let’s go through all the answer choices.
For the first option,
sungod.groupby('Appearance_Order').count() will create a
DataFrame with one row for each unique value of
'Appearance_Order', and each column will contain the same
value, which represents the number of Sun God festivals that had at
least a certain amount of performers. For example, the first row of
sungod.groupby('Appearance_Order').count() will correspond
to an 'Appearance_Order' of 1, and each column will contain
a count of the number of Sun God festivals with at least one performer.
Since every festival has at least one performer, the largest count in
any column, including 'Year' will be in this first row. So
sungod.groupby('Appearance_Order').count().get('Year').max()
represents the total number of Sun God festivals, which is not the
quantity we’re trying to find.
For the second option, sungod.groupby('Year').count()
will create a DataFrame with one row per year, with each column
containing a count of the number of artists that performed in that
year’s festival. If we take the largest such count in any one column, we
are finding the largest number of artists that ever performed in a
single Sun God festival. Therefore,
sungod.groupby('Year').count().get('Artist').max() is
correct.
The third option works because we can find the desired quantity by
simply looking for the largest value in the
'Appearance_Order' column. For example, if the largest
number of artists to ever perform in a Sun God festival was, say, 17,
then for that year’s festival, the last artist to appear would have a
value of 17 in the 'Appearance_Order' column. There can be
no 18 anywhere in the 'Appearance_Order' column, otherwise
that would mean there was some festival with 18 performers. Therefore,
sungod.get('Appearance_Order').max() is correct.
The fourth option is not even correct Python code. The DataFrame
produced by sungod.groupby('Year').max() is indexed by
'Year' and no longer has 'Year' as a column.
So we’d get an error if we tried to access this nonexistent column, as
in sungod.groupby('Year').max().get('Year').",78.0,Easy
391,Sp,22,Midterm,,Problem 6,Problem 6,"Fill in the blank in the code below so that
chronological is a DataFrame with the same rows as
sungod, but ordered chronologically by appearance on stage.
That is, earlier years should come before later years, and within a
single year, artists should appear in the DataFrame in the order they
appeared on stage at Sun God. Note that groupby
automatically sorts the index in ascending order.
chronological = sungod.groupby(___________).max().reset_index()

 ['Year', 'Artist', 'Appearance_Order']
 ['Year', 'Appearance_Order']
 ['Appearance_Order', 'Year']
 None of the above.","['Year', 'Appearance_Order']
The fact that groupby automatically sorts the index in
ascending order is important here. Since we want earlier years before
later years, we could group by 'Year', however if we
just group by year, all the artists who performed in a given
year will be aggregated together, which is not what we want. Within each
year, we want to organize the artists in ascending order of
'Appearance_Order'. In other words, we need to group by
'Year' with 'Appearance_Order' as subgroups.
Therefore, the correct way to reorder the rows of sungod as
desired is
sungod.groupby(['Year', 'Appearance_Order']).max().reset_index().
Note that we need to reset the index so that the resulting DataFrame has
'Year' and 'Appearance_Order' as columns, like
in sungod.",85.0,Easy
392,Sp,22,Midterm,,Problem 7,Problem 7,"Another DataFrame called music contains a row for every
music artist that has ever released a song. The columns are:

'Name' (str): the name of the music
artist
'Genre' (str): the primary genre of the
artist
'Top_Hit' (str): the most popular song by
that artist, based on sales, radio play, and streaming
'Top_Hit_Year' (int): the year in which
the top hit song was released

You want to know how many musical genres have been represented at Sun
God since its inception in 1983. Which of the following expressions
produces a DataFrame called merged that could help
determine the answer?

 merged = sungod.merge(music, left_on='Year', right_on='Top_Hit_Year')
 merged = music.merge(sungod, left_on='Year', right_on='Top_Hit_Year')
 merged = sungod.merge(music, left_on='Artist', right_on='Name')
 merged = music.merge(sungod, left_on='Artist', right_on='Name')","merged = sungod.merge(music, left_on='Artist', right_on='Name')
The question we want to answer is about Sun God music artists’
genres. In order to answer, we’ll need a DataFrame consisting of rows of
artists that have performed at Sun God since its inception in 1983. If
we merge the sungod DataFrame with the music
DataFrame based on the artist’s name, we’ll end up with a DataFrame
containing one row for each artist that has ever performed at Sun God.
Since the column containing artists’ names is called
'Artist' in sungod and 'Name' in
music, the correct syntax for this merge is
merged = sungod.merge(music, left_on='Artist', right_on='Name').
Note that we could also interchange the left DataFrame with the right
DataFrame, as swapping the roles of the two DataFrames in a merge only
changes the ordering of rows and columns in the output, not the data
itself. This can be written in code as
merged = music.merge(sungod, left_on='Name', right_on='Artist'),
but this is not one of the answer choices.",86.0,Easy
393,Sp,22,Midterm,,Problem 8,Problem 8,"Consider an artist that has only appeared once at Sun God. At the
time of their Sun God performance, we’ll call the artist

outdated if their top hit came out more than five
years prior,
trending if their top hit came out within the five
years prior, and
up-and-coming if their top hit came out after they
appeared at Sun God.

Complete the function below so it outputs the appropriate description
for any input artist who has appeared exactly once at Sun God.
def classify_artist(artist):
    filtered = merged[merged.get('Artist') == artist]
    year = filtered.get('Year').iloc[0]
    top_hit_year = filtered.get('Top_Hit_Year').iloc[0]
    if ___(a)___ > 0:
        return 'up-and-coming'
    elif ___(b)___:
        return 'outdated'
    else:
        return 'trending'",,,
394,Sp,22,Midterm,,Problem 8,Problem 8.1,What goes in blank (a)?,"top_hit_year - year
Before we can answer this question, we need to understand what the
first three lines of the classify_artist function are
doing. The first line creates a DataFrame with only one row,
corresponding to the particular artist that’s passed in as input to the
function. We know there is just one row because we are told that the
artist being passed in as input has appeared exactly once at Sun God.
The next two lines create two variables:

year contains the year in which the artist performed at
Sun God, and
top_hit_year contains the year in which their top hit
song was released.

Now, we can fill in blank (a). Notice that the body of the
if clause is return 'up-and-coming'. Therefore
we need a condition that corresponds to up-and-coming, which we are told
means the top hit came out after the artist appeared at Sun God. Using
the variables that have been defined for us, this condition is
top_hit_year > year. However, the if
statement condition is already partially set up with > 0
included. We can simply rearrange our condition
top_hit_year > year by subtracting year
from both sides to obtain top_hit_year - year > 0, which
fits the desired format.",89.0,Easy
395,Sp,22,Midterm,,Problem 8,Problem 8.2,What goes in blank (b)?,"year-top_hit_year > 5
For this part, we need a condition that corresponds to an artist
being outdated which happens when their top hit came out more than five
years prior to their appearance at Sun God. There are several ways to
state this condition: year-top_hit_year > 5,
year > top_hit_year + 5, or any equivalent condition
would be considered correct.",89.0,Easy
396,Sp,22,Midterm,,Problem 9,Problem 9,"The expression below evaluates to True.
(
classify_artist('Michelle Branch')=='outdated' 
and 
classify_artist('Drake')=='trending'
)
Consider the scatterplot created by the code below.
merged.plot(kind='scatter', x='Year', y='Top_Hit_Year');
The point for Drake is somewhere in this scatterplot. Relative to the
point for Drake, there are four quadrants of the scatterplot, as shown
below.


There is one quadrant in which the point for Michelle Branch
cannot appear. Which is it?

 northeast
 northwest
 southwest
 southeast","northwest
The scatterplot shows an artist’s year of performance at Sun God on
the x-axis, and their top hit year on the y-axis.
Since Drake is considered trending, we know his top hit came out
within the five years prior to his Sun God performance. In other words,
the time gap between his top hit release and his Sun God performance is
at most five years.
If Michelle Branch were in the northwest quadrant, that would mean
her x-coordinate was smaller than Drake’s, and her y-coordinate was
larger. In other words, that would mean she performed at Sun God before
Drake and her top hit was released after Drake’s. This means the time
gap between her top hit release and her Sun God performance is less than
five years, so she could not be considered outdated.
We could also answer this question through process of elimination.
This would entail finding scenarios that show each of the other three
quadrants is possible.
Michelle Branch could wind up in the northeast quadrant if, say, she
performed 10 years after Drake and released her top hit 1 year after
Drake. She’d be considered outdated because the time gap between her top
hit and her Sun God performance would exceed five years.
Similarly, Michelle Branch could be in the southwest quadrant if,
say, she performed 1 year before Drake and released her top hit 10 years
before Drake. Again, the time gap between her top hit and her Sun God
performance would exceed five years, making her outdated.
She could also be in the southeast quadrant if, say, she performed 10
years after Drake and released her top hit 10 years before Drake. This
would also make the time gap between her top hit and Sun God performance
more than five years.
So since all three other quadrants are possible and we are told that
one quadrant is impossible, the northwest quadrant must be
impossible.",65.0,Medium
397,Sp,22,Midterm,,Problem 10,Problem 10,"Note: This problem is out of scope; it
covers material no longer included in the course.
In 2014, the UCSD administration made some important changes to Sun
God policies, including:

eliminating guest tickets for non-students,
increasing security, and
introducing on-site medical care.

These changes were implemented because of incidents related to drug
and alcohol abuse at the festival. At the 2013 Sun God festival, 48
students were hospitalized, and at the 2014 festival, only 8 students
were hospitalized. Assuming there was no change in the total number of
attendees from 2013 to 2014, which of the following statements is
correct?

 We cannot be sure if there is an association between administrative
changes and hospitalizations.
 There is an association between administrative changes and
hospitalizations. One or more of the administrative changes is
responsible for the decrease in hospitalizations, but since several
administrative changes happened at the same time, we can’t be sure of
which one to credit with the reduction in hospitalizations.
 There is an association between administrative changes and
hospitalizations. We can’t be sure if any of the administrative changes
are responsible for the reduction in hospitalizations.
 None of the above.","There is an association between
administrative changes and hospitalizations. We can’t be sure if any of
the administrative changes are responsible for the reduction in
hospitalizations.
We know there is an association between administrative changes and
hospitalizations because the number of hospitalized students dropped
after the changes went into effect.
However, since no randomized controlled trial was done, we can’t be
sure of the reason for the reduction in hospitalizations. For example,
maybe there were fewer hospitalizations because a new flavor of
sparkling water came out in 2014, and people drank that instead of
alcohol. We just don’t know enough to conclude any causal explanation
for the reduction in hospitalizations.",54.0,Medium
398,Sp,22,Midterm,,Problem 11,Problem 11,"The fine print of the Sun God festival website says “Ticket does not
guarantee entry. Venue subject to capacity restrictions.” RIMAC field,
where the 2022 festival will be held, has a capacity of 20,000 people.
Let’s say that UCSD distributes 21,000 tickets to Sun God 2022 because
prior data shows that 5% of tickets distributed are never actually
redeemed. Let’s suppose that each person with a ticket this year has a
5% chance of not attending (independently of all others). What is the
probability that at least one student who has a ticket cannot get in due
to the capacity restriction? Fill in the blanks in the code below so
that prob_angry_student evaluates to an approximation of
this probability.
num_angry = 0

for rep in np.arange(10000):
    # randomly choose 21000 elements from [True, False] such that 
    # True has probability 0.95, False has probability 0.05
    attending = np.random.choice([True, False], 21000, p=[0.95, 0.05])
    if __(a)__:
        __(b)__

prob_angry_student = __(c)__",,,
399,Sp,22,Midterm,,Problem 11,Problem 11.1,"What goes in the first blank?

 np.count_nonzero(attending) == 20001
 attending[20000] == False
 attending.sum() > 20000
 np.count_nonzero(attending) > num_angry","attending.sum() > 20000
Let’s look at the variable attending. Since we’re
choosing 21,000 elements from the list [True, False] and
there are 21,000 tickets distributed, this code is randomly determining
whether each ticket holder will actually attend the festival. There’s a
95% chance of each ticket holder attending, which is reflected in the
p=[0.95, 0.05] argument. Remember that
np.random.choice returns an array of random choices, which
in this case means it will contain 21,000 elements, each of which is
True or False.
We want to figure out the probability of at least one ticket holder
showing up and not being admitted. Another way to say this is we want to
find the probability that more than 20,000 ticket holders show up to
attend the festival. The way we approximate a probability through
simulation is we repeat a process many times and see how often some
event occured. The event we’re interested in this case is that more than
20,000 ticket holders came to Sun God. Since we have an array of
True and False values corresponding to whether
each ticket holder actually came, we just need to determine if there are
more than 20,000 True values in the attending
array.
There are several ways to count the number of True
values in a Boolean array. One way is to sum the array since in Python
True counts as 1 and False counts as 0.
Therefore, attending.sum() > 20000 is the condition we
need to check here.",67.0,Medium
400,Sp,22,Midterm,,Problem 11,Problem 11.2,What goes in the second blank?,"num_angry = num_angry + 1
Remember our goal in simulation is to repeat a process many times to
see how often some event occurs. The repetition comes from the
for loop which runs 10,000 times. Each time, we are
simulating the process of 21,000 students each randomly deciding whether
to show up to Sun God or not. We want to know, out of these 10,000
trials, how frequently more than 20,000 of the students will show up. So
when this happens, we want to record that it happened. The standard way
to do that is to keep a counter variable that starts at 0 and gets
incremented, or increased by one, each time we had more than 20,000
attendees in our simulation.
The framework to do this is already set up because a variable called
num_angry is initialized to 0 before the for
loop. This variable is our counter variable, meant to count the number
of trials, out of 10,000, that resulted in at least one student being
angry because they showed up to Sun God with a ticket and were denied
entrance. So all we need to do when there are more than 20,000
True values in the attending array is
increment this counter by one via the code
num_angry = num_angry + 1, sometimes abbreviated as
num_angry += 1.",59.0,Medium
401,Sp,22,Midterm,,Problem 11,Problem 11.3,What goes in the third blank?,"num_angry/10000
To calculate the approximate probability, all we need to do is divide
the number of trials in which a student was angry by the total number of
trials, which is 10,000.",68.0,Medium
402,Sp,22,Midterm,,Problem 12,Problem 12,,,,
403,Sp,22,Midterm,,Problem 12,Problem 12.1,"You’re definitely going to Sun God 2022, but you don’t want to go
alone! Fortunately, you have n friends
who promise to go with you. Unfortunately, your friends are somewhat
flaky, and each has a probability p of
actually going (independent of all others). What is the probability that
you wind up going alone? Give your answer in terms of p and n.","(1-p)^n
If you go alone, it means all of your friends failed to come. We can
think of this as an and condition in order to use
multiplication. The condition is: your first friend doesn’t come
and your second friend doesn’t come, and so on. The probability
of any individual friend not coming is 1-p, so the probability of all your friends
not coming is (1-p)^n.",76.0,Easy
404,Sp,22,Midterm,,Problem 12,Problem 12.2,"In past Sun God festivals, sometimes artists that were part of the
lineup have failed to show up! Let’s say there are n artists scheduled for Sun God 2022, and
each artist has a probability p of
showing up (independent of all others). What is the probability that the
number of artists that show up is less than n, meaning somebody no-shows? Give your
answer in terms of p and n.","1-p^n
It’s actually easier to figure out the opposite event. The opposite
of somebody no-showing is everybody shows up. This is easier to
calculate because we can think of it as an and condition: the
first artist shows up and the second artist shows up, and so
on. That means we just multiply probabilities. Therefore, the
probability of all artists showing up is p^n and the probability of some artist not
showing up is 1-p^n.",73.0,Medium
405,Sp,23,Midterm,,Problem 1,Problem 1,,,,
406,Sp,23,Midterm,,Problem 1,Problem 1.1,"What is the type of the following expression?
(survey
 .sort_values(by=""Class Standing"")
)

 int or float
 list or array
 Boolean
 Series
 string
 DataFrame","DataFrame
The method .sort_values(by=""Class Standing"") sorts the
survey DataFrame by the ""Class Standing""
column and returns a new DataFrame (without modifying the original one).
The resulting DataFrame will be sorted by class standing in ascending
order unless specified otherwise by providing the
ascending=False argument.",94.0,Easy
407,Sp,23,Midterm,,Problem 1,Problem 1.2,"What is the type of the following expression?
(survey
 .sort_values(by=""Class Standing"")
 .groupby(""College"").count()
)

 int or float
 list or array
 Boolean
 Series
 string
 DataFrame","DataFrame
The method .sort_values(by=""Class Standing"") sorts the
survey DataFrame based on the entries in the
""Class Standing"" column and produces a new DataFrame.
Following this, .groupby(""College"") organizes the sorted
DataFrame by grouping entries using the ""College"" column.
After that, the .count() aggregation method computes the
number of rows for each ""College"". The result is a
DataFrame whose index contains the unique values in
""College"" and whose columns all contain the same values –
the number of rows in
survey.sort_values(by=""Class Standing"") for each
""College"".
Note that if we didn’t sort before grouping – that is, if our
expression was just survey.groupby(""College"").count() – the
resulting DataFrame would be the same! That’s because the order of the
rows in survey does not impact the number of rows in
survey that belong to each ""College"".",93.0,Easy
408,Sp,23,Midterm,,Problem 1,Problem 1.3,"What is the type of the following expression?
(survey
 .sort_values(by=""Class Standing"")
 .groupby(""College"").count()
 .get(""IG Followers"")
)

 int or float
 list or array
 Boolean
 Series
 string
 DataFrame","Series
The above expression, before .get(""IG Followers""), is
the same as in the previous subpart. We know that

survey.sort_values(by=""Class Standing"").groupby(""College"").count()

is a DataFrame indexed by ""College"" with multiple
columns, each of which contain the number of rows in
survey.sort_values(by=""Class Standing"") for each
""College"". Then, using .get(""IG Followers"")
extracts just the ""IG Followers"" column from the
aforementioned counts DataFrame, and we know that columns are stored as
Series. We know that this column exists, because
survey.sort_values(by=""Class Standing"").groupby(""College"").count()
will have the same columns as survey, minus
""College"", which was moved to the index.",92.0,Easy
409,Sp,23,Midterm,,Problem 1,Problem 1.4,"What is the type of the following expression?
(survey
 .sort_values(by=""Class Standing"")
 .groupby(""College"").count()
 .get(""IG Followers"")
 .iloc[0]
)

 int or float
 list or array
 Boolean
 Series
 string
 DataFrame","int or float
In the previous subpart, we saw that

survey.sort_values(by=""Class Standing"").groupby(""College"").count().get(""IG Followers"")

is a Series whose index contains the unique names of
""College""s and whose values are the number of rows in
survey.sort_values(by=""Class Standing"") for each college.
The number of rows in
survey.sort_values(by=""Class Standing"") for any particular
college – say, ""Sixth"" – is a number, and so the values in
this Series are all numbers. As such, the answer is int or float.",91.0,Easy
410,Sp,23,Midterm,,Problem 1,Problem 1.5,"What value does the following expression evaluate
to? If you believe the expression errors, provide a one sentence
explanation of why.
For example, if you think the expression evaluates to an int, don’t
write “int"" or”the largest value in the ""IG Followers""
column"", write the specific int, like “23"".
Hint: You have enough information to answer the problem; don’t
forget to look at the data description page.
(survey
.sort_values(by=""Class Standing"")
.groupby(""College"").count()
.get(""IG Followers"")
.index[0]
)","""ERC""
From the previous subpart, we saw that

survey
.sort_values(by=""Class Standing"")
.groupby(""College"").count()
.get(""IG Followers"")

is a Series, indexed by ""College"", whose values contain
the number of rows in
survey.sort_values(by=""Class Standing"") for each
""College"". When we group by ""College"", the
resulting DataFrame (and hence, our Series) is sorted in ascending order
by ""College"". This means that the index of our Series is
sorted alphabetically by ""College"" names. Of the
""College"" names mentioned in the data description
(""ERC"", ""Marshall"", ""Muir"",
""Revelle"", ""Seventh"", ""Sixth"",
and ""Warren""), the first name alphabetically is
""ERC"", so using .index[0] on the above index
gives us ""ERC"".",32.0,Hard
411,Sp,23,Midterm,,Problem 1,Problem 1.6,"Consider again the expression from 1.5. Suppose we remove the
piece
.sort_values(by=""Class Standing"")
but keep all other parts the same. Does this change the value the
expression evaluates to?

 Yes, this changes the value the expression evaluates to.
 No, this does not change the value the expression evaluates to.","No, this does not change the value the
expression evaluates to.
We addressed this in the second subpart of the problem: “Note
that if we didn’t sort before grouping – that is, if our expression was
just survey.groupby(""College"").count() – the resulting
DataFrame would be the same! That’s because the order of the rows in
survey does not impact the number of rows in
survey that belong to each
""College"".”",80.0,Easy
412,Sp,23,Midterm,,Problem 1,Problem 1.7,"What value does the following expression evaluate
to? If you believe the expression errors, provide a one sentence
explanation of why.
For example, if you think the expression evaluates to an int, don’t
write “int” or “the largest value in the ""IG Followers""
column”, write the specific int, like “23”.
Hint: You have enough information to answer the problem; don’t
forget to look at the data description page.
(survey
 .sort_values(by=""Class Standing"")
 .groupby(""College"").count()
 .get(""IG Followers"")
 .loc[0]
)","The code produces an error because, after
the grouping operation, the resulting Series uses the unique college
names as indices, and there isn’t a college named 0.
In the previous few subparts, we’ve established that

survey
.sort_values(by=""Class Standing"")
.groupby(""College"").count()
.get(""IG Followers"")

is a Series, indexed by ""College"", whose values contain
the number of rows in
survey.sort_values(by=""Class Standing"") for each
""College"".
The .loc accessor is used to extract a value from a
Series given its label, or index value. However, since the index values
in the aforementioned Series are ""College"" names, there is
no value whose index is 0, so this throws an error.",54.0,Medium
413,Sp,23,Midterm,,Problem 2,Problem 2,"Fill in the blanks below so that the expression evaluates to the most
unread emails of any student that has 0 Instagram followers.
survey[__(a)__].__(b)__",,,
414,Sp,23,Midterm,,Problem 2,Problem 2.1,What goes in blank (a)?,"survey.get(""IG Followers"") == 0
survey.get(""IG Followers"") is a Series containing the
values in the ""IG Followers"" column.
survey.get(""IG Followers"") == 0, then, is a Series of
Trues and Falses, containing True
for rows where the number of ""IG Followers"" is 0 and
False for all other rows.
Put together, survey[survey.get(""IG Followers"") == 0]
evaluates to a new DataFrame, which only has the rows in
survey where the student’s number of
""IG Followers"" is 0.",89.0,Easy
415,Sp,23,Midterm,,Problem 2,Problem 2.2,"What goes in blank (b)? Select all valid answers.

 get(""Unread Emails"").apply(max)
 get(""Unread Emails"").max()
 sort_values(""Unread Emails"", ascending=False).get(""Unread Emails"").iloc[0]
 sort_values(""Unread Emails"", ascending=False).get(""Unread Emails"").iloc[-1]
 None of the above","Options 2 and 3
Let’s look at each option.

Option 1:
get(""Unread Emails"").apply(max) This option is invalid
because .apply(max) will produce a new Series containing
the maximums of each individual element in ""Unread Emails""
rather than the maximum of the entire column. Since the values in the
""Unread Emails"" columns are numbers, this will try to call
max on a number individually (many times), and that errors,
since e.g. max(2) is invalid.
Option 2: get(""Unread Emails"").max()
This option is valid because it directly fetches the
""Unread Emails"" column and uses the max method
to retrieve the highest value within it, aligning with the goal of
identifying the most unread emails.
Option 3:
sort_values(""Unread Emails"", ascending=False).get(""Unread Emails"").iloc[0]
This option is valid. The
sort_values(""Unread Emails"", ascending=False) method call
sorts the ""Unread Emails"" column in descending order.
Following this, .get(""Unread Emails"") retrieves the sorted
column, and .iloc[0] selects the first element, which,
given the sorting, represents the maximum number of unread emails.
Option 4:
sort_values(""Unread Emails"", ascending=False).get(""Unread Emails"").iloc[-1]
This option is invalid because, although it sorts the
""Unread Emails"" column in descending order, it selects the
last element with .iloc[-1], which represents the smallest
number of unread emails due to the descending sort order.",89.0,Easy
416,Sp,23,Midterm,,Problem 2,Problem 2.3,What goes in blank (c)?,,54.0,Medium
417,Sp,23,Midterm,,Problem 2,Problem 2.4,What goes in blank (d)?,"iloc[0], iloc[-1],
sum(), max(), mean(), or any
other method that when called on a Series with a single value outputs
that same value
To create temp, we grouped on both
""IG Followers"" and ""College"" and used the
max aggregation method. This means that temp
has exactly one row where ""College"" is
""Revelle"" and ""IG Followers"" is 0, since when
we group on both ""IG Followers"" and ""College"",
the resulting DataFrame has exactly one row for every unique combination
of ""IG Followers"" and ""College"" name. We’ve
already identified that before blank (d) is

temp[(temp.get(""College"") == ""Revelle"") & (temp.get(""IG Followers"") == 0)].get(""Unread Emails"")

Since
temp[(temp.get(""College"") == ""Revelle"") & (temp.get(""IG Followers"") == 0)]
is a DataFrame with just a single row, the entire expression is a Series
with just a single value, being the maximum value we ever saw in the
""Unread Emails"" column when ""College"" was
""Revelle"" and ""IG Followers"" was 0. To extract
the one element out of a Series that only has one element, we can use
many aggregation methods (max, min,
mean, median) or use .iloc[0] or
.iloc[-1].",79.0,Easy
418,Sp,23,Midterm,,Problem 2,Problem 2.5,"The student in Revelle with the most unread emails, among those with
0 Instagram followers, is a ""HI25"" (History) major. Is the
value ""HI25"" guaranteed to appear at least once within
temp.get(""Major"")?

 Yes
 No","No
When the survey DataFrame is grouped by both
""IG Followers"" and ""College"" using the
.max aggregation method, it retrieves the maximum value for
each column based on alphabetical order. Even if a student in
""Revelle"" with 0 Instagram followers has the most unread
emails and is a ""HI25"" major, the ""HI25"" value
would only be the maximum for the ""Major"" column if no
other major code in the same group alphabetically surpasses it.
Therefore, if there’s another major code like ""HI26"" or
""MA30"" in that group, it would be chosen as the maximum
instead of ""HI25"".",59.0,Medium
419,Sp,23,Midterm,,Problem 3,Problem 3,"Currently, the ""Section"" column of survey
contains the values ""A"" and ""B"". However, it’s
more natural to think of the lecture sections in terms of their times,
i.e. ""12PM"" and ""1PM"".
Below, we’ve defined four functions, named converter_1,
converter_2, converter_3, and
converter_4. All four functions return ""12PM""
when ""A"" is the argument passed in and return
""1PM"" when ""B"" is the argument passed in.
Where they potentially differ is in how they behave when called on
arguments other than ""A"" and ""B"".
Note: In the answer choices below, by None we mean that
the function call doesn’t return anything, but doesn’t error either.",,,
420,Sp,23,Midterm,,Problem 3,Problem 3.1,"Consider the function converter_1, defined below.
    def converter_1(section):
        if section == ""A"":
            return ""12PM""
        else:
            return ""1PM""
When convert_1 is called on an argument other than
""A"" or ""B"", what does it return?

 ""12PM""
 ""1PM""
 None
 It errors


Consider the function converter_2, defined below.
    def converter_2(section):
        time_dict = {
            ""A"": ""12PM"",
            ""B"": ""1PM""
        }
        return times_dict[section]
When convert_2 is called on an argument other than
""A"" or ""B"", what does it return?

 ""12PM""
 ""1PM""
 None
 It errors


Consider the function converter_3, defined below.
    def converter_3(section):
        if section == ""A"":
            return ""12PM""
        elif section == ""B"":
            return ""1PM""
        else:
            return 1 / 0
When converter_3 is called on an argument other than ""A""
or ""B"", what does it return?

 ""12PM""
 ""1PM""
 None
 It errors


Consider the function converter_4, defined below.
    def converter_4(section):
        sections = [""A"", ""B""]
        times = [""12PM"", ""1PM""]
        for i in np.arange(len(sections)):
            if section == sections[i]:
                return times[i]
When converter_4 is called on an argument other than
""A"" or ""B"", what does it return?

 ""12PM""
 ""1PM""
 None
 It errors","converter_1:
""1PM"", converter_2: It errors,
converter_3: It errors, converter_4:
None.
converter_1: ""1PM"". The function
converter_1 checks if the input section is
equal to ""A"". If it is, the function returns
""12PM"". However, if the input is anything other than
""A"" (including values other than ""B""), the
function defaults to the else condition and returns
""1PM"". So, even for arguments other than ""A""
or ""B"", converter_1 will return
""1PM"".",90.0,Easy
421,Sp,23,Midterm,,Problem 3,Problem 3.2,"Suppose we decide to use converter_4 to convert the
values in the ""Section"" column of survey to
times. To do so, we first use the .apply method, like
so:
    section_times = survey.get(""Section"").apply(converter_4)
What is the type of section_times?

 int or float
 Boolean
 string
 list or array
 Series
 DataFrame


We’d then like to replace the ""Section"" column in
survey with the results in section_times. To
do so, we run the following line of code, after running the assignment
statement above:
    survey.assign(Section=""section_times"")
What is the result of running the above line of code?

 The ""Section"" column in survey now contains
only the values ""12PM"" and ""1PM"".
 The line evaluates to a new DataFrame whose ""Section""
column only contains the values ""12PM"" and
""1PM"", but the ""Section"" column in
survey still contains the values ""A"" and
""B"".
 An error is raised, because we can’t use .assign with a
column name that already exists.
 An error is raised, because the value after the = needs
to be a list, array, or Series, not a string.
 An error is raised, because the value before the = needs
to be a string, not a variable name.","What is the type of section_times?
Series. When the .apply() method is used on a
Series in a DataFrame, it applies the given function to each element in
the Series and returns a new Series with the transformed values. Thus,
the type of section_times is a Series.",76.0,Easy
422,Sp,23,Midterm,,Problem 4,Problem 4,,,,
423,Sp,23,Midterm,,Problem 4,Problem 4.1,"Consider the following block of code.
    A = survey.shape[0]
    B = survey.groupby([""Unread Emails"", ""IG Followers""]).count().shape[0]
Suppose the expression A == B evaluates to
True. Given this fact, what can we conclude?

 There are no two students in the class with the same number of unread
emails.
 There are no two students in the class with the same number of
Instagram followers.
 There are no two students in the class with the same number of
Instagram followers, and there are no two students in the class with the
same number of unread emails.
 There are no two students in the class with both the same number of
unread emails and the same number of Instagram followers.","There are no two students in the class with
both the same number of unread emails and the same number of Instagram
followers.
The DataFrame
survey.groupby([""Unread Emails"", ""IG Followers""]).count()
will have one row for every unique combination of
""Unread Emails"" and ""IG Followers"". If two
students had the same number of ""Unread Emails"" and
""IG Followers"", they would be grouped together, resulting
in fewer groups than the total number of students. But since
A == B, it indicates that there are as many unique
combinations of these two columns as there are rows in the
survey DataFrame. Thus, no two students share the same
combination of ""Unread Emails"" and
""IG Followers"".
Note that if student X has 2 ""Unread Emails"" and 5
""IG Followers"", student Y has 2
""Unread Emails"" and 3 ""IG Followers"", and
student Z has 3 ""Unread Emails"" and 5
""IG Followers"", they all have different combinations of
""Unread Emails"" and ""IG Followers"", meaning
that they’d all be represented by different rows in
survey.groupby([""Unread Emails"", ""IG Followers""]).count().
This is despite the fact that some of their numbers of
""Unread Emails"" and ""IG Followers"" are not
unique.",72.0,Medium
424,Sp,23,Midterm,,Problem 4,Problem 4.2,"We’d like to find the mean number of Instagram followers of all
students in DSC 10. One correct way to do so is given
below.
    mean_1 = survey.get(""IG Followers"").mean()
Another two possible ways to do this are given
below.
    # Possible method 1.
    mean_2 = survey.groupby(""Section"").mean().get(""IG Followers"").mean()

    # Possible method 2.
    X = survey.groupby(""Section"").sum().get(""IG Followers"").sum()
    Y = survey.groupby(""Section"").count().get(""IG Followers"").sum()
    mean_3 = X / Y
Is mean_2 equal to mean_1?

 Yes.
 Yes, if both sections have the same number of students, otherwise
maybe.
 Yes, if both sections have the same number of students, otherwise
no.
 No.

Is mean_3 equal to mean_1?

 Yes.
 Yes, if both sections have the same number of students, otherwise
maybe.
 Yes, if both sections have the same number of students, otherwise
no.
 No.","Is mean_2 is equal to mean_1? Yes,
if both sections have the same number of students, otherwise
maybe.
mean_2 is the “mean of means” – it finds the mean number
of ""IG Followers"" for each section, then finds the mean of
those two numbers. This is not in general to the overall mean, because
it doesn’t consider the fact that Section A may have more students than
Section B (or vice versa); if this is the case, then Section A needs to
be weighted more heavily in the calculation of the overall mean.
Let’s look at a few examples to illustrate our point. - Suppose
Section A has 2 students who both have 10 followers, and Section B has 1
student who has 5 followers. The overall mean number of followers is
\frac{10 + 10 + 5}{3} = 8.33.., while
the mean of the means is \frac{10 + 5}{2} =
7.5. These are not the same number, so mean_2 is not
always equal to mean_1. We can rule out “Yes” as an option.
- Suppose Section A has 2 students where one has 5 followers and one has
7, and Section B has 2 students where one has 3 followers and one has 15
followers. Then, the overall mean is \frac{5 +
7 + 3 + 13}{4} = \frac{28}{4} = 7, while the mean of the means is
\frac{\frac{5+7}{2} + \frac{3 + 13}{2}}{2} =
\frac{6 + 8}{2} = 7. If you experiment (or even write out a full
proof), you’ll note that as long as Sections A and B have the same
number of students, the overall mean number of followers across their
two sections is equal to the mean of their section-specific mean numbers
of followers. We can rule out “No” as an option. - Suppose Section A has
2 students who both have 10 followers, and Section B has 3 students who
both have 10 followers. Then, the overall mean is 10, and so is the mean
of means. So, it’s possible for there to be a different number of
students in Sections A and B and for mean_2 to be equal to
mean_1. It’s not always, true, though, which is why the
answer is “Yes, if both sections have the same number of students,
otherwise maybe” and we can rule out the “otherwise no” case.",15.0,Hard
425,Sp,23,Midterm,,Problem 5,Problem 5,"Teresa and Sophia are bored while waiting in line at Bistro and
decide to start flipping a UCSD-themed coin, with a picture of King
Triton’s face as the heads side and a picture of his mermaid-like tail
as the tails side.


Teresa flips the coin 21 times and sees 13 heads and 8 tails. She
stores this information in a DataFrame named teresa that
has 21 rows and 2 columns, such that:

The ""flips"" column contains ""Heads"" 13
times and ""Tails"" 8 times.
The ""Wolftown"" column contains ""Teresa""
21 times.

Then, Sophia flips the coin 11 times and sees 4 heads and 7 tails.
She stores this information in a DataFrame named sophia
that has 11 rows and 2 columns, such that:

The ""flips"" column contains ""Heads"" 4
times and ""Tails"" 7 times.
The ""Makai"" column contains ""Sophia"" 11
times.",,,
426,Sp,23,Midterm,,Problem 5,Problem 5.1,"How many rows are in the following DataFrame? Give your answer as an
integer.
    teresa.merge(sophia, on=""flips"")
Hint: The answer is less than 200.","108
Since we used the argument on=""flips, rows from
teresa and sophia will be combined whenever
they have matching values in their ""flips"" columns.
For the teresa DataFrame:

There are 13 rows with ""Heads"" in the
""flips"" column.
There are 8 rows with ""Tails"" in the
""flips"" column.

For the sophia DataFrame:

There are 4 rows with ""Heads"" in the
""flips"" column.
There are 7 rows with ""Tails"" in the
""flips"" column.

The merged DataFrame will also only have the values
""Heads"" and ""Tails"" in its
""flips"" column. - The 13 ""Heads"" rows from
teresa will each pair with the 4 ""Heads"" rows
from sophia. This results in 13
\cdot 4 = 52 rows with ""Heads"" - The 8
""Tails"" rows from teresa will each pair with
the 7 ""Tails"" rows from sophia. This results
in 8 \cdot 7 = 56 rows with
""Tails"".
Then, the total number of rows in the merged DataFrame is 52 + 56 = 108.",54.0,Medium
427,Sp,23,Midterm,,Problem 5,Problem 5.2,"Let A be your answer to the previous
part. Now, suppose that:

teresa contains an additional row, whose
""flips"" value is ""Total"" and whose
""Wolftown"" value is 21.
sophia contains an additional row, whose
""flips"" value is ""Total"" and whose
""Makai"" value is 11.

Suppose we again merge teresa and sophia on
the ""flips"" column. In terms of A, how many rows are in the new merged
DataFrame?

 A
 A+1
 A+2
 A+4
 A+231","A+1
The additional row in each DataFrame has a unique
""flips"" value of ""Total"". When we merge on the
""flips"" column, this unique value will only create a single
new row in the merged DataFrame, as it pairs the ""Total""
from teresa with the ""Total"" from
sophia. The rest of the rows are the same as in the
previous merge, and as such, they will contribute the same number of
rows, A, to the merged DataFrame. Thus,
the total number of rows in the new merged DataFrame will be A (from the original matching rows) plus 1
(from the new ""Total"" rows), which sums up to A+1.",46.0,Hard
428,Sp,23,Midterm,,Problem 6,Problem 6,"The histogram below displays the distribution of the number of
Instagram followers each student has, measured in 100s. That is, if a
student is represented in the first bin, they have between 0 and 200
Instagram followers.


For this question only, assume that there are exactly 200 students in
DSC 10.",,,
429,Sp,23,Midterm,,Problem 6,Problem 6.1,"How many students in DSC 10 have between 200 and 800 Instagram
followers? Give your answer as an integer.","90
Remember, the key property of histograms is that the proportion of
values in a bin is equal to the area of the corresponding bar. To find
the number of values in the range 2-8 (the x-axis is measured in hundreds), we’ll need
to find the proportion of values in the range 2-8 and multiply that by
200, which is the total number of students in DSC 10. To find the
proportion of values in the range 2-8, we’ll need to find the areas of
the 2-4, 4-6, and 6-8 bars.
Area of the 2-4 bar: \text{width} \cdot
\text{height} = 2 \cdot 0.1 = 0.2
Area of the 4-6 bar: \text{width} \cdot
\text{height} = 2 \cdot 0.0625 = 0.125.
Area of the 6-8 bar: \text{width} \cdot
\text{height} = 2 \cdot 0.0625 = 0.125.
Then, the total proportion of values in the range 2-8 is 0.2 + 0.125 + 0.125 = 0.45, so the total
number of students with between 200 and 800 Instagram followers is 0.45 \cdot 200 = 90.",49.0,Hard
430,Sp,23,Midterm,,Problem 6,Problem 6.2,"Suppose the height of a bar in the above histogram is h. How many students are represented in the
corresponding bin, in terms of h?
Hint: Just as in the first subpart, you’ll need to use the
assumption from the start of the problem.

 20 \cdot h
 100 \cdot h
 200 \cdot h
 400 \cdot h
 800 \cdot h","400 \cdot
h
As we said at the start of the last solution, the key property of
histograms is that the proportion of values in a bin is equal to the
area of the corresponding bar. Then, the number of students represented
bar a bar is the total number of students in DSC 10 (200) multiplied by
the area of the bar.
Since all bars in this histogram have a width of 2, the area of a bar
in this histogram is \text{width} \cdot
\text{height} = 2 \cdot h. If there are 200 students in total,
then the number of students represented in a bar with height h is 200 \cdot 2
\cdot h = 400 \cdot h.
To verify our answer, we can check to see if it makes sense in the
context of the previous subpart. The 2-4 bin has a height of 0.1, and 400 \cdot
0.1 = 40. The total number of students in the range 2-8 was 90,
so it makes sense that 40 of them came from the 2-4 bar, since the 2-4
bar takes up about half of the area of the 2-8 range.",36.0,Hard
431,Sp,23,Midterm,,Problem 7,Problem 7,"The four most common majors among students in DSC 10 this quarter are
""MA30"" (Mathematics - Computer Science),
""EN30"" (Business Economics), ""EN25""
(Economics), and ""CG35"" (Cognitive Science with a
Specialization in Machine Learning and Neural Computation). We’ll call
these four majors “popular majors"".
There are 80 students in DSC 10 with a popular major. The
distribution of popular majors is given in the bar chart below.",,,
432,Sp,23,Midterm,,Problem 7,Problem 7.1,"Fill in the blank below so that the expression outputs the bar chart
above.
    (survey
     .groupby(""Major"").count()
     .sort_values(""College"")
     .take(____)
     .get(""Section"")
     .plot(kind=""barh"")
    )
What goes in the blank?","[-4, -3. -2, -1]
Let’s break down the code line by line:

.groupby(""Major"") will group the survey DataFrame by
the ""Major"" column.
.count() will count the number of students in each
major.
.sort_values(""College"") sort these counts in ascending
order using the ""College"" column (which now contains
counts).
.take([-4, -3. -2, -1]) will take the last 4 rows of
the DataFrame, which will correspond to the 4 most common majors.
.get(""Section"") get the ""Section"" column,
which also contains counts.
.plot(kind=""barh"") will plot these counts in a
horizontal bar chart.

The argument we give to take is
[-4, -3, -2, -1], because that will give us back a
DataFrame corresponding to the counts of the 4 most common majors, with
the most common major (""MA30"") at the bottom. We want the
most common major at the bottom of the Series that
.plot(kind=""barh"") is called on because the bottom row will
correspond to the top bar – remember, kind=""barh"" plots one
bar per row, but in reverse order.",4.0,Hard
433,Sp,23,Midterm,,Problem 7,Problem 7.2,"Suppose we select two students in popular majors at
random with replacement. What is the probability that both have
""EN"" in their major code? Give your answer in the form of a
simplified fraction.","\frac{1}{4}

Total number of students in popular majors: 30 + 25 + 15 + 10 = 80
The number of students with ""EN"" in their major code is
the sum of ""EN30"" and ""EN25"" students: 25 + 15 = 40

The probability that a single student chosen at random from the
popular majors has ""EN"" in their major code is number of
""EN"" majors divided by total number of popular majors.
P(\text{one has ``EN""}) =
\frac{40}{80} = \frac{1}{2}
Since the students are chosen with replacement, the probabilities
remain consistent for each draw. So the probability that both randomly
chosen students (with replacement) have ""EN"" in their major
code is
P(\text{both have ``EN""}) =
P(\text{one has ``EN""}) \cdot P(\text{one has ``EN""}) =
\frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}",61.0,Medium
434,Sp,23,Midterm,,Problem 7,Problem 7.3,"Suppose we select two students in popular majors at
random with replacement. What is the probability that we select one
""CG35"" major and one ""MA30"" major (in any
order)?

 \frac{1}{2}
 \frac{3}{4}
 \frac{3}{8}
 \frac{3}{16}
 \frac{3}{32}
 \frac{3}{64}","\frac{3}{32}

The probability of selecting one ""CG35"" major first and
then one ""MA30"" major is: \frac{10}{80} \cdot \frac{30}{80} = \frac{1}{8} \cdot \frac{3}{8} = \frac{3}{64}
The probability of selecting one ""MA30"" major first and
then one ""CG35"" major is: \frac{30}{80} \cdot \frac{10}{80} = \frac{3}{8} \cdot \frac{1}{8} = \frac{3}{64}

Since the two events are mutually exclusive (they cannot both
happen), we sum their probabilities to get the combined probability of
the event occurring in any order:
P(\text{one ``CG35"" and one
``MA30""}) = P(\text{``CG35"" then ``MA30"" OR ``MA30""
then ``CG35""}) = P(\text{``CG35 then ``MA30""}) +
P(\text{``MA30"" then ``CG35""}) = \frac{3}{64} + \frac{3}{64} =
\frac{6}{64} = \frac{3}{32}",18.0,Hard
435,Sp,23,Midterm,,Problem 7,Problem 7.4,"Suppose we select k
students in popular majors at random with replacement. What is the
probability that we select at least one ""CG35"" major?

 \frac{7}{8}
 \frac{7^k}{8^k}
 \frac{1}{7^k}
 \frac{1}{8^k}
 \frac{8^k - 7^k}{7^k}
 \frac{8^k - 7^k}{8^k}","\frac{8^k -
7^k}{8^k}
Of the 80 students in popular majors, 10 are in ""CG35"".
To determine the likelihood of selecting at least one
""CG35"" major out of k random draws with replacement, we
first find the probability of not drawing a ""CG35"" major in
all k attempts by
following the steps below:

The probability of not selecting a ""CG35"" major in
one draw is 1 - \frac{10}{80} = 1 - \frac{1}{8} = \frac{7}{8}
The probability of not selecting a ""CG35"" major in
k draws is, then,
\frac{7^k}{8^k}, since each draw is
independent.

Now, by subtracting this probability from 1, we get the probability
of drawing at least one ""CG35"" major in those k attempts. So, the probability of
selecting at least one ""CG35"" major in k attempts is 1 - \frac{7^k}{8^k} = \frac{8^k}{8^k} -
\frac{7^k}{8^k} = \frac{8^k - 7^k}{8^k}",58.0,Medium
436,Sp,23,Midterm,,Problem 8,Problem 8,"We’d like to select three students at random from the entire class to
win extra credit (not really). When doing so, we want to guarantee that
the same student cannot be selected twice, since it wouldn’t really be
fair to give a student double extra credit.
Fill in the blanks below so that prob_all_unique is an
estimate of the probability that all three students selected are in
different majors.
Hint: The function np.unique, when called on an
array, returns an array with just one copy of each unique element in the
input. For example, if vals contains the values
1, 2, 2, 3, 3, 4, np.unique(vals) contains the
values 1, 2, 3, 4.
    unique_majors = np.array([])
    for i in np.arange(10000):
        group = np.random.choice(survey.get(""Major""), 3, __(a)__)
        __(b)__ = np.append(unique_majors, len(__(c)__))
        
    prob_all_unique = __(d)__",,,
437,Sp,23,Midterm,,Problem 8,Problem 8.1,"What goes in blank (a)?

 replace=True
 replace=False","replace=False
Since we want to guarantee that the same student cannot be selected
twice, we should sample without replacement.",77.0,Easy
438,Sp,23,Midterm,,Problem 8,Problem 8.2,What goes in blank (b)?,,65.0,Medium
439,Sp,23,Midterm,,Problem 8,Problem 8.3,What goes in blank (c)?,"np.unique(group)
In each iteration of our for-loop, we’re interested in
finding the number of unique majors among the 3 students who were
selected. We can tell that this is what we’re meant to store in
unique_majors by looking at the options in the next
subpart, which involve checking the proportion of times that the values
in unique_majors is 3.
The majors of the 3 randomly selected students are stored in
group, and np.unique(group) is an array with
the unique values in group. Then,
len(np.unique(group)) is the number of unique majors in the
group of 3 students selected.",45.0,Hard
440,Sp,23,Midterm,,Problem 8,Problem 8.4,"What could go in blank (d)? Select all that apply. At least one
option is correct; blank answers will receive no credit.

 (unique_majors > 2).mean()
 (unique_majors.sum() > 2).mean()
 np.count_nonzero(unique_majors > 2).sum() / len(unique_majors > 2)
 1 - np.count_nonzero(unique_majors != 3).mean()
 unique_majors.mean() - 3 == 0","Option 1 only
Let’s break down the code we have so far:

An empty array named unique_majors is initialized to
store the number of unique majors in each iteration of the
simulation.
The simulation runs 10,000 times, and in every iteration: Three
majors are selected at random from the survey dataset without
replacement. This ensures that the same item is not chosen more than
once within a single iteration. The np.unique function is
employed to identify the number of unique majors among the selected
three. The result is then appended to the unique_majors
array.
Following the simulation, the objective is to determine the fraction
of iterations in which all three selected majors were unique. Since the
maximum number of unique majors that can be selected in a group of three
is 3, the code checks the fraction of times the
unique_majors array contains a value greater than 2.

Let’s look at each option more carefully.

Option 1:
(unique_majors > 2).mean() will create a Boolean array
where each value in unique_majors is checked if it’s
greater than 2. In other words, it’ll return True for each
3 and False otherwise. Taking the mean of this Boolean
array will give the proportion of True values, which
corresponds to the probability that all 3 students selected are in
different majors.
Option 2: (unique_majors.sum() > 2)
will generate a single Boolean value (either True or
False) since you’re summing up all values in the
unique_majors array and then checking if the sum is greater
than 2. This is not what you want. .mean() on a single
Boolean value will raise an error because you can’t compute the mean of
a single Boolean.
Option 3:
np.count_nonzero(unique_majors > 2).sum() / len(unique_majors > 2)
would work without the .sum().
unique_majors > 2 results in a Boolean array where each
value is True if the respective simulation yielded 3 unique
majors and False otherwise. np.count_nonzero()
counts the number of True values in the array, which
corresponds to the number of simulations where all 3 students had unique
majors. This returns a single integer value representing the count. The
.sum() method is meant for collections (like arrays or
lists) to sum their elements. Since np.count_nonzero
returns a single integer, calling .sum() on it will result
in an AttributeError because individual numbers do not have a sum
method. len(unique_majors > 2) calculates the length of
the Boolean array, which is equal to 10,000 (the total number of
simulations). Because of the attempt to call .sum() on an
integer value, the code will raise an error and won’t produce the
desired result.
Option 4:
np.count_nonzero(unique_majors != 3) counts the number of
trials where not all 3 students had different majors. When you call
.mean() on an integer value, which is what
np.count_nonzero returns, it’s going to raise an
error.
Option 5:
unique_majors.mean() - 3 == 0 is trying to check if the
mean of unique_majors is 3. This line of code will return
True or False, and this isn’t the right
approach for calculating the estimated probability.",56.0,Medium
441,Sp,24,Midterm,,Problem 1,Problem 1,"Fill in the blanks in the function sum_phone below. This
function should take as input a string representing a phone number,
given in the form ""xxx-xxx-xxxx"", and return the sum of the
digits of that phone number, as an int.
For example, sum_phone(""501-800-3002"") should evaluate
to 19.
     def sum_phone(phone_number):
        total = 0
        for digit in phone_number:
            if ___(a)___:
                ___(b)___ 
        return total","(a) digit != ""-""
(b) total = total + int(digit)


We only care if the current digit is a number, and the
only character in the string of the phone_number is the
hyphen (-). For instance, in the given phone number of
""501-800-3002"", we want to extract the actual numbers in
the string, without the hyphens. So, because we cannot establish a
numerical value to a hyphen, we exclude it.",34.0,Hard
442,Sp,24,Midterm,,Problem 2,Problem 2,"The first contact in contacts is your friend Calvin, who
has an interesting phone number, with all the digits in descending
order: 987-654-3210. Fill in the blanks below so that each expression
evaluates to the sum of the digits in Calvin’s phone number.",,,
443,Sp,24,Midterm,,Problem 2,Problem 2.1,"contacts.get(""Phone"").apply(sum_phone).iloc[___(a)___]","0
(a) should be filled with 0 because
.iloc[0] refers to the first item in a Series, which
corresponds to Calvin.",89.0,Easy
444,Sp,24,Midterm,,Problem 2,Problem 2.2,"sum_phone(contacts.get(""Phone"").loc[___(b)___])","""Calvin""
(b) should be filled with ""Calvin"" because
.loc[] accesses an element of Series by its row label. In
this case, ""Calvin"" is the index label of the Series
element that contains Calvin’s phone number.",84.0,Easy
445,Sp,24,Midterm,,Problem 2,Problem 2.3,"np.arange(__(c)__,__(d)__,__(e)__).sum() ","(c): 0 or alternate solution 9
(d): 10 or alternate solution
-1
(e): 1 or alternate solution
-1

The expression uses np.arange() to generate a range of
numbers and then sums them up. From the problem, we can see that
Calvin’s phone number includes every digit from 9 to 0, so summing this
is equivalent to summing the digits from 9 down to 0 or from 0 to 9.
np.arange(0, 10, 1) generates [0, 1, 2, 3, 4, 5, 6, 7,
8, 9]. Alternatively, using the numbers in descending order (like the
digits in Calvin’s phone number): np.arange(9, -1, -1)
generates [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]. Both correctly sum up to
45.",74.0,Medium
446,Sp,24,Midterm,,Problem 3,Problem 3,"Write a Python expression that evaluates to a DataFrame of only your
contacts whose phone numbers end in ""6789"".
Note: Do not use slicing, if you know what that is.
You must use methods from this course.","contacts[contacts.get(""Phone"").str.contains(""6789"")] or
contacts[contacts.get(""Phone"").str.contains(""-6789"")]

contacts.get(""Phone""): retrieves the
""Phone"" column from the contacts DataFrame as a
Series.
.str.contains(""6789""): applies a string method that
checks for the presence of the substring ""6789"" in each
phone number, which could only present in the end of the phone number.
It returns a Boolean Series indicating True for phone numbers containing
this substring.
contacts[...]: retrieve all rows where the condition is
True.",63.0,Medium
447,Sp,24,Midterm,,Problem 4,Problem 4,,,,
448,Sp,24,Midterm,,Problem 4,Problem 4.1,"Oh no! A monkey has grabbed your phone and is dialing a phone number
by randomly pressing buttons on the keypad, such that each button
pressed is equally likely to be any of ten digits 0 through 9.
The monkey managed to dial a ten-digit number and call that number.
What is the probability that the monkey calls one of your contacts? Give
your answer as a Python expression, using the DataFrame
contacts.","(contacts.shape[0])/(10**10)

contacts.shape[0]: retrieves the number of rows in the
contacts DataFrame, representing the total number of phone numbers you
have stored in your contacts.
10**10: Since the phone number consists of ten digits,
and each digit can be any number from 0 to 9, there are a total of
10**10 possible ten-digit numbers.

Given that each digit pressed is equally likely and independent of
others, the likelihood of hitting a specific number from your contacts
by random chance is simply the count of your contacts divided by the
total number of possible combinations (which is
10**10).",43.0,Hard
449,Sp,24,Midterm,,Problem 4,Problem 4.2,"Now, your cat is stepping carefully across the keypad of your phone,
pressing 10 buttons. Each button is sampled randomly without
replacement from the digits 0 through 9.
You catch your cat in the act of dialing, when the cat has already
dialed 987-654. Based on this information, what is the probability that
the cat dials your friend Calvin’s number, 987-654-3210? Give your
answer as an unsimplified mathematical expression.","\dfrac{1}{4 \cdot
3 \cdot 2 \cdot 1}
The cat has already dialed “987-654”. Since the first six digits are
fixed and chosen without replacement, the only remaining digits to be
dialed are 3, 2, 1, and 0. The sequence “3210” must be dialed in that
exact order from the remaining digits.

The probability of dialing ‘3’ first is \dfrac{1}{4} (4 digits are remaining).
The probability of then dialing ‘2’ next is \dfrac{1}{3} (3 digits are remaining).
The probability of then dialing ‘1’ next is \dfrac{1}{2} (2 digits are remaining).
The probability of lastly dialing ‘0’ is \dfrac{1}{1} (as it’s the only digit
left).

The product of these probabilities gives \dfrac{1}{4 \cdot 3 \cdot 2 \cdot 1},
representing the likelihood of this specific sequence occurring.",55.0,Medium
450,Sp,24,Midterm,,Problem 5,Problem 5,"Arya’s phone number has an interesting property: after the area code
(the first three digits), the remaining seven numbers of his phone
number consist of only two distinct digits.
Recall from the previous question that when the monkey dials a phone
number, each digit it selects is equally likely to be any of the digits
0 through 9. Further, when the cat is dialing a phone
number, it makes sure to only use each digit once.
You’re interested in estimating the probability that a phone number
dialed by the monkey or the cat has exactly two distinct digits after
the area code, like Arya’s phone number. You write the following code,
which you plan to use for both the monkey and cat scenarios.
    digits = np.arange(10)
    property_count = 0
    num_trials = 10000
    for i in np.arange(num_trials):
        after_area_code = __(x)__
        num_distinct = len(np.unique(after_area_code))
        if __(y)__:
            property_count = property_count + 1
    probability_estimate = property_count / num_trials",,,
451,Sp,24,Midterm,,Problem 5,Problem 5.1,"First, you want to estimate the probability that the
monkey randomly generates a number with only 2 distinct
digits after the area code. What code should be used to fill in blank
(x)?","np.random.choice(digits, 7, replace = True) or
np.random.choice(digits, 7)
The code simulates the monkey dialing seven digits where each digit
is selected randomly from the digits 0 through 9, and digits can repeat
(replace = True).",50.0,Medium
452,Sp,24,Midterm,,Problem 5,Problem 5.2,"Next, you want to estimate the probability that the
cat randomly generates a number with only 2 distinct
digits after the area code. What code should be used to fill in blank
(x)?","np.random.choice(digits, 7, replace = False)
The code simulates random dialing by a cat without replacement
(replace = False). Each digit from 0 to 9 is used only
once.",,
453,Sp,24,Midterm,,Problem 5,Problem 5.3,"In either case, whether you’re simulating the monkey or the cat, what
should be used to fill in blank (y)?","num_distinct == 2
This part of the code checks if the number of unique digits in the
dialed number is exactly two.",52.0,Medium
454,Sp,24,Midterm,,Problem 5,Problem 5.4,"When you are simulating the cat, what will the value
of probability_estimate be after the code executes?","0
Since the cat dials each digit without replacement, it’s impossible
for the dialed number to contain only two distinct digits (as it would
need to repeat some digits to achieve this). Thus, no trial will meet
the condition num_distinct == 2, resulting in a
property_count of 0 and therefore a probability_estimate of 0.",,
455,Sp,24,Midterm,,Problem 6,Problem 6,"Suppose Charlie and Norah each have separate DataFrames for their
contacts, called charlie and norah,
respectively. These DataFrames have the same column names and format as
your DataFrame, contacts.
As illustrated in the diagram below, Charlie has 172 contacts in
total, whereas Norah has 88 contacts. 12 of these contacts are shared,
meaning they appear in both charlie and
norah.",,,
456,Sp,24,Midterm,,Problem 6,Problem 6.1,"What does the following expression evaluate to?
charlie.merge(norah, left_index=True, right_index=True).shape[0]    ","12
The code merges DataFrames charlie and
norah on their indexes, so the resulting DataFrame will
contain one row for every match between their indexes (‘Person’ since
they follow the same format as DataFrame contact). From the
Venn Diagram, we know that Charlie and Norah have 12 contacts in common,
so the resulting DataFrame will contain 12 rows: one row for each shared
contact.
Thus,
charlie.merge(norah, left_index=True, right_index=True).shape[0]
returns the row number of the resulting DataFrame, which is 12.",66.0,Medium
457,Sp,24,Midterm,,Problem 6,Problem 6.2,"One day, when updating her phone’s operating system, Norah
accidentally duplicates the 12 contacts she has in common with Charlie.
Now, the norah DataFrame has 100 rows.
What does the following expression evaluate to?
norah.merge(norah, left_index=True, right_index=True).shape[0]   ","24 \cdot 2 + 76 =
124
Since Norah duplicates 12 contacts, the norah DataFrame
now has 76 unique rows + 12 rows + 12 duplicated rows. Note that the
above code is now merging norah with itself on indexes.
After merging, the resulting DataFrame will contain 76 unique rows,
as there is only one match for each unique row. As for the duplicated
rows, each row can match twice, and we have 24 rows. Thus the resulting
DataFrame’s row number = 76 + 2 \cdot 24 =
124.
For better understanding, imagine we have a smaller DataFrame
nor with only one contact Jim. After duplication, it will
have two identical rows of Jim. For easier explanation, let’s denote the
original row Jim1, and duplicated row Jim2. When merging Nor with
itself, Jim1 can be matched with Jim1 and Jim2, and Jim2 can be matched
with Jim1 and Jim2, resulting $= 2 = 4 $ number of rows.",3.0,Hard
458,Sp,24,Midterm,,Problem 7,Problem 7,"Recall from the data description that the ""DOB"" column
in contacts contains the date of birth of everyone in your
contacts list, as a string formatted like ""MM-DD-YYYY"".
Looking at the calendar, you see that today’s date is May 3rd, 2024,
which is ""05-03-2024"".",,,
459,Sp,24,Midterm,,Problem 7,Problem 7.1,"Using today’s date, fill in the blanks in the function
age_today so that the function takes as input someone’s
date of birth, as a string formatted like ""MM-DD-YYYY"", and
returns that person’s age, as of today.

    def age_today(dob):
        dob = dob.split(""-"")
        month = ___(a)___ # the month, as an int
        day = ___(b)___ # the day, as an int
        year = ___(c)___ # the year, as an int
        if ___(d)___:
            return 2024 - year
        return 2024 - year - 1","(a): int(dob[0])
(b): int(dob[1])
(c): int(dob[2])
(d):
(month < 5) or (month == 5 and day <= 3)

.split() method divides a string into a list based on delimiter. In
this probelm, dob.split(""-"") returns a list of substrings
that were separated by hyphens in the original string formatted like
""MM-DD-YYYY"". Thus, after using the .split(""-""), the resulting list will be formatted like[“MM”,
“DD”, “YYYY”]. Thus, we can access month, day, year by its position in
the list dob.
Note that the comment asks month, day, year as int, do we need to
convert them from string to int datatype, by using int(). Thus, we have
month = int(dob[0]), day = int(dob[1]),
year = int(dob[2]). This is for us to calculate age later
by comparing month and day numebrs.
Then to calculate the age of the person given today is “05-03-2024”,
we can use 2024-year. But if a person is born after May
3rd, then they are techinically 1 year younger than that, so we should
use 2024-year-1. Thus in the if statment, we use
(month < 5) or (month == 5 and day <= 3) to compare
if the person is born in January, February, March, April or one of the
first three days of May.",59.0,Medium
460,Sp,24,Midterm,,Problem 7,Problem 7.2,"Write a Python expression that evaluates to the average age of all of
your contacts, as of today.","contacts.get(""DOB"").apply(age_today).mean()
contacts.get(""DOB"") gets the column ‘DOB’ column in
contacts DataFrame as a series.
.apply(age_today) applies the function
age_today to each element in the series to calculate age,
and returns a series of ages of all contacts.
.mean() calculates the average of the series, giving the
average age of all contacts.",80.0,Easy
461,Sp,24,Midterm,,Problem 8,Problem 8,"You wonder if any of your friends have the same birthday, for example
two people both born on April 3rd. Fill in the blanks below so that the
given expression evaluates to the largest number of people in
contacts who share the same birthday.
Note: People do not need to be born in the same year
to share a birthday!
contacts.groupby(___(a)___).___(b)___.get(""Phone"").___(c)___",,72.0,Medium
462,Sp,24,Midterm,,Problem 9,Problem 9,"Consider the histogram generated by the following code.
contacts.plot(kind=""hist"", y=""Day"", density=True, bins=np.arange(1, 33))
Which of the following questions would you be able to answer from
this histogram? Select all that apply.

 How many of your contacts were born on a Monday?
 How many of your contacts were born on January 1st?
 How many of your contacts were born on the first day of the
month?
 How many of your contacts were born on the last day of the month?","Option 3: How many of your contacts were
born on the first day of the month?
contacts.plot(kind=""hist"", y=""Day"", density=True, bins=np.arange(1, 33))
generates a histogram that visualizes the distribution of values in the
‘Day’ column of the contacts DataFrame.
bins=np.arange(1, 33) creates the bins
[1,2], [2,3], ..., [31,32]. Each bar in the histogram
represents the density of a day.
We can answer the question “How many of your contacts were born on
the first day of the month?” by checking the bar containing the first
day (the first bar).
However, the histogram doesn’t contain information of day of the week
or month, so it can’t answer questions in options 1 or 2.
We also can’t choose option 4, because we have months with varying
numbers of days (28, 29, 30, or 31). For months with 31 days, we know
for sure it’s the last day of the month. For bars with 28, 29, and 30
days, we won’t be able to tell which proportion of days are the last day
of the month since months with 31 days will have a count in all the
previous number of days.",80.0,Easy
463,Sp,24,Midterm,,Problem 10,Problem 10,"After looking at the distribution of ""Day"" for your
contacts, you wonder how that distribution might look for other groups
of people.
In this problem, we will consider the distribution of day of birth
(just the day of the month, not the month or year) for all babies born
in the year 2023, under the assumption that all babies born in 2023 were
equally likely to be born on each of the 365 days of the year.
The density histogram below shows this distribution.
Note: 2023 was not a leap year, so February had 28
days.",,,
464,Sp,24,Midterm,,Problem 10,Problem 10.1,"What is height (a)?

 \dfrac{1}{28}
 \dfrac{28}{365}
 \dfrac{12 \cdot 28}{365}
 \dfrac{1}{12}
 \dfrac{12}{365}
 1","\dfrac{12}{365}
2023 was not a leap year, so there are in total 365 days, and
February only has 28 days. All the months have at least 28 days, so in
bin [1,29], there are 12 * 28 data values. And, all the months other
than February have 29th and 30th days, so there are 2 * 11 data values
in bin [29,31]. Lastly, there are 7 months with 31 days, so there are 7
data values in bin [31,32].
There are in total 365 data values, so the proportion of data points
falling into bin [1,29] =\dfrac{12 \cdot
28}{365}. Thus, the area of the bar of bin [1,29] = \dfrac{12 \cdot 28}{365}. Area = Height *
Width, width of the bin is 28, so height(a) =
\dfrac{12 \cdot 28}{365 \cdot 28} = \dfrac{12}{365}.",38.0,Hard
465,Sp,24,Midterm,,Problem 10,Problem 10.2,"Express height (b) in terms of height
(a).

 \text{(b)} = \dfrac{7}{12} \cdot
\text{(a)}
 \text{(b)} = \dfrac{27}{28} \cdot
\text{(a)}
 \text{(b)} = \dfrac{28}{29} \cdot
\text{(a)}
 \text{(b)} = \dfrac{11}{12} \cdot
\text{(a)}
 \text{(b)} = \dfrac{7}{11} \cdot
\text{(a)}
 \text{(b)} = \dfrac{12 \cdot 28}{365} \cdot
\text{(a)}","\text{(b)} =
\dfrac{11}{12} \cdot \text{(a)}
Similarly, height(b) = \dfrac{2 \cdot
11}{365 \cdot 2} = \dfrac{11}{365} = \dfrac{11}{12} \cdot
\dfrac{12}{365}.",43.0,Hard
466,Fa,21,Final,,Problem 1,Problem 1,,,,
467,Fa,21,Final,,Problem 1,Problem 1.1,"Which of the following blocks of code correctly assigns
random_art_museums to an array of the names of 10 art
museums, randomly selected without replacement from those in
art_museums? Select all that apply.
Option 1:
def get_10(df):
    return np.array(df.sample(10).get('Name'))

random_art_museums = get_10(art_museums)
Option 2:
def get_10(art_museums):
    return np.array(art_museums.sample(10).get('Name'))

random_art_museums = get_10(art_museums)
Option 3:
def get_10(art_museums):
    random_art_museums = np.array(art_museums.sample(10).get('Name'))

random_art_museums = get_10(art_museums)
Option 4:
def get_10():
    return np.array(art_museums.sample(10).get('Name'))

random_art_museums = get_10()
Option 5:
random_art_museums = np.array([])

def get_10():
    random_art_museums = np.array(art_museums.sample(10).get('Name'))
    return random_art_museums

get_10()

 Option 1
 Option 2
 Option 3
 Option 4
 Option 5
 None of the above",,85.0,Easy
468,Fa,21,Final,,Problem 1,Problem 1.2,"London has the most art museums in the top 100 of any city in the
world. The most visited art museum in London is
'Tate Modern'.
Which of the following blocks of code correctly assigns
best_in_london to 'Tate Modern'? Select all
that apply.
Option 1:
def most_common(df, col):
    return df.groupby(col).count().sort_values(by='Rank', ascending=False).index[0]

def most_visited(df, col, value):
    return df[df.get(col)==value].sort_values(by='Visitors', ascending=False).get('Name').iloc[0]

best_in_london = most_visited(art_museums, 'City', most_common(art_museums, 'City'))
Option 2:
def most_common(df, col):
    print(df.groupby(col).count().sort_values(by='Rank', ascending=False).index[0])

def most_visited(df, col, value):
    print(df[df.get(col)==value].sort_values(by='Visitors', ascending=False).get('Name').iloc[0])

best_in_london = most_visited(art_museums, 'City', most_common(art_museums, 'City'))
Option 3:
def most_common(df, col):
    return df.groupby(col).count().sort_values(by='Rank', ascending=False).index[0]

def most_visited(df, col, value):
    print(df[df.get(col)==value].sort_values(by='Visitors', ascending=False).get('Name').iloc[0])

best_in_london = most_visited(art_museums, 'City', most_common(art_museums, 'City'))

 Option 1
 Option 2
 Option 3
 None of the above","Option 1 only
At a glance, it may seem like there’s a lot of reading to do to
answer the question. However, it turns out that all 3 options follow
similar logic; the difference is in their use of print and
return statements. Whenever we want to “save” the output of
a function to a variable name or use it in another function, we need to
return somewhere within our function. Only Option 1
contains a return statement in both
most_common and most_visited, so it is the
only correct option.
Let’s walk through the logic of Option 1 (which we don’t necessarily
need to do to answer the problem, but we should in order to enhance our
understanding):

First, we use most_common to find the city with the
most art museums. most_common does this by grouping the
input DataFrame df (art_museums, in this case)
by 'City' and using the .count() method to
find the number of rows per 'City'. Note that when using
.count(), all columns in the aggregated DataFrame will
contain the same information, so it doesn’t matter which column you use
to extract the counts per group. After sorting by one of these columns
('Rank', in this case) in decreasing order,
most_common takes the first value in the
index, which will be the name of the 'City'
with the most art museums. This is London,
i.e. most_common(art_museums, 'City') evaluates to
'London' in Option 1 (in Option 2, it evaluates to
None, since most_common there doesn’t
return anything).
Then, we use most_visited to find the museum with the
most visitors in the city with the most museums. This is achieved by
keeping only the rows of the input DataFrame df (again,
art_museums in this case) where the value in the
col ('City') column is value
(most_common(art_museums, 'City'), or
'London'). Now that we only have information for museums in
London, we can sort by 'Visitors' to find the most visited
such museum, and take the first value from the resulting
'Name' column. While all 3 options follow this logic, only
Option 1 returns the desired value, and so only Option
1 assigns best_in_london correctly. (Even if Option 2’s
most_visited used return instead of
print, it still wouldn’t work, since Option 2’s
most_common also uses print instead of
return).",86.0,Easy
469,Fa,21,Final,,Problem 2,Problem 2,"In this question, we’ll keep working with the
art_museums DataFrame.",,,
470,Fa,21,Final,,Problem 2,Problem 2.1,"(Remember to keep the data description from the top of the exam open
in another tab!)
'Tate Modern' is the most popular art museum in London.
But what’s the most popular art museum in each city?
It turns out that there’s no way to answer this easily using the
tools that you know about so far. To help, we’ve created a new Series
method, .last(). If s is a Series,
s.last() returns the last element of s
(i.e. the element at the very end of s).
.last() works with .groupby, too (just like
.mean() and .count()).
Fill in the blanks so that the code below correctly assigns
best_per_city to a DataFrame with one row per city, that
describes the name, number of visitors, and rank of the most visited art
museum in each city. best_per_city should be sorted in
decreasing order of number of visitors. The first few rows of
best_per_city are shown below.


best_per_city = __(a)__.groupby(__(b)__).last().__(c)__

What goes in blank (a)?
What goes in blank (b)?
What goes in blank (c)?",,65.0,Medium
471,Fa,21,Final,,Problem 2,Problem 2.2,"Assume you’ve defined best_per_city correctly.
Which of the following options evaluates to the number of visitors to
the most visited art museum in Amsterdam? Select all that apply.

 best_per_city.get('Visitors').loc['Amsterdam']
 best_per_city[best_per_city.index == 'Amsterdam'].get('Visitors').iloc[0]
 best_per_city[best_per_city.index == 'Amsterdam'].get('Visitors').iloc[-1]
 best_per_city[best_per_city.index == 'Amsterdam'].get('Visitors').loc['Amsterdam']
 None of the above","best_per_city.get('Visitors').loc['Amsterdam'],
best_per_city[best_per_city.index == 'Amsterdam'].get('Visitors').iloc[0],
best_per_city[best_per_city.index == 'Amsterdam'].get('Visitors').iloc[-1],
best_per_city[best_per_city.index == 'Amsterdam'].get('Visitors').loc['Amsterdam']
(Select all except “None of the above”)
best_per_city.get('Visitors').loc['Amsterdam'] We first
use .get(column_name) to get a series with number of
visitors to the most visited art museum, and then locate the number of
visitors to the most visited art museum in Amsterdam using
.loc[index] since we have ""City"" as index.
best_per_city[best_per_city.index == 'Amsterdam'].get('Visitors').iloc[0]
We first query the best_per_city to only include the
DataFrame with one row with index 'Amsterdam'. Then, we get
the 'Visitors' column of this DataFrame. Finally, we use
iloc[0] to access the first and the only value in this
column.
best_per_city[best_per_city.index == 'Amsterdam'].get('Visitors').iloc[-1]
We first query the best_per_city to only include the
DataFrame with one row with index 'Amsterdam'. Then, we get
the 'Visitors' column of this DataFrame. Finally, we use
iloc[-1] to access the last and the only value in this
column.
best_per_city[best_per_city.index == 'Amsterdam'].get('Visitors').loc['Amsterdam']
We first query the best_per_city to only include the
DataFrame with one row with index 'Amsterdam'. Then, we get
the 'Visitors' column of this DataFrame. Finally, we use
loc['Amsterdam'] to access the value in this column with
index 'Amsterdam'.",84.0,Easy
472,Fa,21,Final,,Problem 3,Problem 3,"The table below shows the average amount of revenue from different
sources for art museums in 2003 and 2013.",,,
473,Fa,21,Final,,Problem 3,Problem 3.1,"What is the total variation distance between the distribution of
revenue sources in 2003 and the distribution of revenue sources in 2013?
Give your answer as a proportion (i.e. a decimal between 0 and 1),
not a percentage. Round your answer to three decimal
places.","0.19
Recall, the total variation distance (TVD) is the sum of the absolute
differences in proportions, divided by 2. The absolute differences in
proportions for each source are as follows:

Admissions: |0.15 - 0.24| =
0.09
Restaurants and Catering: |0.09 - 0.12| =
0.03
Store: |0.52 - 0.33| = 0.19
Other: |0.24 - 0.31| = 0.07

Then, we have
\text{TVD} = \frac{1}{2} (0.09 + 0.03 +
0.19 + 0.07) = 0.19",95.0,Easy
474,Fa,21,Final,,Problem 3,Problem 3.2,"Which type of visualization would be best suited for comparing the
two distributions in the table?

 Scatter plot
 Line plot
 Overlaid histogram
 Overlaid bar chart","Overlaid bar chart
A scatter plot visualizes the relationship between
two numerical variables. In this problem, we only have to visualize the
distribution of a categorical variable.
A line plot shows trends in numerical variables over
time. In this problem, we only have categorical variables. Moreover,
when it says over time, it is suitable for plotting change in multiple
years (e.g. 2001, 2002, 2003, … , 2013), or even with data of days. In
this question, we only want to compare the distribution of 2003 and
2013, this makes the line plot not useful. In addition, if you try to
draw a line plot for this question, you will find the line plot fails to
visualize distribution (e.g. the idea of 15%, 9%, 52%, and 24% add up to
100%).
An overlaid graph is useful in this question since this visualizes
comparison between the two distributions.
However, an overlaid histogram is not useful in this
problem. The key reason is the differences between a histogram and a bar
chart.
Bar Chart: Space between the bars; 1 categorical axis, 1 numerical
axis; order does not matter
Histogram: No space between the bars; intervals on axis; 2 numerical
axes; order matters
In the question, we are plotting 2003 and 2013 distributions of four
categories (Admissions, Restaurants and Catering, Store, and Other).
Thus, an overlaid bar chart is more appropriate.",74.0,Medium
475,Fa,21,Final,,Problem 3,Problem 3.3,"Note: This problem is out of scope; it
covers material no longer included in the course.
Notably, there was an economic recession in 2008-2009. Which of the
following can we conclude was an effect of the recession?

 The increase in revenue from admissions, as more people were visiting
museums.
 The decline in revenue from museum stores, as people had less money
to spend.
 The decline in total revenue, as fewer people were visiting
museums.
 None of the above","None of the above
Since we are only given the distribution of the revenue, and have no
information about the amount of revenue in 2003 and 2013, we cannot
conclude how the revenue has changed from 2003 to 2013 after the
recession.
For instance, if the total revenue in 2003 was 100 billion USD and
the total revenue in 2013 was 50 billion USD, revenue from admissions in
2003 was 100 * 15% = 15 billion USD, and revenue from admissions in 2003
was 50 * 24% = 12 billion USD. In this case, we will have 15 > 12,
the revenue from admissions has declined rather than increased (As
stated by ‘The increase in revenue from admissions, as more people were
visiting museums.’). Similarly, since we don’t know the total revenue in
2003 and 2013, we cannot conclude ‘The decline in revenue from museum
stores, as people had less money to spend.’ or ‘The decline in total
revenue, as fewer people were visiting museums.’",72.0,Medium
476,Fa,21,Final,,Problem 4,Problem 4,,,,
477,Fa,21,Final,,Problem 4,Problem 4.1,"The Museum of Natural History has a large collection of dinosaur
bones, and they know the approximate year each bone is from. They want
to use this sample of dinosaur bones to estimate the total
number of years that dinosaurs lived on Earth. We’ll make the
assumption that the sample is a uniform random sample from the
population of all dinosaur bones. Which statistic below will give the
best estimate of the population parameter?

 sample sum
 sample max - sample min
 2 \cdot (sample mean - sample
min)
 2 \cdot (sample max - sample
mean)
 2 \cdot sample mean
 2 \cdot sample median","sample max - sample min
Our goal is to estimate the total number of years that
dinosaurs lived on Earth. In other words, we want to know the
range of time that dinosaurs lived on Earth, and by definition range =
biggest value - smallest value. By using “sample max - sample min”, we
calculate the difference between the earliest and the latest dinosaur
bones in this uniform random sample, which helps us to estimate the
population range.",52.0,Medium
478,Fa,21,Final,,Problem 4,Problem 4.2,"The curator at the Museum of Natural History, who happens to have
taken a data science course in college, points out that the estimate of
the parameter obtained from this sample could certainly have come out
differently, if the museum had started with a different sample of bones.
The curator suggests trying to understand the distribution of the sample
statistic. Which of the following would be an appropriate way to create
that distribution?

 bootstrapping the original sample
 using the Centrual Limit Theorem
 both bootstrapping and the Central Limit Theorem
 neither bootstrapping nor the Central Limit Theorem","neither bootstrapping nor the Central Limit
Theorem
Recall, the Central Limit Theorem (CLT) says that the probability
distribution of the sum or average of a large random
sample drawn with replacement will be roughly normal, regardless of the
distribution of the population from which the sample is drawn. Thus, the
theorem only applies when our sample statistics is sum or average, while
in this question, our statistics is range, so CLT does not apply.
Bootstrapping is a valid technique for estimating measures of central
tendency, e.g. the population mean, median, standard deviation, etc. It
doesn’t work well in estimating extreme or sensitive values, like the
population maximum or minimum. Since the statistic we’re trying to
estimate is the difference between the population maximum and population
minimum, bootstrapping is not appropriate.",20.0,Hard
479,Fa,21,Final,,Problem 5,Problem 5,,,,
480,Fa,21,Final,,Problem 5,Problem 5.1,"Now, the Museum of Natural History wants to know how many visitors
they have in a year. However, their computer systems are rather archaic
and so they aren’t able to keep track of the number of tickets sold for
an entire year. Instead, they randomly select five days in the year, and
keep track of the number of visitors on those days. Let’s call these
numbers v_1, v_2, v_3,
v_4, and v_5.
Which of the following is the best estimate the number of visitors
for the entire year?

 v_1 + v_2 + v_3 + v_4 + v_5
 \frac{5}{365}\cdot(v_1 + v_2 + v_3 + v_4 +
v_5)
 \frac{365}{5}\cdot(v_1 + v_2 + v_3 + v_4 +
v_5)
 365\cdot v_3","\frac{365}{5}\cdot(v_1 + v_2 + v_3 + v_4 +
v_5)
Our sample is the number of visitors on the five days, and our
population is the number of visitors in all 365 days.
First, we calculate the sample mean, the average number of visitors
in the 5 days, which is m =
\frac{1}{5}\cdot(v_1 + v_2 + v_3 + v_4 + v_5). We use this
statistic to estimate the population mean, the average number of
visitors in this year.
Then, we use the estimated population mean to calculate the estimated
pupulation sum, so we multiply the number of days in a year (365) with
the estimated population mean. We get 365 m =
\frac{365}{5}\cdot(v_1 + v_2 + v_3 + v_4 + v_5)",92.0,Easy
481,Fa,21,Final,,Problem 5,Problem 5.2,"Now we’re interested in predicting the admission cost of a museum
based on its number of visitors. Suppose:

admission cost and number of visitors are linearly associated
with a correlation coefficient of 0.25,
the number of visitors at the Museum of Natural History is six
standard deviations below average,
the average cost of museum admission is 15 dollars, and
the standard deviation of admission cost is 3 dollars.

What would the regression line predict for the admission cost (in
dollars) at the Museum of Natural History? Give your answer as a number
without any units, rounded to three decimal places.","10.500
Recall, we can make predictions in standard units with the following
formula
 \text{predicted}\ y_{su} = r \cdot
x_{su}
We’re given that the correlation coefficient, r, between visitors and admission cost is
0.25. Here, we’re using the number of visitors (x) to predict admission cost (y). Given that the number of visitors at the
Museum of Natural History is 6 standard deviations
below average,
\text{predicted}\ y_{su} =  r \cdot x_{su}
= 0.25 \cdot -6 =  -1.5
We then compute y, which is the admission cost (in dollars) at the
Museum of Natural History.
\begin{align*} y_{su} &= \frac{y-
\text{Mean of } y}{\text{SD of } y}\\
-1.5 &= \frac{y-15}{3} \\
-1.5 \cdot 3 &= y-15\\
-4.5 + 15 &= y\\
y &= \boxed{10.5}
\end{align*}

So, the regression line predicts that the admission cost at the
Museum of Natural History is $10.50.",62.0,Medium
482,Fa,21,Final,,Problem 6,Problem 6,"Researchers from the San Diego Zoo, located within Balboa Park,
collected physical measurements of several species of penguins in a
region of Antarctica.
One piece of information they tracked for each of 330 penguins was
its mass in grams. The average penguin mass is 4200 grams, and the
standard deviation is 840 grams.",,,
483,Fa,21,Final,,Problem 6,Problem 6.1,"Consider the histogram of mass below.


Select the true statement below.

 The median mass of penguins is larger than the average mass of
penguins
 The median mass of penguins is roughly equal to the average mass of
penguins (within 50 grams)
 The median mass of penguins is less than the average mass of
penguins
 It is impossible to determine the relationship between the median and
average mass of penguins just by looking at the above histogram","The median mass of penguins is less than the
average mass of penguins
This is a distribution that is skewed to the right, so mean is
greater than median.",87.0,Easy
484,Fa,21,Final,,Problem 6,Problem 6.2,"Which of the following is a valid conclusion that we can draw solely
from the histogram above?

 The number of penguins with a mass of exactly 3500 grams is greater
than the number of penguins with a mass of exactly 5500 grams.
 The number of penguins with a mass of at most 3500 grams is greater
than the number of penguins with a mass of at least 5500 grams.
 There is an odd number of penguins in the dataset.
 The number penguins with a mass of exactly 4000 grams is greater than
zero.
 None of the above.","The number of penguins with a mass of at
most 3500 grams is greater than the number of penguins with a mass of at
least 5500 grams.
Recall, a histogram has intervals on the axis, so we cannot know the
frequency of an exact value. Thus, we cannot conclude statements 1, 3,
4. Since the frequency of an exact value is unknown, for statement 3, it
is possible that all numbers we have in this distribution are even.
Although in the graph, we are only given frequency rather than number,
we can justify statement 2 by comparing the area in the left side of
3500, and the area in the right side of 5500. You can either estimate by
visually comparing the areas of both parts or compute the area sum of
both sides by estimating the bars’ height and windth.",89.0,Easy
485,Fa,21,Final,,Problem 6,Problem 6.3,"For your convenience, we show the histogram of mass again below.


Recall, there are 330 penguins in our dataset. Their average mass is
4200 grams, and the standard deviation of mass is 840 grams.
Per Chebyshev’s inequality, at least what percentage of penguins have
a mass between 3276 grams and 5124 grams? Input your answer as a
percentage between 0 and 100, without the % symbol. Round to three
decimal places.","17.355
Recall, Chebyshev’s inequality states that No matter what the shape
of the distribution is, the proportion of values in the range “average ±
z SDs” is at least 1 -
\frac{1}{z^2}.
To approach the problem, we’ll start by converting 3276 grams and
5124 grams to standard units. Doing so yields \frac{3276 - 4200}{840} = -1.1, similarly,
\frac{5124 - 4200}{840} = 1.1. This
means that 3276 is 1.1 standard deviations below the
mean, and 5124 is 1.1 standard deviations above the
mean. Thus, we are calculating the proportion of values in the range
“average ± 1.1 SDs”.
When z = 1.1, we have 1 - \frac{1}{z^2} = 1 - \frac{1}{1.1^2} \approx
0.173553719, which as a percentage rounded to three decimal
places is 17.355\%.",76.0,Easy
486,Fa,21,Final,,Problem 6,Problem 6.4,"Per Chebyshev’s inequality, at least what percentage of penguins have
a mass between 1680 grams and 5880 grams?

 50%
 55.5%
 65.25%
 68%
 75%
 88.8%
 95%","75%
Recall: proportion with z SDs of the
mean








Percent in Range
All Distributions (via Chebyshev’s Inequality)
Normal Distributions




\text{average} \pm 1 \
\text{SD}
\geq 0\%
\approx 68\%


\text{average} \pm 2\text{SDs}
\geq 75\%
\approx 95\%


\text{average} \pm 3\text{SDs}
\geq 88\%
\approx 99.73\%



To approach the problem, we’ll start by converting 3276 grams and
5124 grams to standard units. Doing so yields \frac{1680 - 4200}{840} = -3, similarly,
\frac{5880 - 4200}{840} = 2. This means
that 1680 is 3 standard deviations below the mean, and
5880 is 2 standard deviations above the mean.
Proportion of values in [-3 SUs, 2 SUs] >= Proportion of values in
[-2 SUs, 2 SUs] >= 75% (Since we cannot assume that the distribution
is normal, we look at the All Distributions (via Chebyshev’s
Inequality) column for proportion).
Thus, at least 75% of the penguins have a mass
between 1680 grams and 5880 grams.",72.0,Medium
487,Fa,21,Final,,Problem 6,Problem 6.5,"The distribution of mass in grams is not roughly normal. Is the
distribution of mass in standard units roughly normal?

 Yes
 No
 Impossible to tell","No
The shape of the distribution does not change since we are scaling
the x values for all data.",60.0,Medium
488,Fa,21,Final,,Problem 6,Problem 6.6,"Suppose all 330 penguin body masses (in grams) that the researchers
collected are stored in an array called masses. We’d like
to estimate the probability that two different randomly selected
penguins from our dataset have body masses within 50 grams of one
another (including a difference of exactly 50 grams). Fill in the
missing pieces of the simulation below so that the function
estimate_prob_within_50g returns an estimate for this
probability.
def estimate_prob_within_50g():
    num_reps = 10000
    within_50g_count = 0
    for i in np.arange(num_reps):
        two_penguins = np.random.choice(__(a)__)
        if __(b)__:
            within_50g_count = within_50g_count + 1
    return within_50g_count / num_reps
What goes in blank (a)? What goes in blank (b)?","(a) masses, 2, replace=False
(b) abs(two_penguins[0] - two_penguins[1])<=50

Recall, np.random.choice( ) can have three parameters
array, n, replace=False, and returns n elements from the
array at random, without replacement. We are randomly choosing 2
different penguins from the masses
array, so we are using np.random.choice( )
without replacement.
We want to count the number of pairs of penguins that have body
masses difference within 50 grams, so we are using the index to access
the two penguins generated from two_penguins and
calculating their absolute difference with abs(). And in
this if condition, we only want to have penguins with
absolute difference less than or equal to 50, so we write a
<= condition to justify whether the generated pairs of
penguins fulfill this requirement.",84.0,Easy
489,Fa,21,Final,,Problem 6,Problem 6.7,"Recall, there are 330 penguins in our dataset. Their average mass is
4200 grams, and the standard deviation of mass is 840 grams. Assume that
the 330 penguins in our dataset are a random sample from the population
of all penguins in Antarctica. Our sample gives us one estimate of the
population mean.
To better estimate the population mean, we bootstrapped our sample
and plotted a histogram of the resample means, then took the middle 68
percent of those values to get a confidence interval. Which option below
shows the histogram of the resample means and the confidence interval we
found?

Option 1



Option 2


 Option 3 

 Option 4 ","Option 2
Recall, according to the Central Limit Theorem (CLT), the probability
distribution of the sum or mean of a large random sample drawn with
replacement will be roughly normal, regardless of the distribution of
the population from which the sample is drawn.
Thus, our graph should have a normal distribution. We eliminate
Option 4.
Recall that the standard normal curve has inflection points at z = +-1, which is 68% proportion of a normal
distribution.(inflection point is where a curve goes from “opening down”
to “opening up”) Since we have a confidence intervel of 68% in this
question, by looking at the inflection point, we can eliminate
Option 3
To compute the SD of the sample mean’s distribution, when we don’t
know the population’s SD, we can use the sample’s SD (840): \text{SD of Distribution of Possible Sample Means}
\approx \frac{\text{Sample SD}}{\sqrt{\text{sample size}}} =
\frac{840}{\sqrt{330}} \approx 46.24
Recall: proportion with z SDs of the
mean








Percent in Range
All Distributions (via Chebyshev’s Inequality)
Normal Distributions




\text{average} \pm 1 \
\text{SD}
\geq 0\%
\approx 68\%


\text{average} \pm 2\text{SDs}
\geq 75\%
\approx 95\%


\text{average} \pm 3\text{SDs}
\geq 88\%
\approx 99.73\%



In this question, we want 68% confidence interval, given that the
distribution of sample mean is roughly normal, our CI should have range
\text{sample mean} \pm 1 \ \text{SD}.
Thus, the interval is approximately [4200-46.24 = 4153.76, 4200+46.24=4246.24].
We compare the 68% CI in Option 1, 2 and we choose Option
2 since it has a 68% CI with approximately the same
interval.",66.0,Medium
490,Fa,21,Final,,Problem 6,Problem 6.8,"Suppose boot_means is an array of the resampled means.
Fill in the blanks below so that [left, right] is a 68%
confidence interval for the true mean mass of penguins.
left = np.percentile(boot_means, __(a)__)
right = np.percentile(boot_means, __(b)__)
[left, right]
What goes in blank (a)? What goes in blank (b)?","(a) 16 (b) 84
Recall, np.percentile(array, p) computes the
pth percentile of the numbers in array. To
compute the 68% CI, we need to know the percentile of left tail and
right tail.
left percentile = (1-0.68)/2 = (0.32)/2 =
0.16 so we have 16th percentile
right percentile = 1-((1-0.68)/2) =
1-((0.32)/2) = 1-0.16 = 0.84 so we have 84th percentile",94.0,Easy
491,Fa,21,Final,,Problem 6,Problem 6.9,"Which of the following is a correct interpretation of this confidence
interval? Select all that apply.

 There is an approximately 68% chance that mean weight of all penguins
in Antarctica falls within the bounds of this confidence interval.
 Approximately 68% of penguin weights in our sample fall within the
bounds of this confidence interval.
 Approximately 68% of penguin weights in the population fall within
the bounds of this interval.
 If we created many confidence intervals using the same method,
approximately 68% of them would contain the mean weight of all penguins
in Antarctica.
 None of the above","Option 4 (If we created many confidence
intervals using the same method, approximately 68% of them would contain
the mean weight of all penguins in Antarctica.)
Recall, what a k% confidence level
states is that approximately k% of the
time, the intervals you create through this process will contain the
true population parameter.
In this question, our population parameter is the mean weight of all
penguins in Antarctica. So 86% of the time, the intervals you create
through this process will contain the mean weight of all penguins in
Antarctica. This is the same as Option 4. However, it will be false if
we state it in the reverse order (Option 1) since our population
parameter is already fixed.",81.0,Easy
492,Fa,21,Final,,Problem 7,Problem 7,"Now let’s study the relationship between a penguin’s bill length (in
millimeters) and mass (in grams). Suppose we’re given that

bill length and body mass have a correlation coefficient of
0.55
the average bill length is 44 mm and the standard deviation of bill
lengths is 6 mm
as before, the average body mass is 4200 grams and the standard
deviation of body mass is 840 grams",,,
493,Fa,21,Final,,Problem 7,Problem 7.1,"Which of the four scatter plots below describe the relationship
between bill length and body mass, based on the information provided in
the question?



 Option 1
 Option 2
 Option 3
 Option 4",,91.0,Easy
494,Fa,21,Final,,Problem 7,Problem 7.2,"Suppose we want to find the regression line that uses bill length,
x, to predict body mass, y. The line is of the form y = mx +\ b. What are m and b?
What is m? Give your answer as a
number without any units, rounded to three decimal places.
What is b? Give your answer as a
number without units, rounded to three decimal places.","m = 77,
b = 812
m = r \cdot \frac{\text{SD of }y
}{\text{SD of }x} = 0.55 \cdot \frac{840}{6} = 77 b = \text{mean of }y - m \cdot \text{mean of }x =
4200-77 \cdot 44 = 812",92.0,Easy
495,Fa,21,Final,,Problem 7,Problem 7.3,"What is the predicted body mass (in grams) of a penguin whose bill
length is 44 mm? Give your answer as a number without any units, rounded
to three decimal places.","4200
y = mx\ +\ b = 77 \cdot 44 + 812 = 3388
+812 = 4200",95.0,Easy
496,Fa,21,Final,,Problem 7,Problem 7.4,"A particular penguin had a predicted body mass of 6800 grams. What is
that penguin’s bill length (in mm)? Give your answer as a number without
any units, rounded to three decimal places.","77.766
In this question, we want to compute x value given y value y = mx\ +\ b y -
b = mx \frac{y - b}{m} = x\ \ \text{(m
is nonzero)} x = \frac{y - b}{m} =
\frac{6800 - 812}{77} = \frac{5988}{77} \approx 77.766",88.0,Easy
497,Fa,21,Final,,Problem 7,Problem 7.5,"Below is the residual plot for our regression line.

Which of the following is a valid conclusion that we can draw solely
from the residual plot above?

 For this dataset, there is another line with a lower root mean
squared error
 The root mean squared error of the regression line is 0
 The accuracy of the regression line’s predictions depends on bill
length
 The relationship between bill length and body mass is likely
non-linear
 None of the above","The accuracy of the regression line’s
predictions depends on bill length
The vertical spread in this residual plot is uneven, which implies
that the regression line’s predictions aren’t equally accurate for all
inputs. This doesn’t necessarily mean that fitting a nonlinear curve
would be better. It just impacts how we interpret the regression line’s
predictions.",40.0,Hard
498,Fa,21,Final,,Problem 8,Problem 8,"Each individual penguin in our dataset is of a certain species
(Adelie, Chinstrap, or Gentoo) and comes from a particular island in
Antarctica (Biscoe, Dream, or Torgerson). There are 330 penguins in our
dataset, grouped by species and island as shown below.


Suppose we pick one of these 330 penguins, uniformly at random, and
name it Chester.",,,
499,Fa,21,Final,,Problem 8,Problem 8.1,"What is the probability that Chester comes from Dream island? Give
your answer as a number between 0 and 1, rounded to three decimal
places.","0.373
P(Chester comes from Dream island) = # of penguins in dream island
/ # of all penguins in the data = \frac{55+68}{330} \approx 0.373",94.0,Easy
500,Fa,21,Final,,Problem 8,Problem 8.2,"If we know that Chester comes from Dream island, what is the
probability that Chester is an Adelie penguin? Give your answer as a
number between 0 and 1, rounded to three decimal places.","0.447
P(Chester is an Adelie penguin given that Chester comes from Dream
island) = # of Adelie penguins from Dream island / # of penguins from Dream island = \frac{55}{55+68} \approx 0.447",91.0,Easy
501,Fa,21,Final,,Problem 8,Problem 8.3,"If we know that Chester is not from Dream island, what is the
probability that Chester is not an Adelie penguin? Give your answer as a
number between 0 and 1, rounded to three decimal places.","0.575
Method 1
P(Chester is not an Adelie penguin given that Chester is not from
Dream island) = # of penguins that are not Adelie penguins from islands
other than Dream island / # of penguins
in island other than Dream island = \frac{119\
\text{(eliminate all penguins that are Adelie or from Dream island, only
Gentoo penguins from Biscoe are left)}}{44+44+119} \approx
0.575
Method 2
P(Chester is not an Adelie penguin given that Chester is not from
Dream island) = 1- (# of penguins that are Adelie penguins from islands
other than Dream island / # of penguins
in island other than Dream island) = 1-\frac{44+44}{44+44+119} \approx 0.575",85.0,Easy
502,Fa,21,Final,,Problem 9,Problem 9,"We’re now interested in investigating the differences between the
masses of Adelie penguins and Chinstrap penguins. Specifically, our null
hypothesis is that their masses are drawn from the same population
distribution, and any observed differences are due to chance only.
Below, we have a snippet of working code for this hypothesis test,
for a specific test statistic. Assume that adelie_chinstrap
is a DataFrame of only Adelie and Chinstrap penguins, with just two
columns – 'species' and 'mass'.
stats = np.array([])
num_reps = 500
for i in np.arange(num_reps):
    # --- line (a) starts ---
    shuffled = np.random.permutation(adelie_chinstrap.get('species'))
    # --- line (a) ends ---
    
    # --- line (b) starts ---
    with_shuffled = adelie_chinstrap.assign(species=shuffled)
    # --- line (b) ends ---

    grouped = with_shuffled.groupby('species').mean()

    # --- line (c) starts ---
    stat = grouped.get('mass').iloc[0] - grouped.get('mass').iloc[1]
    # --- line (c) ends ---

    stats = np.append(stats, stat)",,,
503,Fa,21,Final,,Problem 9,Problem 9.1,"Which of the following statements best describe the procedure
above?

 This is a standard hypothesis test, and our test statistic is the
total variation distance between the distribution of Adelie masses and
Chinstrap masses
 This is a standard hypothesis test, and our test statistic is the
difference between the expected proportion of Adelie penguins and the
proportion of Adelie penguins in our resample
 This is a permutation test, and our test statistic is the total
variation distance between the distribution of Adelie masses and
Chinstrap masses
 This is a permutation test, and our test statistic is the difference
in the mean Adelie mass and mean Chinstrap mass","This is a permutation test, and our test
statistic is the difference in the mean Adelie mass and mean Chinstrap
mass (Option 4)
Recall, a permutation test helps us decide whether two random samples
come from the same distribution. This test matches our goal of testing
whether the masses of Adelie penguins and Chinstrap penguins are drawn
from the same population distribution. The code above are also doing
steps of a permutation test. In part (a), it shuffles
'species' and stores the shuffled series to
shuffled. In part (b), it assign the shuffled series of
values to 'species' column. Then, it uses
grouped = with_shuffled.groupby('species').mean() to
calculate the mean of each species. In part (c), it computes the
difference between mean mass of the two species by first getting the
'mass' column and then accessing mean mass of each group
(Adelie and Chinstrap) with positional index 0 and
1.",98.0,Easy
504,Fa,21,Final,,Problem 9,Problem 9.2,"Currently, line (c) (marked with a comment) uses .iloc. Which of the
following options compute the exact same statistic as line (c) currently
does?
Option 1:
stat = grouped.get('mass').loc['Adelie'] - grouped.get('mass').loc['Chinstrap']
Option 2:
stat = grouped.get('mass').loc['Chinstrap'] - grouped.get('mass').loc['Adelie']

 Option 1 only
 Option 2 only
 Both options
 Neither option","Option 1 only
We use df.get(column_name).iloc[positional_index] to
access the value in a column with positional_index.
Similarly, we use df.get(column_name).loc[index] to access
value in a column with its index. Remember
grouped is a DataFrame that
groupby('species'), so we have species name
'Adelie' and 'Chinstrap' as index for
grouped.
Option 2 is incorrect since it does subtraction in the reverse order
which results in a different stat compared to
line(c). Its output will be -1
\cdot stat. Recall, in
grouped = with_shuffled.groupby('species').mean(), we use
groupby() and since 'species' is a column with
string values, our index will be sorted in alphabetical order. So,
.iloc[0] is 'Adelie' and .iloc[1]
is 'Chinstrap'.",81.0,Easy
505,Fa,21,Final,,Problem 9,Problem 9.3,"Is it possible to re-write line (c) in a way that uses
.iloc[0] twice, without any other uses of .loc
or .iloc?

 Yes, it’s possible
 No, it’s not possible","Yes, it’s possible
There are multiple ways to achieve this. For instance
stat = grouped.get('mass').iloc[0] - grouped.sort_index(ascending = False).get('mass').iloc[0].",64.0,Medium
506,Fa,21,Final,,Problem 9,Problem 9.4,"What would happen if we removed line (a), and replaced
line (b) with
with_shuffled = adelie_chinstrap.sample(adelie_chinstrap.shape[0], replace=False)
Select the best answer.

 This would still run a valid hypothesis test
 This would not run a valid hypothesis test, as all values in the
stats array would be exactly the same
 This would not run a valid hypothesis test, even though there would
be several different values in the stats array
 This would not run a valid hypothesis test, as it would incorporate
information about Gentoo penguins","This would not run a valid hypothesis test,
as all values in the stats array would be exactly the same
(Option 2)
Recall, DataFrame.sample(n, replace = False) (or
DataFrame.sample(n) since replace = False is
by default) returns a DataFrame by randomly sampling n rows
from the DataFrame, without replacement. Since our n is
adelie_chinstrap.shape[0], and we are sampling without
replacement, we will get the exactly same Dataframe (though the order of
rows may be different but the stats array would be exactly
the same).",87.0,Easy
507,Fa,21,Final,,Problem 9,Problem 9.5,"What would happen if we removed line (a), and replaced
line (b) with
with_shuffled = adelie_chinstrap.sample(adelie_chinstrap.shape[0], replace=True)
Select the best answer.

 This would still run a valid hypothesis test
 This would not run a valid hypothesis test, as all values in the
stats array would be exactly the same
 This would not run a valid hypothesis test, even though there would
be several different values in the stats array
 This would not run a valid hypothesis test, as it would incorporate
information about Gentoo penguins","This would not run a valid hypothesis test,
even though there would be several different values in the
stats array (Option 3)
Recall, DataFrame.sample(n, replace = True) returns a
new DataFrame by randomly sampling n rows from the
DataFrame, with replacement. Since we are sampling with replacement, we
will have a DataFrame which produces a stats array with
some different values. However, recall, the key idea behind a
permutation test is to shuffle the group labels. So, the above code does
not meet this key requirement since we only want to shuffle the
""species"" column without changing the size of the two
species. However, the code may change the size of the two species.",66.0,Medium
508,Fa,21,Final,,Problem 9,Problem 9.6,"What would happen if we replaced line (a) with
with_shuffled = adelie_chinstrap.assign(
    species=np.random.permutation(adelie_chinstrap.get('species')
)
and replaced line (b) with
with_shuffled = with_shuffled.assign(
    mass=np.random.permutation(adelie_chinstrap.get('mass')
)
Select the best answer.

 This would still run a valid hypothesis test
 This would not run a valid hypothesis test, as all values in the
stats array would be exactly the same
 This would not run a valid hypothesis test, even though there would
be several different values in the stats array
 This would not run a valid hypothesis test, as it would incorporate
information about Gentoo penguins","This would still run a valid hypothesis test
(Option 1)
Our goal for the permutation test is to randomly assign birth weights
to groups, without changing group sizes. The above code shuffles
'species' and 'mass' columns and assigns them
back to the DataFrame. This fulfills our goal.",81.0,Easy
509,Fa,21,Final,,Problem 9,Problem 9.7,"Suppose we run the code for the hypothesis test and see the following
empirical distribution for the test statistic. In red is the observed
statistic.


Suppose our alternative hypothesis is that Chinstrap penguins weigh
more on average than Adelie penguins. Which of the following is closest
to the p-value for our hypothesis test?

 0
 \frac{1}{4}
 \frac{1}{3}
 \frac{2}{3}
 \frac{3}{4}
 1","\frac{1}{3}
Recall, the p-value is the chance, under the null hypothesis, that
the test statistic is equal to the value that was observed in the data
or is even further in the direction of the alternative. Thus, we compute
the proportion of the test statistic that is equal or less than the
observed statistic. (It is less than because less than corresponds to
the alternative hypothesis “Chinstrap penguins weigh more on average
than Adelie penguins”. Recall, when computing the statistic, we use
Adelie’s mean mass minus Chinstrap’s mean mass. If Chinstrap’s mean mass
is larger, the statistic will be negative, the direction of less than
the observed statistic).
Thus, we look at the proportion of area less than or on the red line
(which represents observed statistic), it is around \frac{1}{3}.",80.0,Easy
510,Fa,21,Final,,Problem 10,Problem 10,"At the San Diego Model Railroad Museum, there are different admission
prices for children, adults, and seniors. Over a period of time, as
tickets are sold, employees keep track of how many of each type of
ticket are sold. These ticket counts (in the order child, adult, senior)
are stored as follows.
admissions_data = np.array([550, 1550, 400])",,,
511,Fa,21,Final,,Problem 10,Problem 10.1,"Complete the code below so that it creates an array
admissions_proportions with the proportions of tickets sold
to each group (in the order child, adult, senior).
def as_proportion(data):
    return __(a)__

admissions_proportions = as_proportion(admissions_data)
What goes in blank (a)?","data/data.sum()
To calculate proportion for each group, we divide each value in the
array (tickets sold to each group) by the sum of all values (total
tickets sold). Remember values in an array can be processed as a
whole.",95.0,Easy
512,Fa,21,Final,,Problem 10,Problem 10.2,"The museum employees have a model in mind for the proportions in
which they sell tickets to children, adults, and seniors. This model is
stored as follows.
model = np.array([0.25, 0.6, 0.15])
We want to conduct a hypothesis test to determine whether the
admissions data we have is consistent with this model. Which of the
following is the null hypothesis for this test?

 Child, adult, and senior tickets might plausibly be purchased in
proportions 0.25, 0.6, and 0.15.
 Child, adult, and senior tickets are purchased in proportions 0.25,
0.6, and 0.15.
 Child, adult, and senior tickets might plausibly be purchased in
proportions other than 0.25, 0.6, and 0.15.
 Child, adult, and senior tickets, are purchased in proportions other
than 0.25, 0.6, and 0.15.","Child, adult, and senior tickets are
purchased in proportions 0.25, 0.6, and 0.15. (Option 2)
Recall, null hypothesis is the hypothesis that there is no
significant difference between specified populations, any observed
difference being due to sampling or experimental error. So, we assume
the distribution is the same as the model.",88.0,Easy
513,Fa,21,Final,,Problem 10,Problem 10.3,"Which of the following test statistics could we use to test our
hypotheses? Select all that could work.

 sum of differences in proportions
 sum of squared differences in proportions
 mean of differences in proportions
 mean of squared differences in proportions
 none of the above","sum of squared differences in proportions,
mean of squared differences in proportions (Option 2, 4)
We need to use squared difference to avoid the case that large
positive and negative difference cancel out in the process of
calculating sum or mean, resulting in small sum of difference or mean of
difference that does not reflect the actual deviation. So, we eliminate
Option 1 and 3.",77.0,Easy
514,Fa,21,Final,,Problem 10,Problem 10.4,"Below, we’ll perform the hypothesis test with a different test
statistic, the mean of the absolute differences in proportions.
Recall that the ticket counts we observed for children, adults, and
seniors are stored in the array
admissions_data  = np.array([550, 1550, 400]), and that our
model is model = np.array([0.25, 0.6, 0.15]).
For our hypothesis test to determine whether the admissions data is
consistent with our model, what is the observed value of the test
statistic? Give your answer as a number between 0 and 1, rounded to
three decimal places. (Suppose that the value you calculated is assigned
to the variable observed_stat, which you will use in later
questions.)","0.02
We first calculate the proportion for each value in
admissions_data \frac{550}{550+1550+400} = 0.22 \frac{1550}{550+1550+400} = 0.62 \frac{400}{550+1550+400} = 0.16 So, we have
the distribution of the admissions_data
Then, we calculate the observed value of the test statistic (the mean
of the absolute differences in proportions) \frac{|0.22-0.25|+|0.62-0.6|+|0.16-0.15|}{number\
of\ goups} =\frac{0.03+0.02+0.01}{3} =
0.02",82.0,Easy
515,Fa,21,Final,,Problem 10,Problem 10.5,"Now, we want to simulate the test statistic 10,000 times under the
assumptions of the null hypothesis. Fill in the blanks below to complete
this simulation and calculate the p-value for our hypothesis test.
Assume that the variables admissions_data,
admissions_proportions, model, and
observed_stat are already defined as specified earlier in
the question.
simulated_stats = np.array([]) 
for i in np.arange(10000):
    simulated_proportions = as_proportions(np.random.multinomial(__(a)__, __(b)__))
    simulated_stat = __(c)__
    simulated_stats = np.append(simulated_stats, simulated_stat)

p_value = __(d)__
What goes in blank (a)? What goes in blank (b)? What goes in blank
(c)? What goes in blank (d)?","(a) admissions_data.sum() (b)
model (c)
np.abs(simulated_proportions - model).mean() (d)
np.count_nonzero(simulated_stats >= observed_stat) / 10000
Recall, in np.random.multinomial(n, [p_1, ..., p_k]),
n is the number of experiments, and
[p_1, ..., p_k] is a sequence of probability. The method
returns an array of length k in which each element contains the number
of occurrences of an event, where the probability of the ith event is
p_i.
We want our simulated_proportion to have the same data
size as admissions_data, so we use
admissions_data.sum() in (a).
Since our null hypothesis is based on model, we simulate
based on distribution in model, so we have
model in (b).
In (c), we compute the mean of the absolute differences in
proportions. np.abs(simulated_proportions - model) gives us
a series of absolute differences, and .mean() computes the
mean of the absolute differences.
In (d), we calculate the p_value. Recall, the
p_value is the chance, under the null hypothesis, that the
test statistic is equal to the value that was observed in the data or is
even further in the direction of the alternative.
np.count_nonzero(simulated_stats >= observed_stat) gives
us the number of simulated_stats greater than or equal to
the observed_stat in the 10000 times simulations, so we
need to divide it by 10000 to compute the proportion of
simulated_stats greater than or equal to the
observed_stat, and this gives us the
p_value.",79.0,Easy
516,Fa,21,Final,,Problem 10,Problem 10.6,"True or False: the p-value represents the probability that the null
hypothesis is true.

 True
 False","False
Recall, the p-value is the chance, under the null hypothesis, that
the test statistic is equal to the value that was observed in the data
or is even further in the direction of the alternative. It only gives us
the strength of evidence in favor of the null hypothesis, which is
different from “the probability that the null hypothesis is true”.",64.0,Medium
517,Fa,21,Final,,Problem 10,Problem 10.7,"The new statistic that we used for this hypothesis test, the mean of
the absolute differences in proportions, is in fact closely related to
the total variation distance. Given two arrays of length three,
array_1 and array_2, suppose we compute the
mean of the absolute differences in proportions between
array_1 and array_2 and store the result as
madp. What value would we have to multiply
madp by to obtain the total variation distance
array_1 and array_2? Give your answer as a
number rounded to three decimal places.","1.5
Recall, the total variation distance (TVD) is the sum of the absolute
differences in proportions, divided by 2. When we compute the mean of
the absolute differences in proportions, we are computing the sum of the
absolute differences in proportions, divided by the number of groups
(which is 3). Thus, to get TVD, we first multiply our current statistics
(the mean of the absolute differences in proportions) by 3, we get the
sum of the absolute differences in proportions. Then according to the
definition of TVD, we divide this value by 2. Thus, we have \text{current statistics}\cdot 3 / 2 = \text{current
statistics}\cdot 1.5.",65.0,Medium
518,Fa,22,Final,,Problem 1,Problem 1,"In this question, we’ll explore the number of dependents of each
applicant. To begin, let’s define the variable dep_counts as
follows.
dep_counts = apps.groupby(""dependents"").count().get([""status""])
The visualization below shows the distribution of the numbers of
dependents per applicant. Note that every applicant has 6 or fewer
dependents.


Use dep_counts and the visualization above to answer the
following questions.",,,
519,Fa,22,Final,,Problem 1,Problem 1.1,"What is the type of the variable dep_counts?

 array
 Series
 DataFrame","DataFrame
As usual, .groupby produces a new DataFrame. Then we use
.get on this DataFrame with a list as the input, which
produces a DataFrame with just one column. Remember that
.get(""status"") produces a Series, but
.get([""status""]) produces a DataFrame",78.0,Easy
520,Fa,22,Final,,Problem 1,Problem 1.2,"What type of data visualization is shown above?

 line plot
 scatter plot
 bar chart
 histogram","histogram
This is a histogram because the number of dependents per applicant is
a numerical variable. It makes sense, for example, to subtract the
number of dependents for two applicants to see how many more dependents
one applicant has than the other. Histograms show distributions of
numerical variables.",91.0,Easy
521,Fa,22,Final,,Problem 1,Problem 1.3,"How many of the 1,000 applicants in apps have 2 or more
dependents? Give your answer as an integer.","400
The bars of a density histogram have a combined total area of 1, and
the area in any bar represents the proportion of values that fall in
that bin.
In tihs problem, we want the total area of the bins corresponding to
2 or more dependents. Since this involves 5 bins, whose exact heights
are unclear, we will instead calculate the proportion of all applicants
with 0 or 1 dependents, and then subtract this proportion from 1.
Since the width of each bin is 1, we have for each bin, 
\begin{align*}
\text{Area} &= \text{Height} \cdot \text{Width}\\
\text{Area} &= \text{Height}.
\end{align*}
Since the height of the first bar is 0.4, this means a proportion of
0.4 applicants have 0 dependents. Similarly, since the height of the
second bar is 0.2, a proportion of 0.2 applicants have 1 dependent. This
means 1-(0.4+0.2) = 0.4 proportion of
applicants have 2 or more dependents. Since there are 1,000 applicants
total, this is 400 applicants.",82.0,Easy
522,Fa,22,Final,,Problem 1,Problem 1.4,"Define the DataFrame dependents_status as follows.
dependents_status = apps.groupby([""dependents"", ""status""]).count()
What is the maximum number of rows that
dependents_status could have? Give your answer as an
integer.","14
When we group by multiple columns, the resulting DataFrame has one
row for each combination of values in those columns. Since there are 7
possible values for ""dependents"" (0, 1, 2, 3, 4, 5, 6) and
2 possible values for ""status"" (""approved"",
""denied""), this means there are 7\cdot 2 = 14 possible combinations of values
for these two columns.",59.0,Medium
523,Fa,22,Final,,Problem 1,Problem 1.5,"Recall that dep_counts is defined as follows.
dep_counts = apps.groupby(""dependents"").count().get([""status""])
Below, we define several more variables.
variable1 = dep_counts[dep_counts.get(""status"") >= 2].sum()

variable2 = dep_counts[dep_counts.index > 2].get(""status"").sum() 

variable3 = (dep_counts.get(""status"").sum() 
            - dep_counts[dep_counts.index < 2].get(""status"").sum())
            
variable4 = dep_counts.take(np.arange(2, 7)).get(""status"").sum()

variable5 = (dep_counts.get(""status"").sum() 
            - dep_counts.get(""status"").loc[1] 
            - dep_counts.get(""status"").loc[2])
Which of these variables are equal to your answer from part (c)?
Select all that apply.

 variable1
 variable2
 variable3
 variable4
 variable5
 None of the above.","variable3,
variable4
First, the DataFrame dep_counts is indexed by
""dependents"" and has just one column, called
""status"" containing the number of applicants with each
number of dependents. For example, dep_counts may look like
the DataFrame shown below.


variable1 does not work because it doesn’t make sense to
query with the condition dep_counts.get(""status"") >= 2.
In the example dep_counts shown above, all rows would
satisfy this condition, but not all rows correspond to applicants with 2
or more dependents. We should be querying based on the values in the
index instead.
variable2 is close but it uses a strict inequality
> where it should use >= because we want
to include applicants with 2 dependents.
variable3 is correct. It uses the same approach we used
in part (c). That is, in order to calculate the number of applicants
with 2 or more dependents, we calculate the total number of applicants
minus the number of applicants with less than 2 dependents.
variable4 works as well. The strategy here is to keep
only the rows that correspond to 2 or more dependents. Recall that
np.arange(2, 7) evaluates to the array
np.array([2, 3, 4, 5, 6]). Since we are told that each
applicant has 6 or fewer dependents, keeping only these rows
correspondings to keeping all applicants with 2 or more dependents.
variable5 does not work because it subtracts away the
applicants with 1 or 2 dependents, leaving the applicants with 0, 3, 4,
5, or 6 dependents. This is not what we want.",77.0,Easy
524,Fa,22,Final,,Problem 1,Problem 1.6,"Next, we define variables x and y as
follows.
x = dep_counts.index.values
y = dep_counts.get(""status"")
Note: If idx is the index of a Series or
DataFrame, idx.values gives the values in idx
as an array.
Which of the following expressions evaluate to the mean number of
dependents? Select all that apply.

 np.mean(x * y)
 x.sum() / y.sum()
 (x * y / y.sum()).sum()
 np.mean(x)
 (x * y).sum() / y.sum()
 None of the above.","(x * y / y.sum()).sum(),
(x * y).sum() / y.sum()
We know that x is
np.array([0, 1, 2, 3, 4, 5, 6]) and y is a
Series containing the number of applicants with each number of
dependents. We don’t know the exact values of the data in
y, but we do know there are 7 elements that sum to 1000,
the first two of which are 400 and 200.
np.mean(x * y) does not work because x * y
has 7 elements, so np.mean(x * y) is equivalent to
sum(x * y) / 7, but the mean number of dependents should be
sum(x * y) / 1000 since there are 1000 applicants.
x.sum() / y.sum() evaluates to \frac{21}{1000} regardless of how many
applicants have each number of dependents, so it must be incorrect.
(x * y / y.sum()).sum() works. We can think of
y / y.sum() as a Series containing the proportion of
applicants with each number of dependents. For example, the first two
entries of y / y.sum() are 0.4 and 0.2. When we multiply
this Series by x and sum up all 7 entries, the result is a
weighted average of the different number of dependents, where the
weights are given by the proportion of applicants with each number of
dependents.
np.mean(x) evaluates to 3 regardless of how many
applicants have each number of dependents, so it must be incorrect.
(x * y).sum() / y.sum() works because the numerator
(x * y).sum() represents the total number of dependents
across all 1,000 applicants and the denominator is the number of
applicants, or 1,000. The total divided by the count gives the mean
number of dependents.",71.0,Medium
525,Fa,22,Final,,Problem 1,Problem 1.7,"What does the expression y.iloc[0] / y.sum() evaluate
to? Give your answer as a fully simplified
fraction.","0.4
y.iloc[0] represents the number of applicants with 0
dependents, which is 400. y.sum() represents the total
number of applicants, which is 1,000. So the ratio of these is 0.4.",73.0,Medium
526,Fa,22,Final,,Problem 2,Problem 2,"For each application in apps, we want to assign an age
category based on the value in the ""age"" column, according
to the table below.",,,
527,Fa,22,Final,,Problem 2,Problem 2.1,"Which of the following is a correct way to fill in blanks (a) and
(b)?","Option 4
The line one_age = min(75, one_age) either leaves
one_age alone or sets it equal to 75 if the age was higher
than 75, which means anyone over age 75 is considered to be 75 years old
for the purposes of classifying them into age categories. From the
return statement, we know we need our value for bin_pos to
be either 0, 1 ,2 or 3 since cat_names has a length of 4.
When we divide one_age by 25, we get a decimal number that
represents how many times 25 fits into one_age. We want to
round this number down to get the number of whole copies of 25
that fit into one_age. If that value is 0, it means the
person is a ""young adult"", if that value is 1, it means
they are ""middle aged"", and so on. The rounding down
behavior that we want is accomplished by
int(one_age/25).",76.0,Easy
528,Fa,22,Final,,Problem 2,Problem 2.2,"Which of the following is a correct way to fill in blank (c)?

 age to bin(apps.get(""age""))
 apps.get(""age"").apply(age to bin)
 apps.get(""age"").age to bin()
 apps.get(""age"").apply(age to bin(one age))","apps.get(""age"").apply(age to bin)
We want our result to be a Series because the next line in the code
assigns it to a DataFrame. We also need to use the .apply()
method to apply our function to the entirety of the ""age""
column. The .apply() method only takes in the name of a
function and not its variables, as it treats the entries of the column
as the variables directly.",96.0,Easy
529,Fa,22,Final,,Problem 2,Problem 2.3,"Which of the following is a correct alternate implementation of the
age to bin function? Select all that apply.
Option 1:
def age_to_bin(one_age):
    bin_pos = 3
    if one_age < 25:
        bin_pos = 0
    if one_age < 50:
        bin_pos = 1
    if one_age < 75:
        bin_pos = 2
    return cat_names[bin_pos]
Option 2:
def age_to_bin(one_age):
    bin_pos = 3
    if one_age < 75:
        bin_pos = 2
    if one_age < 50:
        bin_pos = 1
    if one_age < 25:
        bin_pos = 0
    return cat_names[bin_pos]
Option 3:
def age_to_bin(one_age):
    bin_pos = 0
    for cutoff in np.arange(25, 100, 25):
        if one_age >= cutoff:
            bin_pos = bin_pos + 1 
    return cat_names[bin_pos]
Option 4:
def age_to_bin(one_age):
    bin_pos = -1
    for cutoff in np.arange(0, 100, 25):
        if one_age >= cutoff:
            bin_pos = bin_pos + 1 
        return cat_names[bin_pos]

 Option 1
 Option 2
 Option 3
 Option 4
 None of the above.","Option 2 and Option 3
Option 1 doesn’t work for inputs less than 25. For example, on an
input of 10, every condition is satsified, so bin_pos will
be set to 0, then 1, then 2, meaning the function will return
""older adult"" instead of ""young adult"".
Option 2 reverses the order of the conditions, which ensures that
even when a number satisfies many conditions, the last one it satisfies
determines the correct bin_pos. For example, 27 would
satisfy the first 2 conditions but not the last one, and the function
would return ""middle aged"" as expected.
In option 3, np.arange(25, 100, 25) produces
np.array([25,50,75]). The if condition checks
the whether the age is at least 25, then 50, then 75. For every time
that it is, it adds to bin_pos, otherwise it keeps
bin_pos. At the end, bin_pos represents the
number of these values that the age is greater than or equal to, which
correctly determines the age category.
Option 4 is equivalent to option 3 except for two things. First,
bin_pos starts at -1, but since 0 is included in the set of
cutoff values, the first time through the loop will set
bin_pos to 0, as in Option 3. This change doesn’t affect
the behavior of the funtion. The other change, however, is that the
return statement is inside the for-loop, which
does change the behavior of the function dramatically. Now the
for-loop will only run once, checking whether the age is at
least 0 and then returning immediately. Since ages are always at least
0, this function will return ""young adult"" on every input,
which is clearly incorrect.",62.0,Medium
530,Fa,22,Final,,Problem 2,Problem 2.4,"We want to determine the number of ""middle aged""
applicants whose applications were denied. Fill in the blank below so
that count evaluates to that number.
df = apps_cat.________.reset_index()
count = df[(df.get(""age_category"") == ""middle aged"") & 
           (df.get(""status"") == ""denied"")].get(""income"").iloc[0]
What goes in the blank?","groupby([""age_category"", ""status""]).count()
We can tell by the line in which count is defined that
df needs to have columns called
""age category"", ""status"", and
""income"" with one row such that the values in these columns
are ""middle aged"", ""denied"", and the number of
such applicants, respectively. Since there is one row corresponding to a
possible combination of values for ""age category"" and
""status"", this suggests we need to group by the pair of
columns, since .groupby produces a DataFrame with one row
for each possible combination of values in the columns we are grouping
by. Since we want to know how many individuals have this combination of
values for ""age category"" and ""status"", we
should use .count() as the aggregation method. Another clue
to to use .groupby is the presence of
.reset_index() which is needed to query based on columns
called ""age category"" and ""status"".",78.0,Easy
531,Fa,22,Final,,Problem 2,Problem 2.5,"The total variation distance between the distributions of
""age category"" for approved applications and denied
applications is 0.4.
One of the visualizations below shows the distributions of
""age category"" for approved applications and denied
applications. Which visualization is it?","Option 2
TVD represents the total overrepresentation of one distrubtion,
summed across all categories. To find the TVD visually, we can estimate
how much each bar for approved applications extends beyond the
corresponding bar for denied applications in each bar chart.
In Option 1, the approved bar extends beyond the denied bar only in
the ""young adult"" category, and by 0.2, so the TVD for
Option 1 is 0.2. In Option 2, the approved bar extends beyond the denied
bar only in the ""older adult"" category, and by 0.4, so the
TVD for Option 2 is 0.4. In Option 3, the approved bar extends beyond
the denied bar in ""elderly"" by 0.2 and in
""young adult"" by 0.4, for a TVD of 0.6. In Option 4, the
approved bar extends beyond the denied bar in
""young adult only"" by 0.2, for a TVD of 0.2.
Note that even without knowing the exact lengths of the bars in
Option 2, we can still conclude that Option 2 is correct by process of
elimination, since it’s the only one whose TVD appears close to 0.4",60.0,Medium
532,Fa,22,Final,,Problem 3,Problem 3,"In apps, our sample of 1,000 credit card applications,
500 of the applications come from homeowners and 500 come from people
who don’t own their own home. In this sample, homeowner ages have a mean
of 40 and standard deviation of 10. We want to use the bootstrap method
to compute a confidence interval for the mean age of a homeowner in the
population of all credit card applicants.",,,
533,Fa,22,Final,,Problem 3,Problem 3.1,"Note: This problem is out of scope; it
covers material no longer included in the course.
Suppose our computer is too slow to bootstrap 10,000 times, and
instead can only bootstrap 20 times. Here are the 20 resample means,
sorted in ascending order: 
\begin{align*}
    &37, 38, 39, 39, 40, 40, 40, 40, 41 , 41, \\
    &42, 42, 42, 42, 42, 42, 43, 43, 43 , 44
\end{align*}
 What are the left and right endpoints of a bootstrapped
80% confidence interval for the population mean? Use
the mathematical definition of percentile.","Left endpoint = 38, Right endpoint = 43
To find an 80% confidence interval, we need to find the 10th and 90th
percentiles of the resample means. Using the mathematical definiton of
percentile, the 10th percentile is at position 0.1*20 = 2 when we count starting with 1.
Since 38 is the second element of the sorted data, that is the left
endpoint of our confidence interval.",63.0,Medium
534,Fa,22,Final,,Problem 3,Problem 3.2,"Note: This problem is out of scope; it
covers material no longer included in the course.
True or False: Using the mathematical definition of percentile, the
50th percentile of the bootstrapped distribution above equals its
median.

 True
 False","False
The 50th percentile according to the mathematial definition is the
element at position 0.5*20 10 when we
count starting with 1. The 10th element is 41. However, the median of a
data set with 20 elements is halfway between the 10th and 11th values.
So the median in this case is 41.5.",79.0,Easy
535,Fa,22,Final,,Problem 3,Problem 3.3,"Consider the following three quantities:

pop_mean, the unknown mean age of homeowners in the
population of all credit card applicants.
sample_mean, the mean age of homeowners in our
sample of 500 applications in apps. We know this is
40.
resample_mean, the mean age of homeowners in one
particular resample of the applications in apps.

Which of the following statements about the relationship between
these three quantities are guaranteed to be true? Select all that
apply.

 If sample_mean is less than pop_mean, then
resample_mean is also less than pop_mean.
 The mean of sample_mean and resample_mean
is closer to pop_mean than either of the two values
individually.
 resample_mean is closer than sample_mean to
pop_mean.
 resample_mean is further than sample_mean
from pop_mean.
 None of the above.","None of the above.
Whenever we take a sample from a population, there is no guaranteed
relationship between the mean of the sample and the mean of the
population. Sometimes the mean of the sample comes out larger than the
population mean, sometimes smaller. We know this from the CLT which says
that the distribution of the sample mean is centered at the
population mean. Similarly, when we resample from an original mean, the
resample mean could be larger or smaller than the original sample’s
mean. The three quantities pop_mean,
sample_mean, and resample_mean can be in any
relative order. This means none of the statements listed here are
necessarily true.",37.0,Hard
536,Fa,22,Final,,Problem 4,Problem 4,"In apps, our sample of 1,000 credit card applications,
applicants who were approved for the credit card have fewer dependents,
on average, than applicants who were denied. The mean number of
dependents for approved applicants is 0.98, versus 1.07 for denied
applicants.
To test whether this difference is purely due to random chance, or
whether the distributions of the number of dependents for approved and
denied applicants are truly different in the population of all credit
card applications, we decide to perform a permutation test.
Consider the incomplete code block below.
def shuffle_status(df):
    shuffled_status = np.random.permutation(df.get(""status""))
    return df.assign(status=shuffled_status).get([""status"", ""dependents""])

def test_stat(df):
    grouped = df.groupby(""status"").mean().get(""dependents"")
    approved = grouped.loc[""approved""]
    denied = grouped.loc[""denied""]
    return __(a)__

stats = np.array([])
for i in np.arange(10000):
    shuffled_apps = shuffle_status(apps)
    stat = test_stat(shuffled_apps)
    stats = np.append(stats, stat)

p_value = np.count_nonzero(__(b)__) / 10000
Below are six options for filling in blanks (a) and (b) in the code
above.",,,
537,Fa,22,Final,,Problem 4,Problem 4.1,"Suppose we choose the following pair of hypotheses.

Null Hypothesis: In the population, the number
of dependents of approved and denied applicants come from the same
distribution.
Alternative Hypothesis: In the population, the
number of dependents of approved applicants and denied applicants do not
come from the same distribution.

Which of the six presented options could correctly fill in blanks (a)
and (b) for this pair of hypotheses? Select all that apply.

 Option 1
 Option 2
 Option 3
 Option 4
 Option 5
 Option 6
 None of the above.","Option 4, Option 6
For blank (a), we want to choose a test statistic that helps us
distinguish between the null and alternative hypotheses. The alternative
hypothesis says that denied and approved
should be different, but it doesn’t say which should be larger. Options
1 through 3 therefore won’t work, because high values and low values of
these statistics both point to the alternative hypothesis, and moderate
values point to the null hypothesis. Options 4 through 6 all work
because large values point to the alternative hypothesis, and small
values close to 0 suggest that the null hypothesis should be true.
For blank (b), we want to calculate the p-value in such a way that it
represents the proportion of trials for which the simulated test
statistic was equal to the observed statistic or further in the
direction of the alternative. For all of Options 4 through 6, large
values of the test statistic indicate the alternative, so we need to
calculate the p-value with a >= sign, as in Options 4
and 6.
While Option 3 filled in blank (a) correctly, it did not fill in
blank (b) correctly. Options 4 and 6 fill in both blanks correctly.",78.0,Easy
538,Fa,22,Final,,Problem 4,Problem 4.2,"Now, suppose we choose the following pair of hypotheses.

Null Hypothesis: In the population, the number
of dependents of approved and denied applicants come from the same
distribution.
Alternative Hypothesis: In the population, the
number of dependents of approved applicants is smaller on average than
the number of dependents of denied applicants.

Which of the six presented options could correctly fill in blanks (a)
and (b) for this pair of hypotheses? Select all that apply.","Option 1
As in the previous part, we need to fill blank (a) with a test
statistic such that large values point towards one of the hypotheses and
small values point towards the other. Here, the alterntive hypothesis
suggests that approved should be less than
denied, so we can’t use Options 4 through 6 because these
can only detect whether approved and denied
are not different, not which is larger. Any of Options 1 through 3
should work, however. For Options 1 and 2, large values point towards
the alternative, and for Option 3, small values point towards the
alternative. This means we need to calculate the p-value in blank (b)
with a >= symbol for the test statistic from Options 1
and 2, and a <= symbol for the test statistic from
Option 3. Only Options 1 fills in blank (b) correctly based on the test
statistic used in blank (a).",83.0,Easy
539,Fa,22,Final,,Problem 4,Problem 4.3,Option 6 from the start of this question is repeated below.,"np.abs(stats) >= np.abs(test_stat(apps))
First, we need to understand how Option 6 works. Option 6 produces
large values of the test statistic when approved is very
different from denied, then calculates the p-value as the
proportion of trials for which the simulated test statistic was larger
than the observed statistic. In other words, Option 6 calculates the
proportion of trials in which approved and
denied are more different in a pair of random samples than
they are in the original samples.
For Option 7, the test statistic for a pair of random samples may
come out very large or very small when approved is very
different from denied. Similarly, the observed statistic
may come out very large or very small when approved and
denied are very different in the original samples. We want
to find the proportion of trials in which approved and
denied are more different in a pair of random samples than
they are in the original samples, which means we want the proportion of
trials in which the absolute value of approved - denied in
a pair of random samples is larger than the absolute value of
approved - denied in the original samples.",56.0,Medium
540,Fa,22,Final,,Problem 4,Problem 4.4,"In our implementation of this permutation test, we followed the
procedure outlined in lecture to draw new pairs of samples under the
null hypothesis and compute test statistics — that is, we randomly
assigned each row to a group (approved or denied) by shuffling one of
the columns in apps, then computed the test statistic on
this random pair of samples.
Let’s now explore an alternative solution to drawing pairs of samples
under the null hypothesis and computing test statistics. Here’s the
approach:

Shuffle, i.e. re-order, the rows of the DataFrame.
Use the values at the top of the resulting ""dependents""
column as the new “denied” sample, and the values at the at the bottom
of the resulting ""dependents"" column as the new “approved”
sample. Note that we don’t necessarily split the DataFrame exactly in
half — the sizes of these new samples depend on the number of “denied”
and “approved” values in the original DataFrame!

Once we generate our pair of random samples in this way, we’ll
compute the test statistic on the random pair, as usual. Here, we’ll use
as our test statistic the difference between the mean number of
dependents for denied and approved applicants, in the order
denied minus approved.
Fill in the blanks to complete the simulation
below.
Hint: np.random.permutation shouldn’t appear
anywhere in your code.
    def shuffle_all(df):
        '''Returns a DataFrame with the same rows as df, but reordered.'''
        return __(a)__

    def fast_stat(df):
        # This function does not and should not contain any randomness.
        denied = np.count_nonzero(df.get(""status"") == ""denied"")
        mean_denied = __(b)__.get(""dependents"").mean()
        mean_approved = __(c)__.get(""dependents"").mean()
        return mean_denied - mean_approved

    stats = np.array([])
    for i in np.arange(10000):
        stat = fast_stat(shuffle_all(apps))
        stats = np.append(stats, stat)","The blanks should be filled in as
follows:


df.sample(df.shape[0])


df.take(np.arange(denied))


df.take(np.arange(denied, df.shape[0]))


For blank (a), we are told to return a DataFrame with the same rows
but in a different order. We can use the .sample method for
this question. We want each row of the input DataFrame df
to appear once, so we should sample without replacement, and we should
have has many rows in the output as in df, so our sample
should be of size df.shape[0]. Since sampling without
replacement is the default behavior of .sample, it is
optional to specify replace=False.",59.0,Medium
541,Fa,22,Final,,Problem 5,Problem 5,"Choose the best tool to answer each of the following questions. Note
the following:

By “hypothesis testing”, we mean “standard” hypothesis testing,
i.e. hypothesis testing that doesn’t involve
permutation testing or bootstrapping.
By “bootstrapping”, we mean bootstrapping that
doesn’t involve hypothesis testing.",,,
542,Fa,22,Final,,Problem 5,Problem 5.1,"Are incomes of applicants with 2 or fewer dependents drawn randomly
from the distribution of incomes of all applicants?

 Hypothesis Testing
 Permutation Testing
 Bootstrapping",,47.0,Hard
543,Fa,22,Final,,Problem 5,Problem 5.2,"What is the median income of credit card applicants with 2 or fewer
dependents?

 Hypothesis Testing
 Permutation Testing
 Bootstrapping",,88.0,Easy
544,Fa,22,Final,,Problem 5,Problem 5.3,"Are credit card applications approved through a random process in
which 50% of applications are approved?

 Hypothesis Testing
 Permutation Testing
 Bootstrapping",,74.0,Medium
545,Fa,22,Final,,Problem 5,Problem 5.4,"Is the median income of applicants with 2 or fewer dependents less
than the median income of applicants with 3 or more dependents?

 Hypothesis Testing
 Permutation Testing
 Bootstrapping",,57.0,Medium
546,Fa,22,Final,,Problem 5,Problem 5.5,"What is the difference in median income of applicants with 2 or fewer
dependents and applicants with 3 or more dependents?

 Hypothesis Testing
 Permutation Testing
 Bootstrapping",,63.0,Medium
547,Fa,22,Final,,Problem 6,Problem 6,"In this question, we’ll explore the relationship between the ages and
incomes of credit card applicants.",,,
548,Fa,22,Final,,Problem 6,Problem 6.1,"The credit card company that owns the data in apps, BruinCard, has
decided not to give us access to the entire apps DataFrame,
but instead just a sample of apps called
small apps. We’ll start by using the information in
small_apps to compute the regression line that predicts the
age of an applicant given their income.
For an applicant with an income that is \frac{8}{3} standard deviations above the
mean income, we predict their age to be \frac{4}{5} standard deviations above the
mean age. What is the correlation coefficient, r, between incomes and ages in
small_apps? Give your answer as a fully simplified
fraction.","r =
\frac{3}{10}
To find the correlation coefficient r we use the equation of the regression line
in standard units and solve for r as
follows. 
\begin{align*}
\text{predicted } y_{\text{(su)}} &= r \cdot x_{\text{(su)}} \\
\frac{4}{5} &= r \cdot \frac{8}{3} \\
r &= \frac{4}{5} \cdot \frac{3}{8} \\
r &= \frac{3}{10}
\end{align*}",52.0,Medium
549,Fa,22,Final,,Problem 6,Problem 6.2,"Now, we want to predict the income of an applicant given their age.
We will again use the information in small_apps to find the
regression line. The regression line predicts that an applicant whose
age is \frac{4}{5} standard deviations
above the mean age has an income that is s standard deviations above the mean income.
What is the value of s? Give your
answer as a fully simplified fraction.","s =
\frac{6}{25}
We again use the equation of the regression line in standard units,
with the value of r we found in the
previous part. 
\begin{align*}
\text{predicted } y_{\text{(su)}} &= r \cdot x_{\text{(su)}} \\
s &= \frac{3}{10} \cdot \frac{4}{5} \\
s &= \frac{6}{25}
\end{align*}

Notice that when we predict income based on age, our predictions are
different than when we predict age based on income. That is, the answer
to this question is not \frac{8}{3}. We can think of this phenomenon
as a consequence of regression to the mean which means that the
predicted variable is always closer to average than the original
variable. In part (a), we start with an income of \frac{8}{3} standard units and predict an age
of \frac{4}{5} standard units, which is
closer to average than \frac{8}{3}
standard units. Then in part (b), we start with an age of \frac{4}{5} and predict an income of \frac{6}{25} standard units, which is closer
to average than \frac{4}{5} standard
units. This happens because whenever we make a prediction, we multiply
by r which is less than one in
magnitude.",21.0,Hard
550,Fa,22,Final,,Problem 6,Problem 6.3,"BruinCard has now taken away our access to both apps and
small_apps, and has instead given us access to an even
smaller sample of apps called mini_apps. In
mini_apps, we know the following information: - All incomes
and ages are positive numbers. - There is a positive linear association
between incomes and ages.
We use the data in mini_apps to find the regression line
that will allow us to predict the income of an applicant given their
age. Just to test the limits of this regression line, we use it to
predict the income of an applicant who is -2 years old,
even though it doesn’t make sense for a person to have a negative
age.
Let I be the regression line’s
prediction of this applicant’s income. Which of the following
inequalities are guaranteed to be satisfied? Select all that apply.

 I < 0
 I < \text{mean income}
 | I - \text{mean income}| \leq | \text{mean
age} + 2 |
 \dfrac{| I - \text{mean
income}|}{\text{standard deviation of incomes}} \leq \dfrac{| \text{mean
age} + 2 |}{\text{standard deviation of ages}}
 None of the above.","I < \text{mean
income}, \dfrac{| I - \text{mean
income}|}{\text{standard deviation of incomes}} \leq \dfrac{| \text{mean
age} + 2 |}{\text{standard deviation of ages}}
To understand this answer, we will investigate each option.

I < 0:

This option asks whether income is guaranteed to be negative. This is
not necessarily true. For example, it’s possible that the slope of the
regression line is 2 and the intercept
is 10, in which case the income
associated with a -2 year old would be
6, which is positive.

I < \text{mean income}:

This option asks whether the predicted income is guaranteed to be
lower than the mean income. It helps to think in standard units. In
standard units, the regression line goes through the point (0, 0) and has slope r, which we are told is positive. This means
that for a below-average x, the
predicted y is also below average. So
this statement must be true.

| I - \text{mean income}| \leq |
\text{mean age} + 2 |:

First, notice that | \text{mean age} + 2 |
= | -2 - \text{mean age}|, which represents the horizontal
distance betweeen these two points on the regression line: (\text{mean age}, \text{mean income}), (-2, I). Likewise, | I - \text{mean income}| represents the
vertical distance between those same two points. So the inequality can
be interpreted as a question of whether the rise of the
regression line is less than or equal to the run, or whether
the slope is at most 1. That’s not guaranteed when we’re working in
original units, as we are here, so this option is not necessarily
true.

\dfrac{| I - \text{mean
income}|}{\text{standard deviation of incomes}} \leq \dfrac{| \text{mean
age} + 2 |}{\text{standard deviation of ages}}:

Since standard deviation cannot be negative, we have \dfrac{| I - \text{mean income}|}{\text{standard
deviation of incomes}} = \left| \dfrac{I - \text{mean
income}}{\text{standard deviation of incomes}} \right| =
I_{\text{(su)}}. Similarly, \dfrac{|\text{mean age} + 2|}{\text{standard
deviation of ages}} = \left| \dfrac{-2 - \text{mean age}}{\text{standard
deviation of ages}} \right| = -2_{\text{(su)}}. So this option is
asking about whether the predicted income, in standard units, is
guaranteed to be less (in absolute value) than the age. Since we make
predictions in standard units using the equation of the regression line
\text{predicted } y_{\text{(su)}} = r \cdot
x_{\text{(su)}} and we know |r|\leq
1, this means |\text{predicted }
y_{\text{(su)}}| \leq | x_{\text{(su)}}|. Applying this to ages
(x) and incomes (y), this says exactly what the given
inequality says. This is the phenomenon we call regression to the
mean.",69.0,Medium
551,Fa,22,Final,,Problem 6,Problem 6.4,"Yet again, BruinCard, the company that gave us access to
apps, small_apps, and mini_apps,
has revoked our access to those three DataFrames and instead has given
us micro_apps, an even smaller sample of
apps.
Using micro_apps, we are again interested in finding the
regression line that will allow us to predict the income of an applicant
given their age. We are given the following information:

The correlation coefficient, r,
between ages and incomes is -\frac{1}{3} (note the negative sign).
The mean income is \frac{7}{2}
(remember, incomes are measured in tens of thousands of dollars).
The mean age is 33.
The regression line predicts that a 24 year old applicant has an
income of \frac{31}{2}.

Suppose the standard deviation of incomes in micro_apps
is an integer multiple of the standard deviation of ages in
micro_apps. That is,
\text{standard deviation of income} = k
\cdot \text{standard deviation of age}.
What is the value of k? Give your
answer as an integer.","k = 4
To find this answer, we’ll use the definition of the regression line
in original units, which is \text{predicted }
y = mx+b, where m = r \cdot
\frac{\text{SD of } y}{\text{SD of }x}, \: \: b = \text{mean of } y - m
\cdot \text{mean of } x
Next we substitute these value for m
and b into \text{predicted } y = mx + b, interpret x as age and y as income, and use the given information to
find k. 
\begin{align*}
\text{predicted } y &= mx+b \\
\text{predicted } y &= r \cdot \frac{\text{SD of } y}{\text{SD of
}x} \cdot x+ \text{mean of } y - r \cdot \frac{\text{SD of } y}{\text{SD
of }x} \cdot \text{mean of } x\\
\text{predicted income}&= r \cdot \frac{\text{SD of
income}}{\text{SD of age}} \cdot \text{age}+ \text{mean income} - r
\cdot \frac{\text{SD of income}}{\text{SD of age}} \cdot \text{mean age}
\\
\frac{31}{2}&= -\frac{1}{3} \cdot k \cdot 24+ \frac{7}{2} +
\frac{1}{3} \cdot k \cdot 33 \\
\frac{31}{2}&= -8k+ \frac{7}{2} + 11k \\
\frac{31}{2}&= 3k+ \frac{7}{2}  \\
3k &= \frac{31}{2} - \frac{7}{2} \\
3k &= 12 \\
k &= 4
\end{align*}

Another way to solve this problem uses the equation of the regression
line in standard units and the definition of standard units.

\begin{align*}
\text{predicted } y_{\text{(su)}} &= r \cdot x_{\text{(su)}} \\
\frac{\text{predicted income} - \text{mean income}}{\text{SD of income}}
&= r \cdot \frac{\text{age} - \text{mean age}}{\text{SD of age}} \\
\frac{\frac{31}{2} - \frac{7}{2}}{k\cdot \text{SD of age}} &=
-\frac{1}{3} \cdot \frac{24 - 33}{\text{SD of age}} \\
\frac{12}{k\cdot \text{SD of age}} &= -\frac{1}{3} \cdot
\frac{-9}{\text{SD of age}} \\
\frac{12}{k\cdot \text{SD of age}} &= \frac{3}{\text{SD of age}} \\
\frac{k\cdot \text{SD of age}}{\text{SD of age}}  &= \frac{12}{3}\\
k &= 4
\end{align*}",45.0,Hard
552,Fa,22,Final,,Problem 7,Problem 7,"Below, we define a new DataFrame called
seven_apps and display it fully.
seven_apps = apps.sample(7).sort_values(by=""dependents"", ascending=False)
seven_apps


Consider the process of resampling 7 rows from
seven_apps with replacement, and computing the
maximum number of dependents in the resample.",,,
553,Fa,22,Final,,Problem 7,Problem 7.1,"If we take one resample, what is the probability that the maximum
number of dependents in the resample is less than 3?
Leave your answer unsimplified.","\left( 1 -
\frac{1}{7}\right)^7 = \left( \frac{6}{7}\right)^7
Of the 7 rows in the seven_apps DataFrame, there are 6
rows that have a value less than 3 in the dependents
column. This means that if we were to sample one row
from seven_apps, there would be a \frac{6}{7} chance of selecting one of the
rows that has less than 3 dependents. The question is asking what the
probability that the maximum number of dependents in the resample is
less than 3. One resample of the DataFrame is equivalent to sampling one
row from seven_apps 7 different times, without replacement.
So the probability of getting a row with less than 3 dependents, 7 times
consecutively, is \left(
\frac{6}{7}\right)^7.",47.0,Hard
554,Fa,22,Final,,Problem 7,Problem 7.2,"If we take 50 resamples, what is the probability that the maximum
number of dependents is never 3, in any resample? Leave
your answer unsimplified.","\left[ \left( 1 -
\frac{1}{7}\right)^7 \right]^{50} = \left(
\frac{6}{7}\right)^{350}
We know from the previous part of this question that the probability
of one resample of seven_apps having a maximum number of
dependents less than 3 is \left(
\frac{6}{7}\right)^7. Now we must repeat this process 50 times
independently, and so the probability that all 50 resamples have a
maximum number of dependents less than 3 is \left(\left( \frac{6}{7}\right)^{7}\right)^{50} =
\left( \frac{6}{7}\right)^{350}. Another way to interpret this is
that we must select 350 rows, one a time, such that none of them are the
one row containing 3 dependents.",41.0,Hard
555,Fa,22,Final,,Problem 7,Problem 7.3,"If we take 50 resamples, what is the probability that the maximum
number of dependents is 3 in every resample? Leave your
answer unsimplified.","\left[1 - \left( 1
- \frac{1}{7}\right)^7 \right]^{50} = \left[1 - \left(
\frac{6}{7}\right)^7 \right]^{50}
We’ll first take a look at the probability of one
resample of seven_apps having the maximum number
of dependents be 3. In order for this to happen, at least one row of the
7 selected for the resample must be the row containing 3 dependents. The
probability of getting this row at least once is equal to the complement
of the probability of never getting this row, which we calculated in
part (a) to be \left(
\frac{6}{7}\right)^7. Therefore, the probability that at least
one row in the resample has 3 dependents, is 1
-\left( \frac{6}{7}\right)^7.
Now that we know the probability of getting one resample where the
maximum number of dependents is 3, we can calculate the probability that
the same thing happens in 50 independent resamples by multiplying this
probability by itself 50 times. Therefore, the probability that the
maximum number of dependents is 3 in each of 50 resamples is \left[1 - \left( \frac{6}{7}\right)^7
\right]^{50}.",27.0,Hard
556,Fa,22,Final,,Problem 8,Problem 8,"At TritonCard, a new UCSD alumni-run credit card company,
applications are approved at random. Each time someone submits an
application, a TritonCard employee rolls a fair six-sided die two times.
If both rolls are the same even number — that is, if both are 2, both
are 4, or both are 6 — TritonCard approves the application. Otherwise,
they reject it.",,,
557,Fa,22,Final,,Problem 8,Problem 8.1,"You submit k identical TritonCard
applications. The probability that at least one of your applications is
approved is of the form 1-\left(\frac{a}{b}\right)^k. What are the
values of a and b? Give your answers as
integers such that the fraction \frac{a}{b} is fully
simplified.","a = 11, b =
12
The format of the answer suggests we should use the complement rule.
The opposite of at least one application being approved is that no
applications are approved, or equivalently, all applications are
denied.
Consider one application. Its probability of being approved is \frac{3}{6}*\frac{1}{6} = \frac{3}{36} =
\frac{1}{12} because we need to get any one of the three even
numbers on the first roll, then the second roll must match the first. So
one application has a probability of being denied equal to \frac{11}{12}.
Therefore, the probability that all k applications are denied is \left(\frac{11}{12}\right)^k. The probability
that this does not happen, or at least one is approved,
is given by 1-\left(\frac{11}{12}\right)^k.",41.0,Hard
558,Fa,22,Final,,Problem 8,Problem 8.2,"Every TritonCard credit card has a 3-digit security code on the back,
where each digit is a number 0 through 9. There are 1,000 possible
3-digit security codes: 000, 001, \dots, 998,
999.
Tony, the CEO of TritonCard, wants to only issue credit cards whose
security codes satisfy all of the following criteria:

The first digit is odd.
The middle digit is 0.
The last digit is even.

Tony doesn’t have a great way to generate security codes meeting
these three criteria, but he does know how to generate security codes
with three unique (distinct) digits. That is,
no number appears in the security code more than once. So, Tony decides
to randomly select a security code from among those with three unique
digits. If the randomly selected security code happens to meet the
desired criteria, TritonCard will issue a credit card with this security
code, otherwise Tony will try his process again.
What is the probability that Tony’s first randomly selected security
code satisfies the given criteria? Give your answer as a fully
simplified fraction.","\frac{1}{36}
Imagine generating a security code with three unique digits by
selecting one digit at a time. In other words, we would need to select
three values without replacement from the set of digits
0, 1, 2, \dots, 9. The probability that
the first digit is odd is \frac{5}{10}.
Then, assuming the first digit is odd, the probability of the middle
digit being 0 is \frac{1}{9} since only
nine digits are remaining, and one of them must be 0. Then, assuming we
have chosen an odd number for the first digit and 0 or the middle digit,
there are 8 remaining digits we could select, and only 4 of them are
even, so the probability of the third digit being even is \frac{4}{8}. Multiplying these all together
gives the probability that all three criteria are satisfied: \frac{5}{10} \cdot \frac{1}{9} \cdot \frac{4}{8} =
\frac{1}{36}",38.0,Hard
559,Fa,22,Final,,Problem 8,Problem 8.3,"Daphne, the newest employee at TritonCard, wants to try a different
way of generating security codes. She decides to randomly select a
3-digit code from among all 1,000 possible security
codes (i.e. the digits are not necessarily unique). As before, if the
code randomly selected code happens to meet the desired criteria,
TritonCard will issue a credit card with this security code, otherwise
Daphne will try her process again.
What is the probability that Daphne’s first randomly selected
security code satisfies the given criteria? Give your answer as a
fully simplified fraction.","\frac{1}{40}
We’ll use a similar strategy as in the previous part. This time,
however, we need to select three values with
replacement from the set of digits 0,
1, 2, \dots, 9. The probability that the first digit is odd is
\frac{5}{10}. Then, assuming the first
digit is odd, the probability of the middle digit being 0 is \frac{1}{10} since any of the ten digits can
be chosen, and one of them is 0. Then, assuming we have chosen an odd
number for the first digit and 0 or the middle digit, the probability of
getting an even number for the third digit is \frac{5}{10}, which actually does not depend
at all on what we selected for the other digits. In fact, when we sample
with replacement, the probabilities of each digit satisfying the given
criteria don’t depend on whether the other digits satisfied the given
criteria (in other words, they are independent). This is different from
the previous part, where knowledge of previous digits satisfying the
criteria informed the chances of the next digit satisfying the criteria.
So for this problem, we can really just think of each of the three
digits separately and multiply their probabilties of meeting the desired
criteria: \frac{5}{10} \cdot \frac{1}{10}
\cdot \frac{5}{10} = \frac{1}{40}",51.0,Medium
560,Fa,22,Final,,Problem 9,Problem 9,"After you graduate, you are hired by TritonCard! On your new work
computer, you install numpy, but something goes wrong with
the installation — your copy of numpy doesn’t come with
np.random.multinomial. To demonstrate your resourcefulness
to your new employer, you decide to implement your own version of
np.random.multinomial.
Below, complete the implementation of the function
manual_multinomial so that
manual_multinomial(n, p) works the same way as
np.random.multinomial(n, p). That is,
manual_multinomial should take in an integer n
and an array of probabilities p. It should return an array
containing the counts in each category when we randomly draw
n items from a categorical distribution where the
probabilities of drawing an item from each category are given in the
array p. The array returned by
manual_multinomial(n, p) should have a length of
len(p) and a sum of n.
For instance, to simulate flipping a coin five times, we could call
manual_multinomial(5, np.array([0.5, 0.5])), and the output
might look like array([2, 3]).
def manual_multinomial(n, p):
    values = np.arange(len(p))
    choices = np.random.choice(values, size=__(a)__, replace=__(b)__, p=p)
    value_counts = np.array([])
    for value in values:
        value_count = __(c)__
        value_counts = np.append(value_counts, value_count)
    return value_counts",,,
561,Fa,22,Final,,Problem 9,Problem 9.1,What goes in blank (a)?,"n
The size argument in np.random.choice provides the
number of samples we draw. In the manual_multinomial
function, we randomly draw n items, and so the size should
be n.",81.0,Easy
562,Fa,22,Final,,Problem 9,Problem 9.2,What goes in blank (b)?,"True
Here, we are using np.random.choice to simulate picking
n elements from values. We draw with
replacement since we are allowed to have repeated elements. For example,
if we were flipping a coin five times, we would need to have repeated
elements, since there are only two possible outcomes of a coin flip but
we are flipping the coin more than two times.",79.0,Easy
563,Fa,22,Final,,Problem 9,Problem 9.3,What goes in blank (c)?,"np.count_nonzero(choices == value)
The choices variable contains an array of the
n randomly drawn values selected from values.
In each iteration of the for-loop, we want to count the number of
elements in choices that are equal to the given
value. To do this, we can use
np.count_nonzero(choices == value). In the end,
value_counts is an array that says how many times we
selected 0, how many times we selected 1, and so on.",37.0,Hard
564,Fa,22,Final,,Problem 10,Problem 10,"The credit card company that owns the data in apps,
BruinCard, has decided not to give us access to the entire
apps DataFrame, but instead just a random sample of 100
rows of apps called hundred_apps.
We are interested in estimating the mean age of all applicants in
apps given only the data in hundred_apps. The
ages in hundred_apps have a mean of 35 and a standard
deviation of 10.",,,
565,Fa,22,Final,,Problem 10,Problem 10.1,"Give the endpoints of the CLT-based 95% confidence interval for the
mean age of all applicants in apps, based on the data in
hundred_apps.","Left endpoint = 33, Right endpoint = 37
According to the Central Limit Theorem, the standard deviation of the
distribution of the sample mean is \frac{\text{sample SD}}{\sqrt{\text{sample size}}} =
\frac{10}{\sqrt{100}} = 1. Then using the fact that the
distribution of the sample mean is roughly normal, since 95% of the area
of a normal curve falls within two standard deviations of the mean, we
can find the endpoints of the 95% CLT-based confidence interval as 35 - 2 = 33 and 35
+ 2 = 37.
We can think of this as using the formula below: 
\left[\text{sample mean} - 2\cdot \frac{\text{sample
SD}}{\sqrt{\text{sample size}}}, \: \text{sample mean} + 2\cdot
\frac{\text{sample SD}}{\sqrt{\text{sample size}}}
\right]. Plugging in the appropriate quantities yields [35 - 2\cdot\frac{10}{\sqrt{100}}, 35 -
2\cdot\frac{10}{\sqrt{100}}] = [33, 37].",67.0,Medium
566,Fa,22,Final,,Problem 10,Problem 10.2,"Which of the following three visualizations best depict the
distribution of sample_means?","Option 1
As we found in the previous part, the distribution of the sample mean
should have a standard deviation of 1. We also know it should be
centered at the mean of our sample, at 35, but since all the options are
centered here, that’s not too helpful. Only Option 1, however, has a
standard deviation of 1. Remember, we can approximate the standard
deviation of a normal curve as the distance between the mean and either
of the inflection points. Only Option 1 looks like it has inflection
points at 34 and 36, a distance of 1 from the mean of 35.
If you chose Option 2, you probably confused the standard deviation
of our original sample, 10, with the standard deviation of the
distribution of the sample mean, which comes from dividing that value by
the square root of the sample size.",57.0,Medium
567,Fa,22,Final,,Problem 10,Problem 10.3,"Which of the following statements are guaranteed to be true? Select
all that apply.

 We used bootstrapping to compute sample_means.
 The ages of credit card applicants are roughly normally
distributed.
 A CLT-based 90% confidence interval for the mean age of credit card
applicants, based on the data in hundred apps, would be narrower than
the interval you gave in part (a).
 The expression np.percentile(sample_means, 2.5)
evaluates to the left endpoint of the interval you gave in part (a).
 If we used the data in hundred_apps to create 1,000
CLT-based 95% confidence intervals for the mean age of applicants in
apps, approximately 950 of them would contain the true mean
age of applicants in apps.
 None of the above.","A CLT-based 90% confidence interval for the
mean age of credit card applicants, based on the data in
hundred_apps, would be narrower than the interval you gave
in part (a).
Let’s analyze each of the options:

Option 1: We are not using bootstrapping to compute sample means
since we are sampling from the apps DataFrame, which is our
population here. If we were bootstrapping, we’d need to sample from our
first sample, which is hundred_apps.
Option 2: We can’t be sure what the distribution of the ages of
credit card applicants are. The Central Limit Theorem says that the
distribution of sample_means is roughly normally
distributed, but we know nothing about the population
distribution.
Option 3: The CLT-based 95% confidence interval that we
calculated in part (a) was computed as follows: \left[\text{sample mean} - 2\cdot
\frac{\text{sample SD}}{\sqrt{\text{sample size}}},
\text{sample mean} + 2\cdot \frac{\text{sample SD}}{\sqrt{\text{sample
size}}}
\right] A CLT-based 90% confidence interval would be computed as
\left[\text{sample mean} - z\cdot
\frac{\text{sample SD}}{\sqrt{\text{sample size}}},
\text{sample mean} + z\cdot \frac{\text{sample SD}}{\sqrt{\text{sample
size}}}
\right] for some value of z less
than 2. We know that 95% of the area of a normal curve is within two
standard deviations of the mean, so to only pick up 90% of the area,
we’d have to go slightly less than 2 standard deviations away. This
means the 90% confidence interval will be narrower than the 95%
confidence interval.
Option 4: The left endpoint of the interval from part (a) was
calculated using the Central Limit Theorem, whereas using
np.percentile(sample_means, 2.5) is calculated empirically,
using the data in sample_means. Empirically calculating a
confidence interval doesn’t necessarily always give the exact same
endpoints as using the Central Limit Theorem, but it should give you
values close to those endpoints. These values are likely very similar
but they are not guaranteed to be the same. One way to see this is that
if we ran the code to generate sample_means again, we’d
probably get a different value for
np.percentile(sample_means, 2.5).
Option 5: The key observation is that if we used the data in
hundred_apps to create 1,000 CLT-based 95% confidence
intervals for the mean age of applicants in apps, all of
these intervals would be exactly the same. Given a sample, there is only
one CLT-based 95% confidence interval associated with it. In our case,
given the sample hundred_apps, the one and only CLT-based
95% confidence interval based on this sample is the one we found in part
(a). Therefore if we generated 1,000 of these intervals, either they
would all contain the parameter or none of them would. In order for a
statement like the one here to be true, we would need to collect 1,000
different samples, and calculate a confidence interval from each
one.",49.0,Hard
568,Fa,22,Final,,Problem 11,Problem 11,"Suppose variables v1, v2, v3,
and v4, have already been initialized to specific numerical
values. Right now, we don’t know what values they’ve been set to.
The function f shown below takes in a number,
v, and outputs an integer between -2 and 2, depending on
the value of v relative to v1,
v2, v3, and v4.
def f(v):
    if v <= v1:
        return -2
    elif v <= v2:
        return -1
    elif v <= v3:
        return 0
    elif v <= v4:
        return 1
    else:
        return 2
Recall that in the previous problem, we created an array called
sample_means containing 10,000 values, each of which is the
mean of a random sample of 100 applicant ages drawn from the DataFrame
apps, in which ages have a mean of 35 and a standard
deviation of 10.
When we call the function f on every value
v in sample_means, we produce a collection of
10,000 values all between -2 and 2. A density histogram of these values
is shown below.


The heights of the five bars in this histogram, reading from left to
right, are
x, 3x, 12x, 3x, x.",,,
569,Fa,22,Final,,Problem 11,Problem 11.1,"What is the value of x (i.e. the
height of the shortest bar in the histogram)? Give your answer as a
fully simplified fraction.","\frac{1}{20}
In any density histogram, the total area of all bars is 1. This
histogram has five bars, each of which has a width of 1 (e.g. 3 - 2 = 1). Since \text{Area} = \text{Height} \cdot
\text{Width}, we have that the area of each bar is equal to its
height. So, the total area of the histogram in this case is the sum of
the heights of each bar:
\text{Total Area} = x + 3x + 12x + 3x + x
= 20x
Since we know that the total area is equal to 1, we have
20x = 1 \implies \boxed{x =
\frac{1}{20}}",72.0,Medium
570,Fa,22,Final,,Problem 11,Problem 11.2,"What does the expression below evaluate to? Give your answer as an
integer.
np.count_nonzero((sample_means > v2) & (sample_means <= v4))
Hint: Don’t try to find the values of v2 and
v4 – you can answer this question without them!","7,500
First, it’s a good idea to understand what the integer we’re trying
to find actually means in the context of the information provided. In
this case, it’s the number of sample means that are greater than
v2 and less than or equal to v4. Here’s how to
arrive at that conclusion:

First, note that sample_means is an array of length
10,000.
sample_means > v2 and
sample_means <= v4 are both Boolean arrays of length
10,000.
(sample_means > v2) & (sample_means <= v4) is
also a Boolean array of length 10,000, which contains True
for every sample mean that is greater than v2 and less than
or equal to v4 and False for every other
sample mean.
Finally,
np.count_nonzero((sample_means > v2) & (sample_means <= v4))
is a number between 0 and 10,000, corresponding to the
number of True elements in the array
(sample_means > v2) & (sample_means <= v4).

Remember, the histogram we’re looking at visualizes the distribution
of the 10,000 values that result from calling f on every
value in sample_means. To proceed, we need to understand
how the function f decides what value to return
for a given input, v:

If the input v is greater than v2, then
the first two conditions (v <= v1 and
v <= v2) are False, and so the only
possible values of f(v) are 0, 1,
or 2.
If the input v is less than or equal to
v4, the only possible values of f(v) are
-2, -1, 0, 1.
Thus, if the input v is greater than
v2 and less than or equal to v4, the
only possible values of f(v) are 0 and
1.

Now, our job boils down to finding the number of values in the
visualized distribution that are equal to 0 or 1. This is equivalent to
finding the number of values that fall in the [0, 1) and [1,
2) bins – since f only returns integer values, the
only value in the [0, 1) bin is 0 and
the only value in the [1, 2) bin is 1
(remember, histogram bins are left-inclusive and right-exclusive).
To do this, we need to find the proportion of values in
those two bins, and multiply that proportion by the total number of
values (10,000).
We know that the area of a bar is equal to the proportion of values
in that bin. We also know that, in this case, the area
of each bar is equal to its height, since the width of each bin is 1.
Thus, the proportion of values in a given bin is equal to the height of
the corresponding bar. As such, the proportion of values in the [0, 1) bin is 12x, and the proportion of values in the
[1, 2) bin is 3x, meaning the proportion of values in the
histogram that are equal to either 0 or 1 is 12x + 3x = 15x.
In the previous subpart, we found that x =
\frac{1}{20}, so the proportion of values in the histogram that
are equal either 0 or 1 is 15 \cdot
\frac{1}{20} = \frac{3}{4}*, and since there are 10,000 values
being visualized in the histogram total, \frac{3}{4} \cdot 10,000 = 7,500 of them are
equal to either 0 or 1.
Thus, 7,500 of the values in sample_means are greater
than v2 and less than or equal to v4, so
np.count_nonzero((sample_means > v2) & (sample_means <= v4))
evaluates to 7,500.
Note: It’s possible to answer this subpart without knowing the
value of x, i.e. without answering the
previous subpart. The area of the [0,
1) and [1, 2) bars is 15x, and the total area of the histogram is
20x. So, the proportion of the area in
[0, 1) or [1,
2) is \frac{15x}{20x} = \frac{15}{20} =
\frac{3}{4}, which is the same value we found by substituting
x = \frac{1}{20} into 15x.",40.0,Hard
571,Fa,22,Final,,Problem 11,Problem 11.3,"What is the value of v3, one of the variables used in
the function f? Give your answer as a number.
Hint: Use the histogram as well as one of the rows of the
table above.","35.84
The table provided above tells us the proportion of values within
u standard deviations of the mean in a
normal distribution, for various values of u. For instance, it tells us that the
proportion of values within 1.28 standard deviations of the mean in a
normal distribution is 0.8.
Let’s reflect on what we know at the moment:

The distribution of the sample mean is roughly normal, by the
Central Limit Theorem. Normal distributions are symmetric, and have a
“peak” at the center. The histogram above is also symmetric and has its
peak at its center.
The proportion of values in the histogram that are equal to 0 is
12x = 12 \cdot \frac{1}{20} = 0.6.
The function f returns 0 for all inputs that are
greater than v2 and less than or equal to v3.
This, combined with the fact above, tells us that the proportion
of sample means between v2 (exclusive) and v3
(inclusive) is 0.6.
From the table provided, we know that in a normal distribution, the
proportion of values within 0.84 standard deviations of the mean is
0.6.

Combining the facts above, we have that v2 is 0.84
standard deviations below the mean of the sample mean’s distribution and
v3 is 0.84 standard deviations above the mean of the sample
mean’s distribution.
The sample mean’s distribution has the following characteristics:
\begin{align*}
\text{Mean of Distribution of Possible Sample Means} &=
\text{Population Mean} = 35 \\
\text{SD of Distribution of Possible Sample Means} &=
\frac{\text{Population SD}}{\sqrt{\text{Sample Size}}} =
\frac{10}{\sqrt{100}} = 1
\end{align*}

0.84 standard deviations above the mean of the sample mean’s
distribution is:
35 + 0.84 \cdot \frac{10}{\sqrt{100}} = 35
+ 0.84 \cdot 1 = \boxed{35.84}
So, the value of v3 is 35.84.",9.0,Hard
572,Fa,22,Final,,Problem 11,Problem 11.4,"Which of the following is closest to the value of the expression
below?
np.percentile(sample_means, 5)

 14
 14.75
 33
 33.35
 33.72","33.35
The table provided tells us that in a normal distribution, 90% of
values are within 1.65 standard deviations of the mean. Since normal
distributions are symmetric, it also means that 5% of values are above
1.65 standard deviations of the mean and, more importantly, 5%
of values are below 1.65 standard deviations of the mean.
The 5th percentile of a distribution is the smallest value that is
greater than or equal to 5% of values, so in this case the 5th
percentile is 1.65 SDs below the mean. As in the previous subpart, the
mean and SD we are referring to are the mean and SD of the distribution
of sample means (sample_means), which we found to be 35 and
1, respectively.
1.65 standard deviations below this mean is
35 - 1.65 \cdot 1 =
\boxed{33.35}",45.0,Hard
573,Fa,23,Final,,Problem 1,Problem 1,,,,
574,Fa,23,Final,,Problem 1,Problem 1.1,"Nate’s favorite number is 5. He calls a number “lucky” if it’s
greater than 500 or if it contains a 5 anywhere in its representation.
For example, 1000.04 and 5.23 are both lucky
numbers.
Complete the implementation of the function check_lucky,
which takes in a number as a float and returns True if it
is lucky and False otherwise. Then, add a column named
""is_lucky"" to txn that contains
True for lucky transaction amounts and False
for all other transaction amounts, and save the resulting DataFrame to
the variable luck.
        def check_lucky(x):
            return __(a)__

        luck = txn.assign(is_lucky = __(b)__)

What goes in blank (a)?
What goes in blank (b)?","(a):
x > 500 or ""5"" in str(x), (b):
txn.get(""amount"").apply(check_lucky)
(a): We want this function to return True if the number
is lucky (greater than 500 or if it has a 5 in it). Checking the first
condition is easy, we can simply use x > 500. To check the second
condition, we’ll convert the number to a string so that we can check
whether it contains ""5"" using the in keyword.
Once we have these two conditions written out, we can combine them with
the or keyword, since either one is enough for the number
to be considered lucky. This gives us the full statement
x > 500 or ""5"" in str(x). Since this will evaluate to
True if and only if the number is lucky, this is all we
need in the return statement.",51.0,Medium
575,Fa,23,Final,,Problem 1,Problem 1.2,"Fill in the blanks below so that lucky_prop evaluates to
the proportion of fraudulent ""visa"" card transactions whose
transaction amounts are lucky.
    visa_fraud = __(a)__
    lucky_prop = visa_fraud.__(b)__.mean()

What goes in blank (a)?
What goes in blank (b)?","(a):
luck[(luck.get(""card"")==""visa"") & (luck.get(""is_fraud""))],
(b): get(""is_lucky"")
(a): The first step in this question is to query the DataFrame so
that we have only the rows which are fraudulent transactions from “visa”
cards. luck.get(""card"")==""visa"" evaluates to
True if and only if the transaction was from a Visa card,
so this is the first part of our condition. To find transactions which
were fraudulent, we can simply find the rows with a value of
True in the ""is_fraud"" column. We can do this
with luck.get(""is_fraud""), which is equivalent to
luck.get(""is_fraud"") == True in this case since the
""is_fraud"" column only contains Trues and Falses. Since we
want only the rows where both of these conditions hold, we can combine
these two conditions with the logical & operator, and
place this inside of square brackets to query the luck DataFrame for
only the rows where both conditions are true, giving us
luck[(luck.get(""card"")==""visa"") & (luck.get(""is fraud"")].
Note that we use the & instead of the keyword
and since & is used for elementwise
comparisons between two Series, like we’re doing here, whereas the
and keyword is used for comparing two Booleans (not two
Series containing Booleans).",52.0,Medium
576,Fa,23,Final,,Problem 1,Problem 1.3,"Fill in the blanks below so that lucky_prop is one value
in the Series many_props.
    many_props = luck.groupby(__(a)__).mean().get(__(b)__)

What goes in blank (a)?
What goes in blank (b)?","(a): [""""card"""", ""is_fraud""],
(b): ""is_lucky""
(a): lucky_prop is the proportion of fraudulent “visa”
card transactions that have a lucky amount. The idea is to create a
Series with the proportions of fraudulent or non-fraudulent transactions
from each card type that have a lucky amount. To do this, we’ll want to
group by the column that describes the card type (""card""),
and the column that describes whether a transaction is fraudulent
(""is_fraud""). Putting this in the proper syntax for a
groupby with multiple columns, we have
[""card"", ""is_fraud""]. The order doesn’t matter, so
[""is_fraud"", """"card""""] is also correct.",55.0,Medium
577,Fa,23,Final,,Problem 2,Problem 2,"Consider the DataFrame combo, defined below.
    combo = txn.groupby([""is_fraud"", ""method"", ""card""]).mean()",,,
578,Fa,23,Final,,Problem 2,Problem 2.1,"What is the maximum possible value of combo.shape[0]?
Give your answer as an integer.","16
combo.shape[0] will give us the number of rows of the
combo DataFrame. Since we’re grouping by
""is_fraud"", ""method"", and ""card"",
we will have one row for each unique combination of values in these
columns. There are 2 possible values for ""is_fraud"", 2
possible values for ""method"", and 2 possible values for
""card"", so the total number of possibilities is 2 * 2 * 4 =
16. This is the maximum number possible because 16 combinations of
""is_fraud"", ""method"", and ""card""
are possible, but they may not all exist in the data.",75.0,Easy
579,Fa,23,Final,,Problem 2,Problem 2.2,"What is the value of combo.shape[1]?

 1
 2
 3
 4
 5
 6","2
combo.shape[1] will give us the number of columns of the
DataFrame. In this case, we’re using .mean() as our
aggregation function, so the resulting DataFrame will only have columns
with numeric types (since BabyPandas automatically ignores columns which
have a data type incompatible with the aggregation function). In this
case, ""amount"" and ""lifetime"" are the only
numeric columns, so combo will have 2 columns.",47.0,Hard
580,Fa,23,Final,,Problem 3,Problem 3,"Consider the variable is_fraud_mean, defined below.
is_fraud_mean = txn.get(""is_fraud"").mean()
Which of the following expressions are equivalent to
is_fraud_mean? Select all that apply.

 txn.groupby(""is_fraud"").mean()
 txn.get(""is_fraud"").sum() / txn.shape[0]
 np.count_nonzero(txn.get(""is_fraud"")) / txn.shape[0]
 (txn.get(""is_fraud"") > 0.8).mean()
 1 - (txn.get(""is_fraud"") == 0).mean()
 None of the above.","B, C, D, and E.
The correct responses are B, C, D, and E. First, we must see that
txn.get(""is_fraud"").mean() will calculate the mean of the
""is_fraud"" column, which is a float representing the
proportion of values in the ""is_fraud"" column that are
True. With this in mind, we can consider each option:

Option A: This operation will result in a DataFrame. We first
group by ""is_fraud"", creating one row for fraudulent
transactions and one row for non-fraudulent ones. We then take the mean
of each numerical column, which will determine the entries of the
DataFrame. Since this results in a DataFrame and not a float, this
answer choice cannot be correct.
Option B: Here we simply take the mean of the
""is_fraud"" column using the definition of the mean as the
sum of the values divided by the nuber of values. This is equivalent to
the original.
Option C: np.count_nonzero will return the number of
nonzero values in a sequence. Since we only have True and False values
in the ""is_fraud"" column, and Python considers
True to be 1 and False to be 0, this means
counting the number of ones is equivalent to the sum of all the values.
So, we end up with an expression equivalent to the formula for the mean
which we saw in part B.
Option D: Recall that ""is_fraud"" contains Boolean
values, and that True evaluates to 1 and False
evaluates to 0. txn.get(""is_fraud"") > 0.8 conducts an
elementwise comparison, evaluating if each value in the column is
greater than 0.8, and returning the resulting Series of Booleans. Any
True (1) value in the column will be greater than 0.8, so
this expression will evaluate to True. Any
False (0) value will still evaluate to False,
so the values in the resulting Series will be identical to the original
column. Therefore, taking the mean of either will give the same
value.
Option E: txn.get(""is_fraud"") == 0 performs an
elementwise comparison, returning a series which has the value
True where ""is_fraud"" is False
(0), and False where ""is_fraud"" is
True. Therefore the mean of this Series represents the
proportion of values in the ""is_fraud"" column that are
False. Since every value in that column is either
False or True, the proportion of
True values is equivalent to one minus the proportion of
False values.",69.0,Medium
581,Fa,23,Final,,Problem 4,Problem 4,"The following code block produces a bar chart, which is shown
directly beneath it.
    (txn.groupby(""browser"").mean()
        .sort_values(by=""is_fraud"", ascending=False)
        .take(np.arange(10))
        .plot(kind=""barh"", y=""is_fraud""))",,,
582,Fa,23,Final,,Problem 4,Problem 4.1,"Based on the above bar chart, what can we conclude about the browser
""icedragon""? Select all that apply.

 In our dataset, ""icedragon"" is the most frequently used
browser among all transactions.
 In our dataset, ""icedragon"" is the most frequently used
browser among fraudulent transactions.
 In our dataset, every transaction made with ""icedragon""
is fraudulent.
 In our dataset, there are more fraudulent transactions made with
""icedragon"" than with any other browser.
 None of the above.","C
First, let’s take a look at what the code is doing. We start by
grouping by browser and taking the mean, so each column will have the
average value of that column for each browser (where each browser is a
row). We then sort in descending order by the ""is_fraud""
column, so the browser which has the highest proportion of fraudulent
transactions will be first, and we take first the ten browsers, or those
with the most fraud. Finally, we plot the ""is_fraud"" column
in a horizontal bar chart. So, our plot shows the proportion of
fraudulent transactions for each browser, and we see that
icedragon has a proportion of 1.0, implying that every
transaction is fraudulent. This makes the third option correct. Since we
don’t have enough information to conclude any of the other options, the
third option is the only correct one.",83.0,Easy
583,Fa,23,Final,,Problem 4,Problem 4.2,"How can we modify the provided code block so that the bar chart
displays the same information, but with the bars sorted in the opposite
order (i.e. with the longest bar at the top)?

 Change ascending=False to
ascending=True.
 Add .sort_values(by=""is_fraud"", ascending=True)
immediately before .take(np.arange(10)).
 Add .sort_values(by=""is_fraud"", ascending=True)
immediately after .take(np.arange(10)).
 None of the above.","C
Let’s analyze each option A: This isn’t correct, because we must
remember that np.take(np.arange(10)) takes the rows indexed
0 through 10. And if we change ascending = False to
ascending = True, the rows indexed 0 through 10 won’t be
the same in the resulting DataFrame (since now they’ll be the 10
browsers with the lowest fraud rate). B: This will have
the same effect as option A, since it’s being applied before the
np.take() operation C: Once we have the 10 rows with the
highest fraud rate, we can sort them in ascending order in order to
reverse the order of the bars. Since we already have the 10 rows from
the original plot, this option is correct.",59.0,Medium
584,Fa,23,Final,,Problem 5,Problem 5,"The DataFrame seven, shown below to the
left, consists of a simple random sample of 7 rows from
txn, with just the ""is_fraud"" and
""amount"" columns selected.
The DataFrame locations, shown below to the
right, is missing some values in its
""is_fraud"" column.",,,
585,Fa,23,Final,,Problem 5,Problem 5.1,"Fill in the blanks to complete the ""is_fraud"" column of
locations so that the DataFrame
seven.merge(locations, on=""is_fraud"") has
19 rows.",,88.0,Easy
586,Fa,23,Final,,Problem 5,Problem 5.2,"True or False: It is possible to fill in the four blanks in the
""is_fraud"" column of locations so that the
DataFrame seven.merge(locations, on=""is_fraud"") has
14 rows.

 True
 False","False
As we discovered by solving problem 5.1, each False
value in locations gives rise to 5 rows of the merged
DataFrame, and each True value gives rise to 2 rows. This
means that the number of rows in the merged DataFrame will be m\cdot5 + n\cdot2, where m is the number of
Falses in location and n is the number of
Trues in location. Namely, m and n are
integers that add up to 5. There’s only a few possibilities so we can
try them all, and see that none add up 14:

0\cdot5 + 5\cdot2 = 10
1\cdot5 + 4\cdot2 = 13
2\cdot5 + 3\cdot2 = 16
3\cdot5 + 2\cdot2 = 19
4\cdot5 + 1\cdot2 = 22",79.0,Easy
587,Fa,23,Final,,Problem 6,Problem 6,"Aaron wants to explore the discrepancy in fraud rates between
""discover"" transactions and ""mastercard""
transactions. To do so, he creates the DataFrame ds_mc,
which only contains the rows in txn corresponding to
""mastercard"" or ""discover"" transactions.
After he creates ds_mc, Aaron groups ds_mc
on the ""card"" column using two different aggregation
methods. The relevant columns in the resulting DataFrames are shown
below.


Aaron decides to perform a test of the following pair of
hypotheses:

Null Hypothesis: The proportion of fraudulent
""mastercard"" transactions is the same as
the proportion of fraudulent ""discover""
transactions.
Alternative Hypothesis: The proportion of
fraudulent ""mastercard"" transactions is less
than the proportion of fraudulent ""discover""
transactions.

As his test statistic, Aaron chooses the difference in
proportion of transactions that are fraudulent, in the order
""mastercard"" minus ""discover"".",,,
588,Fa,23,Final,,Problem 6,Problem 6.1,"What type of statistical test is Aaron performing?

 Standard hypothesis test
 Permutation test","Permutation test
Permutation tests are used to ascertain whether two samples were
drawn from the same population. Hypothesis testing is used when we have
a single sample and a known population, and want to determine whether
the sample appears to have been drawn from that population. Here, we
have two samples (“mastercard” and “discover”)
and no known population distribution, so a permutation test is the
appropriate test.",49.0,Hard
589,Fa,23,Final,,Problem 6,Problem 6.2,"What is the value of the observed statistic? Give your answer either
as an exact decimal or simplified fraction.","0.02
We simply take the difference in fraudulent proportion of
""mastercard"" transactions and fraudulent proportion of
discover transactions. There are 4,000 fraudulent
""mastercard"" transactions and 40,000 total
""mastercard"" transactions, making this proportion for
""mastercard"". Similarly, the proportion of fraudulent
""discover"" transactions is \frac{160}{2000}. Simplifying these
fractions, the difference between them is \frac{1}{10} - \frac{8}{100} = 0.1 - 0.08 =
0.02.",86.0,Easy
590,Fa,23,Final,,Problem 6,Problem 6.3,"Which of the following is closest to the p-value of Aaron’s test?

 0.001
 0.37
 0.63
 0.94
 0.999","0.999
Informally, the p-value is the area of the histogram at or past the
observed statistic, further in the direction of the alternative
hypothesis. In this case, the alternative hypothesis is that the
""mastercard"" proportion is less than the discover
proportion, and our test statistic is computed in the order
""mastercard"" minus ""discover"", so low
(negative) values correspond to the alternative. This means when
calculating the p-value, we look at the area to the left of 0.02 (the
observed value). We see that essentially all of the test statistics fall
to the left of this value, so the p-value should be closest to
0.999.",54.0,Medium
591,Fa,23,Final,,Problem 6,Problem 6.4,"What is the conclusion of Aaron’s test?

 The proportion of fraudulent ""mastercard"" transactions
is less than the proportion of fraudulent
""discover"" transactions.
 The proportion of fraudulent ""mastercard"" transactions
is greater than the proportion of fraudulent
""discover"" transactions.
 The test results are inconclusive.
 None of the above.","None of the above

Option A: Since the p-value was so high, it’s unlikely that the
proportion of fraudulent ""mastercard"" transactions is less
than the proportion of fraudulent ""discover"" transactions,
so we cannot conclude A.
Option B: The test does not allow us to conclude this, because it
was not one of the hypotheses. All we can say is that we don’t think the
alternative hypothesis is true - we can’t say whether any other
statement is true.
Option C: The test did give us valuable information about the
difference in fraud rates: we failed to reject the null hypothesis. So,
the test is conclusive, making option C incorrect. Therefore, option D
(none of the above) is correct.",44.0,Hard
592,Fa,23,Final,,Problem 6,Problem 6.5,"Which of the following is closest to the p-value of Aaron’s new
test?

 0.001
 0.06
 0.37
 0.63
 0.94
 0.999","0.001
Now, we have switched the alternative hypothesis to “
""mastercard"" fraud rate is greater than
""discover"" fraud rate”, whereas before our alternative
hypothesis was that the ""mastercard"" fraud rate was less
than ""discover""’s fraud rate. We have not changed the way
we calculate the test statistic (""mastercard"" minus
""discover""), so now large values of the test statistic
correspond to the alternative hypothesis. So, the area of interest is
the area to the right of 0.02, which is very small, close to 0.001. Note
that this is one minus the p-value we computed before.",65.0,Medium
593,Fa,23,Final,,Problem 7,Problem 7,"Jason is interested in exploring the relationship between the browser
and payment method used for a transaction. To do so, he uses
txn to create create three tables, each of which contain
the distribution of browsers used for credit card transactions and the
distribution of browsers used for debit card transactions, but with
different combinations of browsers combined in a single category in each
table.



Jason calculates the total variation distance (TVD) between the two
distributions in each of his three tables, but he does not record which
TVD goes with which table. He computed TVDs of 0.14, 0.36, and 0.38.",,,
594,Fa,23,Final,,Problem 7,Problem 7.1,"In which table do the two distributions have a TVD of 0.14?

 Table 1
 Table 2
 Table 3","Table 3
Without values in any of the tables, there’s no way to do this
problem computationally. We are told that the three TVDs come out to
0.14, 0.36, and 0.38. The exact numbers are not important but their
relative order is. The key to this problem is noticing that when we
combine two categories into one, the TVD can only decrease, it cannot
increase. One way to see this is to think about combining categories
repeatedly until there’s just one category. Then both distributions must
have a value of 1 in that category so they are identical distributions
with the smallest possible TVD of 0. As we collapse categories, we can
only decrease the TVD. This tells us that Table 1 has the largest TVD,
then Table 2 has the middle TVD, and Table 3 has the smallest, since
each time we are combining categories and shrinking the TVD.",77.0,Easy
595,Fa,23,Final,,Problem 7,Problem 7.2,"In which table do the two distributions have a TVD of 0.36?

 Table 1
 Table 2
 Table 3","Table 2
See the solution to 7.1.",97.0,Easy
596,Fa,23,Final,,Problem 7,Problem 7.3,"In which table do the two distributions have a TVD of 0.38?

 Table 1
 Table 2
 Table 3","Table 1
See the solution to 7.1.",,
597,Fa,23,Final,,Problem 8,Problem 8,"Since txn has 140,000 rows, Jack wants to get a quick
glimpse at the data by looking at a simple random sample of 10 rows from
txn. He defines the DataFrame ten_txns as
follows:
    ten_txns = txn.sample(10, replace=False)
Which of the following code blocks also assign ten_txns
to a simple random sample of 10 rows from txn?
Option 1:
    all_rows = np.arange(txn.shape[0])
    perm = np.random.permutation(all_rows)
    positions = np.random.choice(perm, size=10, replace=False)
    ten_txn = txn.take(positions)
Option 2:
    all_rows = np.arange(txn.shape[0])
    choice = np.random.choice(all_rows, size=10, replace=False)
    positions = np.random.permutation(choice)
    ten_txn = txn.take(positions)
Option 3:
    all_rows = np.arange(txn.shape[0])
    positions = np.random.permutation(all_rows).take(np.arange(10))
    ten_txn = txn.take(positions)
Option 4:
    all_rows = np.arange(txn.shape[0])
    positions = np.random.permutation(all_rows.take(np.arange(10)))
    ten_txn = txn.take(positions)
Select all that apply.

 Option 1
 Option 2
 Option 3
 Option 4
 None of the above.","Option 1, Option 2, and Option 3.
Let’s consider each option.

Option 1: First, all_rows is defined as an array
containing the integer positions of all the rows in the DataFrame. Then,
we randomly shuffle the elements in this array and store it in the array
permutations. Finally, we select 10 integers randomly
(without replacement), and use .take() to select the rows
from the DataFrame with the corresponding integer locations. In other
words, we are randomly selecting ten row numbers and taking those
randomly selected. This gives a simple random sample of 10 rows from the
DataFrame txn, so option 1 is correct.
Option 2: Option 2 is similar to option 1, except that the order
of the np.random.choice and the
np.random.permutation operations are switched. This doesn’t
affect the output, since the choice we made was, by definition, random.
Therefore, it doesn’t matter if we shuffle the rows before or after (or
not at all), since the most this will do is change the order of a sample
which was already randomly selected. So, option 2 is correct.
Option 3: Here, we randomly shuffle the elements of
all_rows, and then we select the first 10 elements with
np.take. Since the shuffling of elements from
all_rows was random, we don’t know which elements are in
the first 10 positions of this new shuffled array (in other words, the
first 10 elements are random). So, when we select the rows from
txn which have the corresponding integer locations in the
next step, we’ve simply selected 10 rows with random integer locations.
Therefore, this is a valid random sample from txn, and
option 3 is correct.
Option 4: The difference between this option and option 3 is the
order in which np.random.permutation and
np.take are executed. Here, we select the first 10 elements
before the permutation (inside the parentheses). As a result, the array
which we’re shuffling with np.random.permutation does not
include all the integer locations like all_rows does, it’s
simply the first ten elements. Therefore, this code produces a random
shuffling of the first 10 rows of txn, which is not a
random sample.",82.0,Easy
598,Fa,23,Final,,Problem 9,Problem 9,"The DataFrame ten_txns, displayed in its
entirety below, contains a simple random sample of 10 rows from
txn.",,,
599,Fa,23,Final,,Problem 9,Problem 9.1,"Suppose we randomly select one transaction from
ten_txns. What is the probability that the selected
transaction is made with a ""card"" of
""mastercard"" or a ""method"" of
""debit""? Give your answer either as an exact decimal or a
simplified fraction.","0.7
We can simply count the number of transactions meeting at least one
of the two criteria. More easily, there are only 3 rows that do not meet
either of the criteria (the rows that are ""visa"" and
""credit"" transactions). Therefore, the probability is 7 out
of 10, or 0.7. Note that we cannot simply add up the probability of
""mastercard"" (0.3) and the probability of
""debit"" (0.6) since there is overlap between these. That
is, some transactions are both ""mastercard"" and
""debit"".",64.0,Medium
600,Fa,23,Final,,Problem 9,Problem 9.2,"Suppose we randomly select two transactions from
""ten_txns"", without replacement, and learn that neither of
the selected transactions is for an amount of 100 dollars. Given this
information, what is the probability that:

the first transaction is made with a ""card"" of
""visa"" and a ""method"" of ""debit"",
and
the second transaction is made with a ""card"" of
""visa"" and a ""method"" of
""credit""?

Give your answer either as an exact decimal or a simplified
fraction.","\frac{2}{15}
We know that the sample space here doesn’t have any of the $100
transactions, so we can ignore the first 4 rows when calculating the
probability. In the remaining 6 rows, there are exactly 2 debit
transactions with ""visa"" cards, so the probability of our
first transaction being of the specified type is \frac{2}{6}. There are also two credit
transactions with ""visa"" cards, but the denominator of the
probability of the second transaction is 5 (not 6), since the sample
space was reduced by one after the first transaction. We’re choosing
without replacement, so you can’t have the same transaction in your
sample twice. Thus, the probability of the second transaction being a
visa credit card is \frac{2}{5}. Now,
we can apply the multiplication rule, and we have that the probability
of both transactions being as described is \frac{2}{6} \cdot \frac{2}{5} = \frac{4}{30} =
\frac{2}{15}.",45.0,Hard
601,Fa,23,Final,,Problem 9,Problem 9.3,"Suppose we randomly select 15 rows, with replacement, from
ten_txns. What’s the probability that in our selection of
15 rows, the maximum transaction amount is less than 25 dollars?

 \frac{3}{10}
 \frac{3}{15}
 \left(\frac{3}{10}\right)^{3}
 \left(\frac{3}{15}\right)^{3}
 \left(\frac{3}{10}\right)^{10}
 \left(\frac{3}{15}\right)^{10}
 \left(\frac{3}{10}\right)^{15}
 \left(\frac{3}{15}\right)^{15}","\left(\frac{3}{10}\right)^{15}
There are only 3 rows in the sample with a transaction amount under
$25, so the chance of choosing one transaction with such a low value is
\frac{3}{10}. For the maximum
transaction amount to be less than 25 dollars, this means all
transaction amounts in our sample have to be less than 25 dollars. To
find the chance that all transactions are for less than $25, we can
apply the multiplication rule and multiply the probability of each of
the 15 transactions being less than $25. Since we’re choosing 15 times
with replacement, the events are independent (choosing a certain
transaction on the first try won’t affect the probability of choosing it
again later), so all the terms in our product are \frac{3}{10}. Thus, the probability is \frac{3}{10} * \frac{3}{10} * \ldots * \frac{3}{10}
= \left(\frac{3}{10}\right)^{15}.",89.0,Easy
602,Fa,23,Final,,Problem 10,Problem 10,"As a senior suffering from senioritis, Weiyue has plenty of time on
his hands. 1,000 times, he repeats the following process, creating 1,000
confidence intervals:

Collect a simple random sample of 100 rows from
txn.
Resample from his sample 10,000 times, computing the mean
transaction amount in each resample.
Create a 95% confidence interval by taking the middle 95% of
resample means.

He then computes the width of each confidence interval by subtracting
its left endpoint from its right endpoint; e.g. if [2, 5] is a confidence interval, its width is
3. This gives him 1,000 widths. Let M
be the mean of these 1,000 widths.",,,
603,Fa,23,Final,,Problem 10,Problem 10.1,"Select the true statement below.

 About 950 of Weiyue’s intervals will contain the mean transaction
amount of all transactions ever.
 About 950 of Weiyue’s intervals will contain the mean transaction
amount of all transactions in txn.
 About 950 of Weiyue’s intervals will contain the mean transaction
amount of all transactions in the first random sample of 100 rows of
txn Weiyue took.
 About 950 of Weiyue’s intervals will contain M.","About 950 of Weiyue’s intervals will contain
the mean transaction amount of all transactions in txn.
By the definition of a 95% confidence interval, 95% of our 1000
confidence intervals will contain the true mean transaction amount in
the population from which our samples were drawn. In this case, the
population is the txn DataFrame. So, 950 of the confidence
intervals will contain the mean transaction amount of all transactions
in txn, which is what the the second answer choice
says.
We can’t conclude that the first answer choice is correct because our
original sample was taken from txn, not from all
transactions ever. We don’t know whether our resamples will be
representative of all transactions ever. The third option is incorrect
because we have no way of knowing what the first random sample looks
like from a statistical standpoint. The last statement is not true
because M concerns the width of the
confidence interval, and therefore is unrelated to the statistics
computed in each resample. For example, if the mean of each resample is
around 100, but the width of each confidence interval is around 5, we
shouldn’t expect $$M to be in any of the confidence intervals.",55.0,Medium
604,Fa,23,Final,,Problem 10,Problem 10.2,"Weiyue repeats his entire process, except this time, he changes his
sample size in step 1 from 100 to 400. Let B be the mean of the widths of the 1,000 new
confidence intervals he creates.
What is the relationship between M
and B?

 M < B
 M \approx B
 M > B","M >
B
As the sample size increases, the width of the confidence intervals
will decrease, so M > B.",70.0,Medium
605,Fa,23,Final,,Problem 10,Problem 10.3,"Weiyue repeats his entire process once again. This time, he still
uses a sample size of 100 in step 1, but instead of creating 95%
confidence intervals in step 3, he creates 99% confidence intervals. Let
C be the mean of the widths of the
1,000 new confidence intervals he generates.
What is the relationship between M
and C?

 M < C
 M \approx C
 M > C","M <
C
All else equal (note that the sample size is the same as it was in
question 10.1), 99% confidence intervals will always be wider than 95%
confidence intervals on the same data, so M
< C.",85.0,Easy
606,Fa,23,Final,,Problem 10,Problem 10.4,"Weiyue repeats his entire process one last time. This time, he still
uses a sample size of 100 in step 1, and creates 95% confidence
intervals in step 3, but instead of bootstrapping, he uses the Central
Limit Theorem to generate his confidence intervals. Let D be the mean of the widths of the 1,000 new
confidence intervals he creates.
What is the relationship between M
and D?

 M < D
 M \approx D
 M > D","M \approx
D
Confidence intervals generated from the Central Limit Theorem will be
approximately the same as those generated from bootstrapping, so M is approximately equal to D.",90.0,Easy
607,Fa,23,Final,,Problem 11,Problem 11,"On Reddit, Yutian read that 22% of all online transactions are
fraudulent. She decides to test the following hypotheses:

Null Hypothesis: The proportion of online
transactions that are fraudulent is 0.22.
Alternative Hypothesis: The proportion of online
transactions that are fraudulent is not 0.22.

To test her hypotheses, she decides to create a 95%
confidence interval for the proportion of online transactions that are
fraudulent using the Central Limit Theorem.
Unfortunately, she doesn’t have access to the entire txn
DataFrame; rather, she has access to a simple random sample of
txn of size n. In her
sample, the proportion of transactions that are fraudulent is
0.2 (or equivalently, \frac{1}{5}).",,,
608,Fa,23,Final,,Problem 11,Problem 11.1,"The width of Yutian’s confidence interval is of the form \frac{c}{5 \sqrt{n}}
where n is the size of her sample
and c is some positive integer. What is
the value of c? Give your answer as an
integer.
Hint: Use the fact that in a collection of 0s and 1s, if the
proportion of values that are 1 is p,
the standard deviation of the collection is \sqrt{p(1-p)}.","8
First, we can calculate the standard deviation of the sample using
the given formula: \sqrt{0.2\cdot(1-0.2)} =
\sqrt{0.16}= 0.4. Additionally, we know that the width of a 95%
confidence interval for a population mean (including a proportion) is
approximately \frac{4 * \text{sample
SD}}{\sqrt{n}}, since 95% of the data of a normal distribution
falls within two standard deviations of the mean on either side. Now,
plugging the sample standard deviation into this formula, we can set
this expression equal to the given formula for the width of the
confidence interval: \frac{c}{5 \sqrt{n}} =
\frac{4 * 0.4}{\sqrt{n}}. We can multiply both sides by \sqrt{n}, and we’re left with \frac{c}{5} = 4 * 0.4. Now, all we have to do
is solve for c by multiplying both
sides by 5, which gives c = 8.",59.0,Medium
609,Fa,23,Final,,Problem 11,Problem 11.2,"There is a positive integer J such
that:

If n < J, Yutian will fail to
reject her null hypothesis at the 0.05 significance
level.
If n > J, Yutian will reject
her null hypothesis at the 0.05 significance
level.

What is the value of J? Give your
answer as an integer.","1600
Here, we have to use the formula for the endpoints of the 95%
confidence interval to see what the largest value of n is such that 0.22 will be contained in the
interval. The endpoints are given by \text{sample mean} \pm 2 * \frac{\text{sample
SD}}{\sqrt{n}}. Since the null hypothesis is that the proportion
is 0.22 (which is greater than our sample mean), we only need to work
with the right endpoint for this question. Plugging in the values that
we have, the right endpoint is given by 0.2 +
2 * \frac{0.4}{\sqrt{n}}. Now we must find a value of n which satisfies the inequality 0.2 + 2 * \frac{0.4}{\sqrt{n}} \geq 0.22, and
since we’re looking for the smallest such value of n (i.e, the last n for which this inequality holds), we can
simply set the two sides equal to each other, and solve for n. From 0.2 + 2 *
\frac{0.4}{\sqrt{n}} = 0.22, we can subtract 0.2 from both sides,
then multiply both sides by \sqrt{n},
and divide both sides by 0.02 (from 0.22 - 0.2). This yields \sqrt{n} = \frac{2 * 0.4}{0.02} = \sqrt{n} =
40, which implies that n is
1600.",21.0,Hard
610,Fa,23,Final,,Problem 12,Problem 12,"On Reddit, Keenan also read that 22% of all online transactions are
fraudulent. He decides to test the following hypotheses at the
0.16 significance level:

Null Hypothesis: The proportion of online
transactions that are fraudulent is 0.22.
Alternative Hypothesis: The proportion of online
transactions that are fraudulent is not 0.22.

Keenan has access to a simple random sample of txn of
size 500. In his sample, the proportion of transactions
that are fraudulent is 0.23.
Below is an incomplete implementation of the function
reject_null, which creates a bootstrap-based confidence
interval and returns True if the conclusion of Keenan’s
test is to reject the null hypothesis, and
False if the conclusion is to fail to
reject the null hypothesis, all at the 0.16
significance level.
    def reject_null():
        fraud_counts = np.array([])
        for i in np.arange(10000):
            fraud_count = np.random.multinomial(500, __(a)__)[0] 
            fraud_counts = np.append(fraud_counts, fraud_count)
            
        L = np.percentile(fraud_counts, __(b)__)
        R = np.percentile(fraud_counts, __(c)__)

        if __(d)__ < L or __(d)__ > R:
            # Return True if we REJECT the null.
            return True
        else:
            # Return False if we FAIL to reject the null.
            return False
Fill in the blanks so that reject_null works as
intended.
Hint: Your answer to (d) should be an integer greater than
50.","(a): [0.23, 0.77], (b):
8, (c): 92, (d): 110
(a): We know that the proportion of fraudulent transactions in the
sample is 0.23 (and therefore the non-fraudulent proportion is 0.77), so
we use these as the probabilities for np.random.multinomial
in our bootstrapping simulation. The syntax for this function requires
us to pass in the probabilities as a list, so the answer is
[0.23, 0.77].",23.0,Hard
611,Fa,23,Final,,Problem 13,Problem 13,"Ashley doesn’t have access to the entire txn DataFrame;
instead, she has access to a simple random sample of
400 rows of txn.
She draws two histograms, each of which depicts the distribution of
the ""amount"" column in her sample, using different
bins.


Unfortunately, DataHub is being finicky and so two of the bars in
Histogram A are deleted after it is created.",,,
612,Fa,23,Final,,Problem 13,Problem 13.1,"In Histogram A, which of the following bins contains approximately 60
transactions?

 [30, 60)
 [90, 120)
 [120, 150)
 [180, 210)","[90,
120)
The number of transactions contained in the bin is given by the area
of the bin times the total number of transactions, since the area of a
bin represents the proportion of transactions that are contained in that
bin. We are asked which bin contains about 60 transactions, or \frac{60}{400} = \frac{3}{20} = 0.15
proportion of the total area. All the bins in Histogram A have a width
of 30, so for the area to be 0.15, we need the height h to satisfy h\cdot
30 = 0.15. This means h =
\frac{0.15}{30} = 0.005. The bin [90,
120) is closest to this height.",90.0,Easy
613,Fa,23,Final,,Problem 13,Problem 13.2,"Let w, x, y, and
z be the heights of bars W, X, Y, and Z,
respectively. For instance, y is about
0.01.
Which of the following expressions gives the height of the bar
corresponding to the [60, 90) bin in
Histogram A?

 ( y + z ) - ( w + x )
 ( w + x ) - ( y + z )
 \frac{3}{2}( y + z ) - ( w + x )
 ( y + z ) - \frac{3}{2}( w + x )
 3( y + z ) - 2( w + x )
 2( y + z ) - 3( w + x )
 None of the above.","\frac{3}{2}( y + z
) - ( w + x )
The idea is that the first three bars in Histogram A represent the
same set of transactions as the first three bars of Histogram B. Setting
these areas equals gives 30w+30x+30u=
45y+45z, where u is the unknown
height of the bar corresponding to the [60,
90) bin. Solving this equation for u gives the result.
\begin{align*}
30w+30x+30u &= 45y+45z \\
30u &= 45y+45z-30w-30x \\
u &= \frac{45y + 45z - 30w - 30x}{30} \\
u &= \frac{3}{2}(y+z) - (w+x)

\end{align*}",50.0,Medium
614,Fa,23,Final,,Problem 14,Problem 14,"As mentioned in the previous problem, Ashley has sample of 400 rows
of txn. Coincidentally, in Ashley’s sample of 400
transactions, the mean and standard deviation of the
""amount"" column both come out to 70 dollars.",,,
615,Fa,23,Final,,Problem 14,Problem 14.1,"Fill in the blank:

“According to Chebyshev’s inequality, at most 25 transactions in
Ashley’s sample
are above ____ dollars; the rest must be below ____ dollars.""

What goes in the blank? Give your answer as an
integer. Both blanks are filled in with the same
number.","350
Chebyshev’s inequality says something about how much data falls
within a given number of standard deviations. The data that doesn’t fall
in that range could be entirely below that range, entirely above that
range, or split some below and some above. So the idea is that we should
figure out the endpoints of the range for which Chebyshev guarantees
that at least 375 transactions must fall. Then at most 25 might fall
above that range. So we’ll fill in the blank with the upper limit of
that range. Now, since there are 400 transactions, 375 as a proportion
becomes \frac{375}{400} =
\frac{15}{16}. That’s 1 -
\frac{1}{16} or 1 -
\left(\frac{1}{4}\right)^2, so we should use z=4 in the statement of Chebyshev’s
inequality. That is, \frac{15}{16}
proportion of the data falls within 4 standard deviations of the mean.
The upper endpoint of that range is 70
(the mean) plus 4 \cdot 70 (four
standard deviations), or 5 \cdot 70 =
350.",30.0,Hard
616,Fa,23,Final,,Problem 14,Problem 14.2,"Now, we’re given that the mean and standard deviation of the
""lifetime"" column in Ashley’s sample are both equal to
c dollars. We’re also given that the
correlation between transaction amounts and lifetime spending in
Ashley’s sample is -\frac{1}{4}.
Which of the four options could be a scatter plot of lifetime
spending vs. transaction amount?



 Option A
 Option B
 Option C
 Option D","Option B
Here, the main factor which we can use to identify the correct plot
is the correlation coefficient. A correlation coefficient of -\frac{1}{4} indicates that the data will
have a slight downward trend (values on the y axis will be lower as we
go further right). This narrows it down to option A or option B, but
option A appears to have too strong of a linear trend. We want the data
to look more like a cloud than a line since the correlation is
relatively close to zero, which suggests that option B is the more
viable choice.",,
617,Fa,23,Final,,Problem 14,Problem 14.3,"The predicted lifetime spending, in dollars, of a
card with a transaction amount of 280 dollars is of the form f \cdot c, where f is a fraction. What is the value of f? Give your answer as a simplified
fraction.","f =
\frac{1}{4}
This problem requires us to make a prediction using the regression
line for a given x = 280. We can solve
this problem using original units or standard units. Since 280 is a
multiple of 70, and the mean and standard deviation are both 70, it’s
pretty straightforward to convert 280 to 3 standard units, as \frac{(280-70)}{70} = \frac{210}{70} = 3. To
make a prediction in standard units, all we need to do is multiply by
r=-\frac{1}{4}, resulting in a
predicted lifetime spending of =-\frac{3}{4} in standard units. Since we are
told in the previous subpart that both the mean and standard deviation
of lifetime spending are c dollars,
then converting to original units gives c +
-\frac{3}{4} \cdot c = \frac{1}{4} \cdot c, so f = \frac{1}{4}.",42.0,Hard
618,Fa,23,Final,,Problem 14,Problem 14.4,"Suppose the intercept of the regression line, when both transaction
amounts and lifetime spending are measured in dollars,
is 40. What is the value of c? Give
your answer as an integer.","c = 32
We start with the formulas for the mean and intercept of the
regression line, then set the mean and SD of x both to 70, and the mean
and SD of y both to c, as well as the intercept b to 40. Then we can
solve for c.
\begin{align*}

m &= r \cdot \frac{\text{SD of } y}{\text{SD of }x} \\
b &= \text{mean of } y - m \cdot \text{mean of } x \\
m &= -\frac{1}{4} \cdot \frac{c}{70} \\
40 &= c - (-\frac{1}{4} \cdot \frac{c}{70}) \cdot 70 \\
40 &= c + \frac{1}{4} c \\
40 &= \frac{5}{4} c \\
c &= 40 \cdot \frac{4}{5} \\
c &= 32

\end{align*}",45.0,Hard
619,Wi,21,Final,,Problem 1,Problem 1,"One way to use np.arange to produce the sequence
[2, 6, 10, 14] is np.arange(2, 15, 4). This
gives three inputs to np.arange.
Fill in the blanks below to show a different way to produce the same
sequence, this time using only one input to np.arange. Each blank below
must be filled in with a single number only, and the final result,
x*np.arange(y)+z, must produce the sequence
[2, 6, 10, 14].
Using x*np.arange(y)+z fill in the missing values:
x = _
y = _
z = _","x = 4, y = 4, z = 2
The question states that we are trying to create the sequence
[2, 6, 10, 14] by filling in the blanks for the statement
x*np.arange(y)+z. If we look at the sequence we are
attempting to derive, we can see that each step in the sequence
increments by 4. Similarly, we can see that the sequence begins at 2. We
know that passing an argument by itself in np.arange will
increment up to that number (for example np.arange(4) will
produce the sequence [0,1,2,3]). Knowing this, we can
create this sequence by setting y to 4. Attempting to reach the desired
sequence of [2, 6, 10, 14] from [0, 1, 2, 3]
we can multiply each number by 4 by setting x to 4 and instantiate the
sequence at 2 by setting z as 2.",96.0,Easy
620,Wi,21,Final,,Problem 2,Problem 2,"The command .set_index can take as input one column, to
be used as the index, or a sequence of columns to be used as a nested
index (sometimes called a MultiIndex). A MultiIndex is the default
behavior of the dataframe returned by .groupby with multiple
columns.
You are given a dataframe called restaurants that contains
information on a variety of local restaurants’ daily number of customers
and daily income. There is a row for each restaurant for each date in a
given five-year time period.
The columns of restaurants are 'name'
(str), 'year' (int),
'month' (int), 'day'
(int), 'num_diners' (int), and
'income' (float).
Assume that in our data set, there are not two different restaurants
that go by the same name (chain restaurants, for example).
Which of the following would be the best way to set the index for
this dataset?

 restaurants.set_index('name')
 restaurants.set_index(['year', 'month', 'day'])
 restaurants.set_index(['name', 'year', 'month', 'day'])","restaurants.set_index(['name', 'year', 'month', 'day'])
The correct answer is to create an index with the
'name', 'year', ‘month’, and
‘day’ columns. The question provides that there is a row
for each restaurant for each data in the five year span. Therefore, we
are interested in the granularity of a specific day (the day, the month,
and the year). In order to have this information available in this
index, we must set the index to be a multi index with columns
['name', 'year', 'month', 'day']. Looking at the other
options, simply looking at the 'name' column would not
account for the fact the dataframe contains daily data on customers and
income for each restaurant. Similarly, the second option of
['name', 'month', 'day'] would not account for the fact
that the data comes in a five year span so there will naturally be five
overlaps (one for each year) for each unique date that must be accounted
for.",53.0,Medium
621,Wi,21,Final,,Problem 3,Problem 3,"If we merge a table with n rows with a table with
m rows, how many rows does the resulting table have?

 n
 m
 max(m,n)
 m * n
 not enough information to tell","not enough information to tell
The question does not provide enough information to know the
resulting table size with certainty. When merging two tables together,
the tables can be merged with a inner, left, right, and outer join. Each
of these joins will produce a different amount of rows. Since the
question does not provide the type of join, it is impossible to tell the
resulting table size.",74.0,Medium
622,Wi,21,Final,,Problem 4,Problem 4,"You sample from a population by assigning each element of the
population a number starting with 1. You include element 1 in your
sample. Then you generate a random number, n, between 2 and
5, inclusive, and you take every nth element after element
1 to be in your sample. For example, if you select n=2,
then your sample will be elements 1, 3, 5, 7, and so
on.",,,
623,Wi,21,Final,,Problem 4,Problem 4.1,"True or False: Before the sample is drawn, you can
calculate the probability of selecting each subset of the
population.","True
The answer is true since someone can easily sketch each sample to
view the probability of selecting a certain subset. For example, when n
= 2 we know the elements are 1, 3, 5, 7, and so on. Similarly we know
this information for n = 3, 4 and 5. Using this information we could
calculate the probability of selecting a subset.",97.0,Easy
624,Wi,21,Final,,Problem 4,Problem 4.2,"True or False: Each subset of the population is
equally likely to be selected.","False
No, each subset of the population is not equally likely to be
selected since the element assigned as element 1 will always be selected
due to the way sampling is conducted as defined in the question. That
is, the question says we always include element one in the sample which
will over represent it in samples as compared to other parts of the
population.",46.0,Hard
625,Wi,21,Final,,Problem 5,Problem 5,"You are given a table called books that contains columns
'author' (str), 'title'
(str), 'num_chapters' (int), and
'publication_year' (int).",,,
626,Wi,21,Final,,Problem 5,Problem 5.1,"What will be the output of the following code?
books.groupby(“publication_year”).mean().shape[1]

 1
 2
 3
 4","1
The output will return 1. Notice that the final function call is to
.shape[1]. We know that .shape[1] is a call to
see how many columns are in the resulting data frame. When we group by
publication year, there is only one column that will be aggregated by
the groupby call (which is the 'num_chapters' column). The
other columns are string, and therefore, will not be aggregated in the
groupby call (since you can’t take the mean of a string). Consequently
.shape[1] will only result one column for the mean of the
'num_chapters' column.",67.0,Medium
627,Wi,21,Final,,Problem 5,Problem 5.2,"Which of the following strategies would work to compute the absolute
difference in the average number of chapters per book for authors “Dean
Koontz” and “Charles Dickens”?

 group by 'author', aggregate with .mean(),
use get on 'num_chapters' column compute the
absolute value of the difference between
iloc[""Charles Dickens""] and
iloc[""Dean Koontz""]
 do two queries to get two separate tables (one for each of “Dean
Koontz” and “Charles Dickens”), use get on the
'num_chapters' column of each table, use the Series method
.mean() on each, compute the absolute value of the
difference in these two means
 group by both 'author' and 'title',
aggregate with .mean(), use get on
'num_chapters' column, use loc twice to find
values in that column corresponding to “Dean Koontz” and “Charles
Dickens”, compute the absolute value of the difference in these two
values
 query using a compound condition to get all books corresponding to
“Dean Koontz” or “Charles Dickens”, group by 'author',
aggregate with .mean(), compute absolute value of the
difference in index[0] and index[1]","do two queries to get two separate tables
(one for each of “Dean Koontz” and “Charles Dickens”), use
get on the 'num_chapters' column of each
table, use the Series method .mean() on each, compute the
absolute value of the difference in these two means
Logically, we want to somehow separate data for author “Dean Koontz”
and “Charles Dickens”. (If we don’t we’ll be taking a mean that includes
the chapters of books from both authors.) To achieve this separation, we
can create two separate tables with a query that specifies a value on
the 'author' column. Now having two separate tables, we can
aggregate on the 'num_chapters' (the column of interest).
To get the 'num_chapters' column we can use the
get method. To actually acquire the mean of the
'num_chapters' column we can evoke the .mean()
call.",,
628,Wi,21,Final,,Problem 5,Problem 5.3,"Which of the following will produce the same value as the total
number of books in the table?

 books.groupby('Title').count().shape[0]
 books.groupby('Author').count().shape[0]
 books.groupby(['Author, 'Title']).count().shape[0]","books.groupby(['Author, 'Title']).count().shape[0]
The key in this question is to understand that different authors can
create books with the same name. The first two options check for each
unique book title (the first response) and check for each unique other
(the second response). To ensure we have all unique author and title
pairs we must group based on both 'Author' and
'Title'. To actually get the number of rows we can take
.shape[0].",,
629,Wi,21,Final,,Problem 6,Problem 6,"Suppose you have a dataset of 29 art galleries that includes the
number of pieces of art in each gallery.
A histogram of the number of art pieces in each gallery, as well as
the code that generated it, is shown below.",,,
630,Wi,21,Final,,Problem 6,Problem 6.1,"How many galleries have at least 80 but less than 100 art pieces?
Input your answer below. Make sure your answer is an integer and does
not include any text or symbols.","7
Through looking at the graph we can find the total number of art
galleries by taking 0.012 (height of bin) * 20 (the size of the bin) *
29 (total number of art galleries). This will yield an anwser of 6.96
which should be rounded to the nearest integer (7).",94.0,Easy
631,Wi,21,Final,,Problem 6,Problem 6.2,"If we added to our dataset two more art galleries, each containing 24
pieces of art, and plotted the histogram again for the larger dataset,
what would be the height of the bin [20,45)? Input your
answer as a number rounded to six decimal places.","0.007742
Taking the area of the bin [20,45] we can find the
number of art galleries already within this bin 0.0055 * 25 = 0.1375
(estimation based on the visualization). To find the number take this
proportion x the total number of art galleries. 0.1375 * 29 = about 4
art galleries. If we add two art galleries to this total we get 4 art
galleries in the [20,45] bin to get 6 art galleries. To
find the frequency of 6 art galleries to the entire data set we can take
6/31. Note that the question asks for the height of the bin.
Therefore, we can take (6/31) / 25 due to the size of the bin which will
give an answer of 0.007742 upon rounding to six decimal places.",66.0,Medium
632,Wi,21,Final,,Problem 7,Problem 7,"Assume df is a DataFrame with distinct rows. Which of
the following best describes df.sample(10)?

 an array of length 10, where some of the entries might be the
same
 an array of length 10, where no two entries can be the same
 a DataFrame with 10 rows, where some of the rows might be the
same
 a DataFrame with 10 rows, where no two rows can be the same","a DataFrame with 10 rows, where no two rows
can be the same
Looking at the documentation for .sample() we can see
that it accepts a few arguments. The first argument specifies the number
of rows (which is why we specify 10). The next argument is a boolean
that specifies if the sampling happens with or without replacement. By
default, the sampling will occur without replacement (which happens in
this question since no argument is specified so the default is evoked).
Looking at the return, we can see that since we are sampling a
dataframe, a dataframe will also be returned which is why a DataFrame
with 10 rows, where no two rows can be the same is correct.",94.0,Easy
633,Wi,21,Final,,Problem 8,Problem 8,"True or False: If you roll two dice, the probability
of rolling two fives is the same as the probability of rolling a six and
a three.","False
The probability of rolling two fives can be found with 1/6 * 1/6 =
1/36. The probability of rolling a six and a three can be found with 2/6
(can roll either a 3 or 6) * 1/6 (roll a different side form 3 or 6,
depending on what you rolled first) = 1/18. Therefore, the probabilities
are not the same.",,
634,Wi,21,Final,,Problem 9,Problem 9,"Suppose you do an experiment in which you do some random process 500
times and calculate the value of some statistic, which is a count of how
many times a certain phenomenon occurred out of the 500 trials. You
repeat the experiment 10,000 times and draw a histogram of the 10,000
statistics.",,,
635,Wi,21,Final,,Problem 9,Problem 9.1,"Is this histogram a probability histogram or an empirical
histogram?

 probability histogram
 empirical histogram","empirical histogram
Empirical histograms refer to distributions of observed data. Since
the question at hand is conducting an experiment and creating a
histogram of observed data from these trials the correct anwser is an
empirical histogram.",90.0,Easy
636,Wi,21,Final,,Problem 9,Problem 9.2,"If you instead repeat the experiment 100,000 times, how will the
histogram change?

 it will become wider
 it will become narrower
 it will barely change at all","it will barely change at all
Doing more of an experiment will barely change the histogram. The
parameter we are trying to estimate through our experiment is some
statistic. The number of experiments has no effect on the histograms
distribution since the value of some statistic is not becoming more
random.",57.0,Medium
637,Wi,21,Final,,Problem 9,Problem 9.3,"For each experiment, if you instead do the random process 5,000
times, how will the histogram change?

 it will become wider
 it will become narrower
 it will barely change at all","it will become wider
By increasing the number of random process we increase the possible
range of values from 500 to 5000. The statistic being calculated is the
count of how many times a phenomenon occurs. If the number of random
process increases 10x the statistic can now take values ranging from
[0, 5000] instead of [0, 500] which will
clearly make the histogram width wider (due to the wider range of values
it can take).",39.0,Hard
638,Wi,21,Final,,Problem 10,Problem 10,"Give an example of a dataset and a question you would want to answer
about that dataset which you could answer by performing a permutation
test (also known as an A/B test).
Creative responses that are different than ones we’ve already seen in
this class will earn the most credit.","Responses vary. For this question we looked
for creative responses. One good example includes
A dataset about prisoners in the US with the sentence times, race,
and crime. Do White people who commit homicide get shorter sentence
times than Black people who commit homicide? We can clearly perform an
A/B test to compare black and white populations as they correlated to
shorter sentence times.",93.0,Easy
639,Wi,21,Final,,Problem 11,Problem 11,"Suppose you draw a sample of size 100 from a population with mean 50
and standard deviation 15. What is the probability that your sample has
a mean between 50 and 53? Input the probability below, as a number
between 0 and 1, rounded to two decimal places.","0.48
This problem is testing our understanding of the Central Limit
Theorem and normal distributions. Recall, the Central Limit Theorem
tells us that the distribution of the sample mean is roughly normal,
with the following characteristics:
\begin{align*}
\text{Mean of Distribution of Possible Sample Means} &=
\text{Population Mean} = 50 \\
\text{SD of Distribution of Possible Sample Means} &=
\frac{\text{Population SD}}{\sqrt{\text{Sample Size}}} =
\frac{15}{\sqrt{100}} = 1.5
\end{align*}

Given this information, it may be easier to express the problem as
“We draw a value from a normal distribution with mean 50 and SD 1.5.
What is the probability that the value is between 50 and 53?” Note that
this probability is equal to the proportion of values between 50
and 53 in a normal distribution whose mean is 50 and 1.5 (since
probabilities can be thought of as proportions).
In class, we typically worked with the standard normal
distribution, in which the mean was 0, the SD was 1, and the x-axis represented values in standard units.
Let’s convert the quantities of interest in this problem to standard
units, keeping in mind that the mean and SD we’re using now are the mean
and SD of the distribution of possible sample means, not of the
population.

50 converted to standard units is \frac{50
- \text{mean}}{\text{SD}} = \frac{50 - 50}{1.5} = 0 (no
calculation was necessary – 0 in standard units is equal to the mean in
original units).
53 converted to standard units is \frac{53
- \text{mean}}{\text{SD}} = \frac{53 - 50}{1.5} = 2.

Now, our problem boils down to finding the proportion of
values in a standard normal distribution that are between 0 and
2, or the proportion of values in a normal distribution
that are in the interval [\text{mean},
\text{mean} + 2 \text{ SDs}].
From class, we know that in a normal distribution, roughly 95% of
values are within 2 standard deviations of the mean, i.e. the proportion
of values in the interval [\text{mean} - 2
\text{ SDs}, \text{mean} + 2 \text{ SDs}] is 0.95.

Since the normal distribution is symmetric about the mean, half of
the values in this interval are to the right of the mean, and half are
to the left. This means that the proportion of values in the interval
[\text{mean}, \text{mean} + 2 \text{
SDs}] is \frac{0.95}{2} = 0.475,
which rounds to 0.48, and thus the desired result is 0.48.",48.0,Hard
640,Wi,21,Final,,Problem 12,Problem 12,"You need to estimate the proportion of American adults who want to be
vaccinated against Covid-19. You plan to survey a random sample of
American adults, and use the proportion of adults in your sample who
want to be vaccinated as your estimate for the true proportion in the
population. Your estimate must be within 0.04 of the true proportion,
95% of the time. Using the fact that the standard deviation of any
dataset of 0’s and 1’s is no more than 0.5, calculate the minimum number
of people you would need to survey. Input your answer below, as an
integer.","625
Note: Before reviewing these solutions, it’s highly recommended
to revisit the lecture on “Choosing Sample Sizes,” since this problem
follows the main example from that lecture almost exactly.
While this solution is long, keep in mind from the start that our
goal is to solve for the smallest sample size necessary
to create a confidence interval that achieves certain criteria.
The Central Limit Theorem tells us that the distribution of the
sample mean is roughly normal, regardless of the distribution of the
population from which the samples are drawn. At first, it may not be
clear how the Central Limit Theorem is relevant, but remember that
proportions are means too – for instance, the proportion of adults who
want to be vaccinated is equal to the mean of a collection of 1s and 0s,
where we have a 1 for each adult that wants to be vaccinated and a 0 for
each adult who doesn’t want to be vaccinated. What this means (😉) is
that the Central Limit Theorem applies to the distribution of
the sample proportion, so we can use it here too.
Not only do we know that the distribution of sample proportions is
roughly normal, but we know its mean and standard deviation, too:
\begin{align*}
\text{Mean of Distribution of Possible Sample Means} &=
\text{Population Mean} = \text{Population Proportion} \\
\text{SD of Distribution of Possible Sample Means} &=
\frac{\text{Population SD}}{\sqrt{\text{Sample Size}}}
\end{align*}

Using this information, we can create a 95% confidence interval for
the population proportion, using the fact that in a normal distribution,
roughly 95% of values are within 2 standard deviations of the mean:
\left[ \text{Population Proportion} - 2
\cdot \frac{\text{Population SD}}{\sqrt{\text{Sample Size}}}, \:
\text{Population Proportion} + 2 \cdot \frac{\text{Population
SD}}{\sqrt{\text{Sample Size}}}  \right]
However, this interval depends on the population proportion (mean)
and SD, which we don’t know. (If we did know these parameters, there
would be no need to collect a sample!) Instead, we’ll use the sample
proportion and SD as rough estimates:
\left[ \text{Sample Proportion} - 2 \cdot
\frac{\text{Sample SD}}{\sqrt{\text{Sample Size}}}, \: \text{Sample
Proportion} + 2 \cdot \frac{\text{Sample SD}}{\sqrt{\text{Sample
Size}}}  \right]
Note that the width of this interval – that is, its right endpoint
minus its left endpoint – is: \text{width} =
4 \cdot \frac{\text{Sample SD}}{\sqrt{\text{Sample Size}}}
In the problem, we’re told that we want our interval to be accurate
to within 0.04, which is equivalent to wanting the width of our interval
to be less than or equal to 0.08 (since the interval extends the same
amount above and below the sample proportion). As such, we need to pick
the smallest sample size necessary such that:
\text{width} = 4 \cdot \frac{\text{Sample
SD}}{\sqrt{\text{Sample Size}}} \leq 0.08
We can re-arrange the inequality above to solve for our sample’s
size:

\begin{align*}
4 \cdot \frac{\text{Sample SD}}{\sqrt{\text{Sample Size}}} &\leq
0.08 \\
\frac{\text{Sample SD}}{\sqrt{\text{Sample Size}}} &\leq 0.02 \\
\frac{1}{\sqrt{\text{Sample Size}}} &\leq \frac{0.02}{\text{Sample
SD}} \\
\frac{\text{Sample SD}}{0.02} &\leq \sqrt{\text{Sample Size}} \\
\left( \frac{\text{Sample SD}}{0.02} \right)^2 &\leq \text{Sample
Size}
\end{align*}

All we now need to do is pick the smallest sample size that satisfies
the above inequality. But there’s an issue – we don’t know what
our sample SD is, because we haven’t collected our sample!
Notice that in the inequality above, as the sample SD increases, so does
the minimum necessary sample size. In order to ensure we don’t collect
too small of a sample (which would result in the width of our confidence
interval being larger than desired), we can use an upper bound
for the SD of our sample. In the problem, we’re told that the largest
possible SD of a sample of 0s and 1s is 0.5 – this means that if we
replace our sample SD with 0.5, we will find a sample size such that the
width of our confidence interval is guaranteed to be less than or equal
to 0.08. This sample size may be larger than necessary, but that’s
better than it being smaller than necessary.
By substituting 0.5 for the sample SD in the last inequality above,
we get

\begin{align*}
\left( \frac{\text{Sample SD}}{0.02} \right)^2 &\leq \text{Sample
Size} \\\
\left( \frac{0.5}{0.02} \right)^2 &\leq \text{Sample Size} \\
25^2 &\leq \text{Sample Size} \implies \text{Sample Size} \geq 625
\end{align*}

We need to pick the smallest possible sample size that is greater
than or equal to 625; that’s just 625.",40.0,Hard
641,Wi,21,Final,,Problem 13,Problem 13,"Hector earned a 77 on an exam where the mean was 70 and the standard
deviation was 5.
Clara earned an 80 on an exam where the mean was 75 and the standard
deviation was 3.
Vivek earned an 83 on an exam where the mean was a 75 and the
standard deviation was 6.

Rank these three students in ascending order of
their exam performance relative to their classmates.

 Hector, Clara, Vivek
 Vivek, Hector, Clara
 Clara, Hector, Vivek
 Vivek, Clara, Hector","Vivek, Hector, Clara
To compare Vivek, Hector, and Clara’s relative performance we want to
compare their Z scores to handle standardization. For Vivek, his Z score
is (83-75) / 6 = 4/3. For Hector, his score is (77-70) / 5 = 7/5. For
Clara, her score is (80-75) / 3 = 5/3. Ranking these, 5/3 > 7/5 >
4/3 which yields the result of Vivek, Hector, Clara.",76.0,Easy
642,Wi,21,Final,,Problem 14,Problem 14,"The data visualization below shows all Olympic gold medals for
women’s gymnastics, broken down by the age of the gymnast.",,,
643,Wi,21,Final,,Problem 14,Problem 14.1,"Based on this data, rank the following three quantities in
ascending order: the median age at which gold medals
are earned, the mean age at which gold medals are earned, the standard
deviation of the age at which gold medals are earned.

 mean, median, SD
 median, mean, SD
 SD, mean, median
 SD, median, mean","SD, median, mean
The standard deviation will clearly be the smallest of the three
values as most of the data is encompassed between the range of
[14-26]. Intuitively, the standard deviation will have to
be about a third of this range which is around 4 (though this is not the
exact standard deviation, but is clearly much less than the mean and
median with values closer to 19-25). Comparing the median and mean, it
is important to visualize that this distribution is skewed right. When
the data is skewed right it pulls the mean towards a higher value (as
the higher values naturally make the average higher). Therefore, we know
that the mean will be greater than the median and the ranking is SD,
median, mean.",,
644,Wi,21,Final,,Problem 14,Problem 14.2,"Which of the following is larger for this dataset?

 the difference between the 50th percentile of ages and the 25th
percentile of ages
 the difference between the 75th percentile of ages and the 50th
percentile of ages
 both are the same","the difference between the 75th percentile
of ages and the 50th percentile of ages
Since the distribution is right skewed, the 75th percentile will have
a larger difference from the 50th percentile than the 25th percentile.
With right skewness, values above the 50th percentile will be more
different than those smaller than the 50th percentile (and thus more
spread out according to the graph).",78.0,Easy
645,Wi,21,Final,,Problem 15,Problem 15,"In a board game, whenever it is your turn, you roll a six-sided die
and move that number of spaces. You get 10 turns, and you win the game
if you’ve moved 50 spaces in those 10 turns. Suppose you create a
simulation, based on 10,000 trials, to show the distribution of the
number of spaces moved in 10 turns. Let’s call this distribution
Dist10. You also wonder how the game would be
different if you were allowed 15 turns instead of 10, so you create
another simulation, based on 10,000 trials, to show the distribution of
the number of spaces moved in 15 turns, which we’ll call
Dist15",,,
646,Wi,21,Final,,Problem 15,Problem 15.1,"What can we say about the shapes of Dist10 and
Dist15?

 both will be roughly normally distributed
 only one will be roughly normally distributed
 neither will be roughly normally distributed","both will be roughly normally
distributed
By the central limit theorem, both simulations will appear to be
roughly normally distributed.",90.0,Easy
647,Wi,21,Final,,Problem 15,Problem 15.2,"What can we say about the centers of Dist10 and
Dist15?

 both will have approximately the same mean
 the mean of Dist10 will be smaller than the mean
of Dist15
 the mean of Dist15 will be smaller than the mean
of Dist10","the mean of Dist10 will
be smaller than the mean of Dist15
The distribution which moves in 10 turns will have a smaller mean as
there are less turns to move spaces. Therefore, the mean movement from
turns will naturally be higher for the distribution with more turns.",83.0,Easy
648,Wi,21,Final,,Problem 15,Problem 15.3,"What can we say about the spread of Dist10 and
Dist15?

 both will have approximately the same standard deviation
 the standard deviation of Dist10 will be smaller
than the standard deviation of Dist15
 the standard deviation of Dist15 will be smaller
than the standard deviation of Dist10","the standard deviation of
Dist10 will be smaller than the standard deviation
of Dist15
Since taking more turns allows for more possible values, the spread
of Dist10 will be smaller than the standard
deviation of Dist15. (ie. consider the possible
range of values that are attainable for each case)",65.0,Medium
649,Wi,21,Final,,Problem 16,Problem 16,"True or False: The slope of the regression line,
when both variables are measured in standard units, is never more than
1.","True
Standard units standardize the data into z scores. When converting to
Z scores the scale of both the dependent and independent variables are
the same, and consequently, the slope can at most increase by 1.
Alternatively, according to the reference sheet, the slope of the
regression line, when both variables are measured in standard units, is
also equal to the correlation coefficient. And by definition, the
correlation coefficient can never be greater than 1 (since you can’t
have more than a ‘perfect’ correlation).",93.0,Easy
650,Wi,21,Final,,Problem 17,Problem 17,"True or False: The slope of the regression line,
when both variables are measured in original units, is never more than
1.","False
Original units refers to units as they are. Clearly, regression
slopes can be greater than 1 (for example if for every change in 1 unit
of x corresponds to a change in 20 units of y the slope will be 20).",96.0,Easy
651,Wi,21,Final,,Problem 18,Problem 18,"True or False: Suppose that from a sample, you
compute a 95% bootstrapped confidence interval for a population
parameter to be the interval [L, R]. Then the average of L and R is the
mean of the original sample.","False
A 95% confidence interval indicates we are 95% confident that the
true population parameter falls within the interval [L, R]. Note that
the problem specifies that the confidence interval is bootstrapped.
Since the interval is found using bootstrapping, L and R averaged will
not be the mean of the original sample since the mean of the original
sample is not what is used in calculating the bootstrapped confidence
interval. The bootstrapped confidence interval is created by re-sampling
the data with replacement over and over again. Thus, while the interval
is typically centered around the sample mean due to the nature of
bootstrapping, the average of L and R (the 2.5th and 97.5th percentiles
of the distribution of bootstrapped means) may not exactly equal the
sample mean, but should be close to it. Additionally, L is the 2.5th
percentile of the distribution of bootstrapped means and R is the 97.5th
percentile, and these are not necessarily the same distance away from
the mean of the sample.",87.0,Easy
652,Wi,21,Final,,Problem 19,Problem 19,"True or False: Suppose that from a sample, you
compute a 95% normal confidence interval for a population parameter to
be the interval [L, R]. Then the average of L and R is the mean of the
original sample.","True
True, a 95% confidence interval indicates we are 95% confident that
the true population parameter falls within the interval [L, R]. Looking
at how a confidence interval is calculated is by adding/ subtracting a
confidence level value (z) by the standard error. Since the top and
bottom of the interval will be different from the mean by the same
amount, the average will be the mean. (For more information, refer to
the reference sheet)",68.0,Medium
653,Wi,21,Final,,Problem 20,Problem 20,"You order 25 large pizzas from your local pizzeria. The pizzeria
claims that these pizzas are 16 inches in diameter, but you’re not so
sure. You measure each pizza’s diameter and collect a dataset of 25
actual pizza diameters. You want to run a hypothesis test to determine
whether the pizzeria’s claim is accurate.",,,
654,Wi,21,Final,,Problem 20,Problem 20.1,"What would your Null Hypothesis be?
What would your Alternative Hypothesis be?","Null Hypothesis: The mean pizza diameter at the local pizzeria is 16
inches.
Alternative Hypothesis: The mean pizza diameter at the local pizzeria
is not 16 inches.
The null hypothesis is the hypothesis where there is no significant
difference from some statement. In this case, the statement of interest
is the pizzeria’s claims of pizzas are 16 inches in diameter.
The alternative hypothesis is a statement that contradicts the null
hypothesis. In this case this statement is that the mean pizza diameter
at the local pizzeria is not 16 inches. (ie. the other option to the
null hypothesis)",,
655,Wi,21,Final,,Problem 20,Problem 20.2,What test statistic would you use?,"Mean Pizza Diameter, or other valid
statistics such as the (absolute) difference between each pizza’s
diameter and 16 (expected value)
Looking at the null and alternative hypothesis we can see we are
directly interested in the mean pizza diameter, so it is most likely the
best measurement for the test statistic. The main idea is that we
somehow want to show the difference in distribution of the pizza
diameters.",70.0,Medium
656,Wi,21,Final,,Problem 20,Problem 20.3,"Explain how you would do your hypothesis test and how you would draw
a conclusion from your results.","Answers vary, should include the
following

Generate confidence interval for population mean (or equivalent)
by bootstrapping (or by calculating directly with the sample).
Correctly describe how to reject or fail to reject the null
hypothesis (depending on whether interval contains 16, for
example).

When conducting the hypothesis test, we first want to create a
confidence interval either by using bootstrapping or constructing a 95%
confidence interval to understand the true mean diameter of pizzas. The
next step is to define the rejection criteria, failing to reject the
null if 16 is within the 95% confidence interval (since we believe the
true population mean) is within this range with 95% confidence. We will
reject the null hypothesis if 16 is not within the 95% confidence
interval. Note that this assumes you used true mean diameter of pizzas
as your test statistic.",38.0,Hard
657,Wi,21,Final,,Problem 21,Problem 21,"A restaurant keeps track of each table’s number of people (average 3;
standard deviation 1) and the amount of the bill (average $60, standard
deviation $12). If the number of people and amount of the bill are
linearly associated with correlation 0.8, what is the predicted bill for
a table of 5 people? Input your answer below, to the nearest
cent. Make sure your answer is just a number and does not
include the $ symbol or any text.","79.20
To answer this question, first find the z score for a table of 5
people. Z = (5-3)/1 = 2. Now having this Z score, find the price that
correlated in the bill distribution by finding the value for 2 standard
deviations larger than the mean while also accounting for the
correlation between the two variables. This is calculated with mean +
((ZSD)  r) which is 60 + ((12 * 2) * 0.8) = 79.20.
Alternatively, we could solve for the regression line and plug our
values in according to the reference sheet:
m = (0.8) * (12/1) and b = 60 - (48/5) * 3 (where m is the slope and
b is the y-intercept)
Thus plugging the appropriate values in our regression line
yields
y = (48/5) * 5 + 60 - (48/5)*3 = 79.2",88.0,Easy
658,Wi,21,Final,,Problem 22,Problem 22,"From a population with mean 500 and standard deviation 50, you
collect a sample of size 100. The sample has mean 400 and standard
deviation 40. You bootstrap this sample 10,000 times, collecting 10,000
resample means.",,,
659,Wi,21,Final,,Problem 22,Problem 22.1,"Which of the following is the most accurate description of the mean
of the distribution of the 10,000 bootstrapped means?

 The mean will be exactly equal to 400.
 The mean will be exactly equal to 500.
 The mean will be approximately equal to 400.
 The mean will be approximately equal to 500.","The mean will be approximately equal to
400.
The distribution of bootstrapped means’ mean will be approximately
400 since that is the mean of the sample and bootstrapping is taking
many samples of the original sample. The mean will not be exactly 400 do
to some randomness though it will be very close.",54.0,Medium
660,Wi,21,Final,,Problem 22,Problem 22.2,"Which of the following is closest to the standard deviation of the
distribution of the 10,000 bootstrapped means?

 400
 40
 4
 0.4","4
To find the standard deviation of the distribution, we can take the
sample standard deviation S divided by the square root of the sample
size. From plugging in, we get 40 / 10 = 4.",51.0,Medium
661,Wi,21,Final,,Problem 23,Problem 23,"Note: This problem is out of scope; it
covers material no longer included in the course.
Recall the mathematical definition of percentile and how we calculate
it.
Let p be a number between 0 and 100.
The pth percentile of a collection is the smallest
value in the collection that is at least as large as p% of all
the values.
By this definition, any percentile between 0 and 100 can be computed
for any collection of values and is always an element of the collection.
Suppose there are n elements in the collection. To find the
pth percentile:

Sort the collection in increasing order.
Find p% of n: (p/100) n. Call
that h. If h is an integer, define
k=h. Otherwise, let k be the smallest integer
greater than h.
Take the kth element of the sorted collection.

You have a dataset of 7 values, which are
[3, 6, 7, 9, 10, 15, 18]. Using the mathematical definition
of percentile above, find the smallest and largest integer values of p
so that the pth percentile of this dataset corresponds to the value 10.
Input your answers below, as integers between 0 and
100.",,,
662,Wi,21,Final,,Problem 23,Problem 23.1,Smallest = _,"58
From the definition provided in the question, we want all values of
(p/100) * n which will yield an integer larger than 4, but less than or
equal to 5 because we want the 5th element (10) in the dataset. To
approach this problem we can find how many percentiles each piece of
data falls within by taking 100 / 7 which yields around 14.3. Wanting to
find the percentiles for the range of 4 to 5 we can multiple (100/7) by
4 to get our lower bound. (100/7) * 4 = 57.14 which is rounded up to 58
since the 57th percentile belongs to the 4th element while 58 belongs to
the fifth element.",73.0,Medium
663,Wi,21,Final,,Problem 23,Problem 23.2,Largest = _,"71
To find the largest we will take (100/7) * 5 which yields 71.43. We
will round down since the 72th percentile belongs to the sixth element
in the data set. For more information look at the solution above.",74.0,Medium
664,Wi,21,Final,,Problem 24,Problem 24,"Are nonfiction books longer than fiction books?
Choose the best data science tool to help you answer this
question.

 hypothesis testing
 permutation (A/B) testing
 Central Limit Theorem
 regression","permutation (A/B) testing
The question Are nonfiction books longer than fiction books?
is investigating the difference between two underlying populations
(nonfiction books and fiction books). A permutation test is the best
data science tool when investigating differences between two underlying
distributions.",90.0,Easy
665,Wi,21,Final,,Problem 25,Problem 25,"Do people have more friends as they get older?
Choose the best data science tool to help you answer this
question.

 hypothesis testing
 permutation (A/B) testing
 Central Limit Theorem
 regression","regression
The question at hand is investigating two continuous variables (time
and number of friends). Regression is the best data science tool as it
is dealing with two continuous variables and we can understand
correlations between time and the number of friends.",90.0,Easy
666,Wi,21,Final,,Problem 26,Problem 26,"Does an ice cream shop sell more chocolate or vanilla ice cream
cones?
Choose the best data science tool to help you answer this
question.

 hypothesis testing
 permutation (A/B) testing
 Central Limit Theorem
 regression","hypothesis testing
The question at hand is dealing with differences between sales of
different flavors of ice cream, which is the same thing as the total of
ice cream cones sold. We can use hypothesis testing to test our null
hypothesis that the count of Vanilla cones sold is higher than
Chocolate, and our alternative hypothesis that the count of Chocolate
cones sold is more than Vanilla. A permutation test is not suitable here
because we are not comparing any numerical quantity associated with each
group. A permutation test could be used to answer questions like “Are
chocolate ice cream cones more expensive than vanilla ice cream cones?”
or “Do chocolate ice cream cones have more calories than vanilla ice
cream cones?”, or any other question where you are tracking a number
(cost or calories) along with each ice cream cone. In our case, however,
we are not tracking a number along with each individual ice cream cone,
but instead tracking a total of ice cream cones sold.
An analogy to this hypothesis test can be found in the “fair or
unfair coin” problem in Lectures 20 and 21, where our null hypothesis is
that the coin is fair and our alternative hypothesis is that the coin is
unfair. The “fairness” of the coin is not a numerical quantity that we
can track with each individual coin flip, just like how the count of ice
cream cones sold is not a numerical quantity that we can track with each
individual ice cream cone.",57.0,Medium
667,Wi,22,Final,,Problem 1,Problem 1,,,,
668,Wi,22,Final,,Problem 1,Problem 1.1,"What type of visualization is best suited for visualizing the trend
in the number of points Kelsey Plum scored per game in 2021?

 Histogram
 Bar chart
 Line chart
 Scatter plot","Line chart
Here, there are two quantitative variables (number of points and game
number), and one of them involves some element of time (game number).
Line charts are appropriate when one quantitative variable is time.",75.0,Easy
669,Wi,22,Final,,Problem 1,Problem 1.2,"Fill in the blanks below so that total_june evaluates to
the total number of points Kelsey Plum scored in June.
june_only = plum[__(a)__]
total_june = june_only.__(b)__

What goes in blank (a)?
What goes in blank (b)?","plum.get('Date').str.contains('-06-')
get('PTS').sum()

To find the total number of points Kelsey Plum scored in June, one
approach is to first create a DataFrame with only the rows for June.
During the month of June, the 'Date' values contain
'-06-' (since June is the 6th month), so
plum.get('Date').str.contains('-06-') is a Series
containing True only for the June rows and
june_only = plum[plum.get('Date').str.contains('-06-')] is
a DataFrame containing only the June rows.
Then, all we need is the sum of the 'PTS' column, which
is given by june_only.get('PTS').sum().",90.0,Easy
670,Wi,22,Final,,Problem 1,Problem 1.3,"Consider the function unknown, defined below.
def unknown(df):
    grouped = plum.groupby('Opp').max().get(['Date', 'PTS'])
    return np.array(grouped.reset_index().index)[df]
What does unknown(3) evaluate to?

 '2021-06-05'
 'WAS'
 The date on which Kelsey Plum scored the most points
 The three-letter code of the opponent on which Kelsey Plum scored the
most points
 The number 0
 The number 3
 An error","The number 3
plum.groupby('Opp').max() finds the largest value in the
'Date', 'Home', 'Won',
'PTS', 'AST', and 'TOV' columns
for each unique 'Opp' (independently for each column).
grouped = plum.groupby('Opp').max().get(['Date', 'PTS'])
keeps only the 'Date' and 'PTS' columns. Note
that in grouped, the index is 'Opp', the
column we grouped on.
When grouped.reset_index() is called, the index is
switched back to the default of 0, 1, 2, 3, 4, and so on. Then,
grouped.reset_index().index is an Index
containing the numbers [0, 1, 2, 3, 4, ...], and
np.array(grouped.reset_index().index) is
np.array([0, 1, 2, 3, 4, ...]). In this array, the number
at position i is just i, so the number at
position df is df. Here, df is
the argument to unknown, and we were asked for the value of
unknown(3), so the correct answer is the number at position
3 in np.array([0, 1, 2, 3, 4, ...]) which is 3.
Note that if we asked for unknown(50) (or
unknown(k), where k is any integer above 30),
the answer would be “An error”, since grouped could not
have had 51 rows. plum has 31 rows, so grouped
has at most 31 rows (but likely less, since Kelsey Plum’s team likely
played the same opponent multiple times).",72.0,Medium
671,Wi,22,Final,,Problem 1,Problem 1.4,"For your convenience, we show the first few rows of plum
again below.

Suppose that Plum’s team, the Las Vegas Aces, won at least one game
in Las Vegas and lost at least one game in Las Vegas. Also, suppose they
won at least one game in an opponent’s arena and lost at least one game
in an opponent’s arena.
Consider the DataFrame home_won, defined below.
home_won = plum.groupby(['Home', 'Won']).mean().reset_index()

How many rows does home_won have?
How many columns does home_won have?","4 rows and 5 columns.
plum.groupby(['Home', 'Won']).mean() contains one row
for every unique combination of 'Home' and
'Won'. There are two values of 'Home' -
True and False – and two values of
'Won' – True and False – leading
to 4 combinations. We can assume that there was at least one row in
plum for each of these 4 combinations due to the assumption
given in the problem:
Suppose that Plum’s team, the Las Vegas Aces, won at least one
game in Las Vegas and lost at least one game in Las Vegas. Also, suppose
they won at least one game in an opponent’s arena and lost at least one
game in an opponent’s arena.
plum started with 7 columns: 'Date',
'Opp', 'Home', 'Won',
'PTS', 'AST', and 'TOV'. After
grouping by ['Home', 'Won'] and using .mean(),
'Home' and 'Won' become the index. The
resulting DataFrame contains all of the columns that the
.mean() aggregation method can work on. We cannot take the
mean of 'Date' and 'Opp', because those
columns are strings, so
plum.groupby(['Home', 'Won']).mean() contains a
MultiIndex with 2 “columns” – 'Home' and
'Won' – and 3 regular columns – 'PTS'
'AST', and 'TOV'. Then, when using
.reset_index(), 'Home' and 'Won'
are restored as regular columns, meaning that
plum.groupby(['Home', 'Won']).mean().reset_index() has
2 + 3 = 5 columns.",78.0,Easy
672,Wi,22,Final,,Problem 1,Problem 1.5,"Consider the DataFrame home_won once again.
home_won = plum.groupby(['Home', 'Won']).mean().reset_index()
Now consider the DataFrame puzzle, defined below. Note
that the only difference between home_won and
puzzle is the use of .count() instead of
.mean().
puzzle = plum.groupby(['Home', 'Won']).count().reset_index()
How do the number of rows and columns in home_won
compare to the number of rows and columns in puzzle?

 home_won and puzzle have the same number of
rows and columns
 home_won and puzzle have the same number of
rows, but a different number of columns
 home_won and puzzle have the same number of
columns, but a different number of rows
 home_won and puzzle have both a different
number of rows and a different number of columns","home_won and
puzzle have the same number of rows, but a different number
of columns
All that changed between home_won and
puzzle is the aggregation method. The aggregation method
has no influence on the number of rows in the output DataFrame, as there
is still one row for each of the 4 unique combinations of
'Home' and 'Won'.
However, puzzle has 7 columns, instead of 5. In the
solution to the above subpart, we noticed that we could not use
.mean() on the 'Date' and 'Opp'
columns, since they contained strings. However, we can use
.count() (since .count() just determines the
number of non-NA values in each group), and so the 'Date'
and 'Opp' columns are not “lost” when aggregating. Hence,
puzzle has 2 more columns than home_won.",85.0,Easy
673,Wi,22,Final,,Problem 1,Problem 1.6,"For your convenience, we show the first few rows of plum
again below.

There is exactly one team in the WNBA that Plum’s team did not win
any games against during the 2021 season. Fill in the blanks below so
that never_beat evaluates to a string containing the
three-letter code of that team.
never_beat = plum.groupby(__(a)__).sum().__(b)__

What goes in blank (a)?
What goes in blank (b)?","'Opp'
sort_values('Won').index[0]

The key insight here is that the values in the 'Won'
column are Boolean, and when Boolean values are used in arithmetic they
are treated as 1s (True) and 0s (False). The
sum of several 'Won' values is the same as the
number of wins.
If we group plum by 'Opp' and use
.sum(), the resulting 'Won' column contains
the number of wins that Plum’s team had against each unique opponent. If
we sort this DataFrame by 'Won' in increasing order (which
is the default behavior of sort_values), the row at the top
will correspond to the 'Opp' that Plum’s team had no wins
against. Since we grouped by 'Opp', team names are stored
in the index, so .index[0] will give us the name of the
desired team.",67.0,Medium
674,Wi,22,Final,,Problem 1,Problem 1.7,"Recall that plum has 31 rows, one corresponding to each
of the 31 games Kelsey Plum’s team played in the 2021 WNBA season.
Fill in the blank below so that win_bool evaluates to
True.
def modify_series(s):
    return __(a)__

n_wins = plum.get('Won').sum()
win_bool = n_wins == (31 + modify_series(plum.get('Won')))
What goes in blank (a)?

 -s.sum()
 -(s == False).sum()
 len(s) - s.sum()
 not s.sum()
 -s[s.get('Won') == False].sum()","-(s == False).sum()
n_wins equals the number of wins that Plum’s team had.
Recall that her team played 31 games in total. In order for
(31 + modify_series(plum.get('Won'))) to be equal to her
team’s number of wins, modify_series(plum.get('Won')) must
be equal to her team’s number of losses, multiplied by -1.
To see this algebraically, let
modified = modify_series(plum.get('Won')). Then:
31 + \text{modified} = \text{wins}
 \text{modified} = \text{wins} - 31 = -(31 -
\text{wins}) = -(\text{losses})
The function modified_series(s) takes in a Series
containing the wins and losses for each of Plum’s team’s games and needs
to return the number of losses multiplied by -1. s.sum()
returns the number of wins, and (s == False).sum() returns
the number of losses. Then, -(s == False).sum() returns the
number of losses multiplied by -1, as desired.",76.0,Easy
675,Wi,22,Final,,Problem 2,Problem 2,"Let’s suppose there are 4 different types of shots a basketball
player can take – layups, midrange shots, threes, and free throws.
The DataFrame breakdown has 4 rows and 50 columns – one
row for each of the 4 shot types mentioned above, and one column for
each of 50 different players. Each column of breakdown
describes the distribution of shot types for a single player.
The first few columns of breakdown are shown below.

For instance, 30% of Kelsey Plum’s shots are layups, 30% of her shots
are midrange shots, 20% of her shots are threes, and 20% of her shots
are free throws.",,,
676,Wi,22,Final,,Problem 2,Problem 2.1,"Below, we’ve drawn an overlaid bar chart showing the shot
distributions of Kelsey Plum and Chiney Ogwumike, a player on the Los
Angeles Sparks.


What is the total variation distance (TVD) between
Kelsey Plum’s shot distribution and Chiney Ogwumike’s shot distribution?
Give your answer as a proportion between 0 and 1 (not a
percentage) rounded to three decimal places.","0.2
Recall, the TVD is the sum of the absolute differences in
proportions, divided by 2. The absolute differences in proportions for
each category are as follows:

Free Throws: |0.05 - 0.2| =
0.15
Threes: |0.35 - 0.2| = 0.15
Midrange: |0.35 - 0.3| = 0.05
Layups: |0.25 - 0.3| = 0.05

Then, we have
\text{TVD} = \frac{1}{2} (0.15 + 0.15 +
0.05 + 0.05) = 0.2",84.0,Easy
677,Wi,22,Final,,Problem 2,Problem 2.2,"Recall, breakdown has information for 50 different
players. We want to find the player whose shot distribution is the
most similar to Kelsey Plum, i.e. has the lowest TVD
with Kelsey Plum’s shot distribution.
Fill in the blanks below so that most_sim_player
evaluates to the name of the player with the most similar shot
distribution to Kelsey Plum. Assume that the column named
'Kelsey Plum' is the first column in breakdown
(and again that breakdown has 50 columns total).
most_sim_player = ''
lowest_tvd_so_far = __(a)__
other_players = np.array(breakdown.columns).take(__(b)__)
for player in other_players:
    player_tvd = tvd(breakdown.get('Kelsey Plum'),
                     breakdown.get(player))
    if player_tvd < lowest_tvd_so_far:
        lowest_tvd_so_far = player_tvd
        __(c)__

What goes in blank (a)?


 -1
 -0.5
 0
 0.5
 1
 np.array([])
 ''


What goes in blank (b)?
What goes in blank (c)?",,70.0,Medium
678,Wi,22,Final,,Problem 2,Problem 2.3,"Let’s again consider the shot distributions of Kelsey Plum and Cheney
Ogwumike.

We define the maximum squared distance (MSD) between
two categorical distributions as the largest squared difference
between the proportions of any category.
What is the MSD between Kelsey Plum’s shot distribution and Chiney
Ogwumike’s shot distribution? Give your answer as a
proportion between 0 and 1 (not a percentage) rounded
to three decimal places.","0.023
Recall, in the solution to the first subpart of this problem, we
calculated the absolute differences between the proportions of each
category.

Free Throws: |0.05 - 0.2| =
0.15
Threes: |0.35 - 0.2| = 0.15
Midrange: |0.35 - 0.3| = 0.05
Layups: |0.25 - 0.3| = 0.05

The squared differences between the proportions of each category are
computed by squaring the results in the list above (e.g. for Free Throws
we’d have (0.05 - 0.2)^2 = 0.15^2). To
find the maximum squared difference, then, all we need to do is find the
largest of 0.15^2, 0.15^2, 0.05^2, and 0.05^2. Since 0.15
> 0.05, we have that the maximum squared distance is 0.15^2 = 0.0225, which rounds to 0.023.",85.0,Easy
679,Wi,22,Final,,Problem 2,Problem 2.4,"For your convenience, we show the first few columns of
breakdown again below.

In basketball:

layups are worth 2 points,
midrange shots are worth 2 points,
threes are worth 3 points, and
free throws are worth 1 point

Suppose that Kelsey Plum is guaranteed to shoot exactly 10 shots a
game. The type of each shot is drawn from the 'Kelsey Plum'
column of breakdown (meaning that, for example, there is a
30% chance each shot is a layup).
Fill in the blanks below to complete the definition of the function
simulate_points, which simulates the number of points
Kelsey Plum scores in a single game. (simulate_points
should return a single number.)
def simulate_points():
    shots = np.random.multinomial(__(a)__, breakdown.get('Kelsey Plum'))
    possible_points = np.array([2, 2, 3, 1])
    return __(b)__

What goes in blank (a)?
What goes in blank (b)?",,84.0,Easy
680,Wi,22,Final,,Problem 2,Problem 2.5,"True or False: If we call simulate_points() 10,000 times
and plot a histogram of the results, the distribution will look roughly
normal.

 True
 False","True
The answer is True because of the Central Limit Theorem. Recall, the
CLT states that no matter what the population distribution looks like,
if you take many repeated samples with replacement, the distribution of
the sample means and sample sums will be roughly normal.
simulate_points() returns the sum of a sample of size 10
drawn with replacement from a population, and so if we generate many
sample sums, the distribution of those sample sums will be roughly
normal.
The distribution we are drawing from is the one below.




Type
Points
Probability




Layups
2
0.3


Midrange
2
0.3


Threes
3
0.2


Free Throws
1
0.2",78.0,Easy
681,Wi,22,Final,,Problem 3,Problem 3,"ESPN (a large sports news network) states that the Las Vegas Aces
have a 60% chance of winning their upcoming game. You’re curious as to
how they came up with this estimate, and you decide to conduct a
hypothesis test for the following hypotheses:

Null Hypothesis: The Las Vegas Aces win each
game with a probability of 60%.
Alternative Hypothesis: The Las Vegas Aces win
each game with a probability above 60%.

In both hypotheses, we are assuming that each game is independent of
all other games.
In the 2021 season, the Las Vegas Aces won 22 of
their games and lost 9 of their games.",,,
682,Wi,22,Final,,Problem 3,Problem 3.1,"Below, we have provided the code necessary to conduct the hypothesis
test described above.
stats = np.array([])
for i in np.arange(10000):
    sim = np.random.multinomial(31, [0.6, 0.4])
    stat = fn(sim)
    stats = np.append(stats, stat)
win_p_value = np.count_nonzero(stats >= fn([22, 9])) / 10000
fn is a function that computes a test
statistic, given a list or array arr of two elements (the
first of which is the number of wins, and the second of which is the
number of losses). You can assume that neither element of
arr is equal to 0.
Below, we define 5 possible test statistics fn.
Option 1:
def fn(arr):
    return arr[0] / arr[1]
Option 2:
def fn(arr):
    return arr[0]
Option 3:
def fn(arr):
    return np.abs(arr[0] - arr[1])
Option 4:
def fn(arr):
    return arr[0] - arr[1]
Option 5:
def fn(arr):
    return arr[1] - arr[0]
Which of the above functions fn would be valid test
statistics for this hypothesis test and p-value calculation?
Select all that apply.

 Option 1
 Option 2
 Option 3
 Option 4
 Option 5","Options 1, 2, and 4
In the code provided to us, stats is an array containing
10,000 p-values generated by the function fn (note that we
are appending stat to stats, and in the line
before that we have stat = fn(sim)). In the very last line
of the code provided, we have:

win_p_value = np.count_nonzero(stats >= fn([22, 9])) / 10000

If we look closely, we see that we are computing the p-value by
computing the proportion of simulated test statistics that were
greater than or equal to (>=) the observed
statistic. Since a p-value is computed as the proportion of
simulated test statistics that were as or more extreme than the
observed statistic, here it must mean that “big” test
statistics are more extreme.
Remember, the direction that is “extreme” is determined by our
alternative hypothesis. Here, the alternative hypothesis is that the Las
Vegas Aces win each game with a probability above 60%. As such, the test
statistic(s) we choose must be large when the probability that
the Aces win a game is high, and small when the probability
that the Aces win a game is low. With this all in mind, we can take a
look at the 5 options, remembering that arr[0] is
the number of simulated wins and arr[1] is the number of
simulated losses in a season of 31 games. This means that when
the Aces win more than they lose, arr[0] > arr[1], and
when they lose more than they win, arr[0] < arr[1].

Option 1: Here, our test statistic is the ratio of
wins to losses, i.e. arr[0] / arr[1]. If the Aces win a
lot, the numerator will be larger than the denominator, so this ratio
will be large. If the Aces lose a lot, the numerator will be smaller
than the denominator, and so this ratio will be small. This is what we
want!
Option 2: Here, our test statistic is the number of
wins, i.e. arr[0]. If the Aces win a lot, this number will
be large, and if the Aces lose a lot, this number will be small. This is
what we want!
Option 3: Here, our test statistic is the absolute
value of the number of wins minus the number of losses,
i.e. np.abs(arr[0] - arr[1]). If the Aces win a lot, then
arr[0] - arr[1] will be large, and so will
np.abs(arr[0] - arr[1]). This seems fine.
However, if the Aces lose a lot, then
arr[0] - arr[1] will be small (negative), but
np.abs(arr[0] - arr[1]) will still be large and positive.
This test statistic doesn’t allow us to differentiate
when the Aces win a lot or lose a lot, so we can’t use it as a test
statistic for our alternative hypothesis.
Option 4: From the explanation of Option 3, we know
that when the Aces win a lot, arr[0] - arr[1] is large.
Furthermore, when the Aces lose a lot, arr[0] - arr[1] is
small (negative numbers are small in this context). This works!
Option 5: arr[1] - arr[0] is the
opposite of arr[0] - arr[1] in Option 4. When the Aces win
a lot, arr[1] - arr[0] is small (negative), and when the
Aces lose a lot, arr[1] - arr[0] is large (positive). This
is the opposite of what we want, so Option 5 does not work.",77.0,Easy
683,Wi,22,Final,,Problem 3,Problem 3.2,"The empirical distribution of one of the 5 test statistics presented
in the previous subpart is shown below. To draw the histogram, we used
the argument bins=np.arange(-10, 25).

Which test statistic does the above empirical distribution belong
to?

 Option 1
 Option 2
 Option 3
 Option 4
 Option 5","Option 4
The distribution visualized in the histogram has the following unique
values: -9, -7, -5, -3, …, 17, 19, 21, 23. Crucially, the test statistic
whose distribution we’ve visualized can both be positive and negative.
Right off the bat, we can eliminate Options 1, 2, and 3:

Option 1: Invalid. Option 1 is computed by dividing
the number of wins (arr[0]) by the number of losses
(arr[1]), and that quotient will always be a non-negative
number.
Option 2: Invalid, since the number of wins
(arr[0]) will always be a non-negative number.
Option 3: Invalid, since the absolute value of any
real number (np.abs(arr[0] - arr[1]), in this case) will
always be a non-negative number.

Now, we must decide between Option 4, whose test statistic is “wins
minus losses” (arr[0] - arr[1]), and Option 5, whose test
statistic is “losses minus wins” (arr[1] - arr[0]).
First, let’s recap how we’re simulating. In the code
provided in the previous subpart, we have the line
sim = np.random.multinomial(31, [0.6, 0.4]). Each time we
run this line, sim will be set to an array with two
elements, the first of which we interpret as the number of simulated
wins and the second of which we interpret as the number of simulated
losses in a 31 game season. The first number in sim will
usually be larger than the second number in sim, since the
chance of a win (0.6) is larger than the chance of a loss (0.4). As
such, When we compute fn(sim) in the following line, the
difference between the wins and losses should typically be positive.
Back to our distribution. Note that the distribution provided in this
subpart is centered at a positive number, around 7.
Since the difference between wins and losses will typically be positive,
it appears that we’ve visualized the distribution of the difference
between wins and losses (Option 4). If we instead visualized the
difference between losses and wins, the distribution should be centered
at a negative number, but that’s not the case.
As such, the correct answer is Option 4.",86.0,Easy
684,Wi,22,Final,,Problem 3,Problem 3.3,"Consider the function fn_plus defined below.
def fn_plus(arr):
    return fn(arr) + 31
True or False: If fn is a valid test
statistic for the hypothesis test and p-value calculation presented at
the start of the problem, then fn_plus is also a valid test
statistic for the hypothesis test and p-value calculation presented at
the start of the problem.

 True
 False","True
All fn_plus is doing is adding 31 to the output of
fn. If we think in terms of pictures, the shape of
the distribution of fn_plus looks the same as the
distribution of fn, just moved to the right by 31 units.
Since the distribution’s shape is no different, the proportion of
simulated test statistics that are greater than the observed test
statistic is no different either, and so the p-value we calculate with
fn_plus is the same as the one we calculate with
fn.",73.0,Medium
685,Wi,22,Final,,Problem 3,Problem 3.4,"Below, we present the same code that is given at the start of the
problem. (Remember to keep the data description from the top of the exam
open in another tab!)
stats = np.array([])
for i in np.arange(10000):
    sim = np.random.multinomial(31, [0.6, 0.4])
    stat = fn(sim)
    stats = np.append(stats, stat)

win_p_value = np.count_nonzero(stats >= fn([22, 9])) / 10000
Below are four possible replacements for the line
sim = np.random.multinomial(31, [0.6, 0.4]).
Option 1:
def with_rep():
    won = plum.get('Won')
    return np.count_nonzero(np.random.choice(won, 31, replace=True))

sim = [with_rep(), 31 - with_rep()]
Option 2:
def with_rep():
    won = plum.get('Won')
    return np.count_nonzero(np.random.choice(won, 31, replace=True))

w = with_rep()
sim = [w, 31 - w]
Option 3:
def without_rep():
    won = plum.get('Won')
    return np.count_nonzero(np.random.choice(won, 31, replace=False))

sim = [without_rep(), 31 - without_rep()]
Option 4:
def perm():
    won = plum.get('Won')
    return np.count_nonzero(np.random.permutation(won))

w = perm()
sim = [w, 31 - w]
Which of the above four options could we replace the line
sim = np.random.multinomial(plum.shape[0], [0.6, 0.4]) with
and still perform a valid hypothesis test for the hypotheses stated at
the start of the problem?

 Option 1
 Option 2
 Option 3
 Option 4","Option 2
The line
sim = np.random.multinomial(plum.shape[0], [0.6, 0.4])
assigns sim to an array containing two numbers such
that:

The numbers are randomly chosen each time the line is run
The numbers always add up to 31

We need to select an option that also creates such an array (or list,
in this case). Note that won = plum.get('Won'), a line that
is common to all four options, assigns won to a Series with
31 elements, each of which is either True or
False (corresponding to the wins and losses that the Las
Vegas Aces earned in their season).
Let’s take a look at the line
np.count_nonzero(np.random.choice(won, 31, replace=True)),
common to the first two options. Here, we are randomly selecting 31
elements from the Series won, with replacement, and
counting the number of Trues (since with
np.count_nonzero, False is counted as
0). Since we are making our selections with replacement,
each selected element has a \frac{22}{31} chance of being
True and a \frac{9}{31}
chance of being False (since won has 22
Trues and 9 Falses). As such,
np.count_nonzero(np.random.choice(won, 31, replace=True))
can be any integer between 0 and 31, inclusive.
Note that if we select without replacement
(replace=False) as Option 3 would like us to, then all 31
selected elements would be the same as the 31 elements in
won. As a result,
np.random.choice(won, 31, replace=False) will always have
22 Trues, just like won, and
np.count_nonzero(np.random.choice(won, 31, replace=True))
will always return 22. That’s not random, and so that’s not quite what
we’re looking for.
With this all in mind, let’s look at the four options.

Option 1: Here, each time we call
with_rep(), we get a random number between 0 and 31
(inclusive), corresponding to the (random) number of simulated wins.
Then, we are assigning sim to be
[with_rep(), 31 - with_rep()]. However, it’s not guaranteed
that the two calls to with_rep return the same number of
wins, so it’s not guaranteed that sum(sim) is 31. Option 1,
then, is invalid.
Option 2: Correct, as we’ll explain below.
Option 3: As mentioned above, Option 3 uses
replace=False, and so without_rep() is always
22 and sim is always [22, 9]. The outcome is
not random.
Option 4: Here, perm() always returns
the same number, 22. This is because all we are doing is shuffling the
entries in the won Series, but we aren’t changing the
number of wins (Trues) and losses (Falses). As
a result, w is always 22 and sim is always
[22, 9], making this non-random, just like in Option
3.

By the process of elimination, Option 2 must be the
correct choice. It is similar to Option 1, but it only calls
with_rep once and “saves” the result to the name
w. As a result, w is random, and
w and 31 - w are guaranteed to sum to 31.
⚠️ Note: It turns out that none of these options run
a valid hypothesis test, since the null hypothesis was that the Las
Vegas Aces win 60% of their games but none of these simulation
strategies use 60% anywhere (instead, they use the observation that the
Aces actually won 22 games). However, this subpart was about the
sampling strategies themselves, so this mistake from our end doesn’t
invalidate the problem.",70.0,Medium
686,Wi,22,Final,,Problem 3,Problem 3.5,"Consider again the four options presented in the previous
subpart.
In which of the four options is it guaranteed that
sum(sim) evaluates to 31? Select all that
apply.

 Option 1
 Option 2
 Option 3
 Option 4",,72.0,Medium
687,Wi,22,Final,,Problem 4,Problem 4,"Consider the definition of the function
diff_in_group_means:
def diff_in_group_means(df, group_col, num_col):
    s = df.groupby(group_col).mean().get(num_col)
    return s.loc[False] - s.loc[True]",,,
688,Wi,22,Final,,Problem 4,Problem 4.1,"It turns out that Kelsey Plum averages 0.61 more assists in games
that she wins (“winning games”) than in games that she loses (“losing
games”). Fill in the blanks below so that observed_diff
evaluates to -0.61.
observed_diff = diff_in_group_means(plum, __(a)__, __(b)__)

What goes in blank (a)?
What goes in blank (b)?",,94.0,Easy
689,Wi,22,Final,,Problem 4,Problem 4.2,"After observing that Kelsey Plum averages more assists in winning
games than in losing games, we become interested in conducting a
permutation test for the following hypotheses:

Null Hypothesis: The number of assists Kelsey Plum
makes in winning games and in losing games come from the same
distribution.
Alternative Hypothesis: The number of assists
Kelsey Plum makes in winning games is higher on average than the number
of assists that she makes in losing games.

To conduct our permutation test, we place the following code in a
for-loop.

won = plum.get('Won')
ast = plum.get('AST')
shuffled = plum.assign(Won_shuffled=np.random.permutation(won)) \
               .assign(AST_shuffled=np.random.permutation(ast))
Which of the following options does not compute a
valid simulated test statistic for this permutation test?

 diff_in_group_means(shuffled, 'Won', 'AST')
 diff_in_group_means(shuffled, 'Won', 'AST_shuffled')
 diff_in_group_means(shuffled, 'Won_shuffled, 'AST')
 diff_in_group_means(shuffled, 'Won_shuffled, 'AST_shuffled')
 More than one of these options do not compute a valid simulated test
statistic for this permutation test","diff_in_group_means(shuffled, 'Won', 'AST')
As we saw in the previous subpart,
diff_in_group_means(shuffled, 'Won', 'AST') computes the
observed test statistic, which is -0.61. There is no randomness involved
in the observed test statistic; each time we run the line
diff_in_group_means(shuffled, 'Won', 'AST') we will see the
same result, so this cannot be used for simulation.
To perform a permutation test here, we need to simulate under the
null by randomly assigning assist counts to groups; here, the groups are
“win” and “loss”.

Option 2: Here, assist counts are shuffled and the
group names are kept in the same order. The end result is a random
pairing of assists to groups.
Option 3: Here, the group names are shuffled and
the assist counts are kept in the same order. The end result is a random
pairing of assist counts to groups.
Option 4: Here, both the group names and assist
counts are shuffled, but the end result is still the same as in the
previous two options.

As such, Options 2 through 4 are all valid, and Option 1 is the only
invalid one.",68.0,Medium
690,Wi,22,Final,,Problem 4,Problem 4.3,"Suppose we generate 10,000 simulated test statistics, using one of
the valid options from Question 4.2. The empirical distribution of test
statistics, with a red line at observed_diff, is shown
below.

Roughly one-quarter of the area of the histogram above is to the left
of the red line. What is the correct interpretation of this result?

 There is roughly a one quarter probability that Kelsey Plum’s number
of assists in winning games and in losing games come from the same
distribution.
 The significance level of this hypothesis test is roughly a
quarter.
 Under the assumption that Kelsey Plum’s number of assists in winning
games and in losing games come from the same distribution, and that she
wins 22 of the 31 games she plays, the chance of her averaging
at least 0.61 more assists in wins than losses is
roughly a quarter.
 Under the assumption that Kelsey Plum’s number of assists in winning
games and in losing games come from the same distribution, and that she
wins 22 of the 31 games she plays, the chance of her averaging 0.61 more
assists in wins than losses is roughly a quarter.","Under the assumption that Kelsey Plum’s
number of assists in winning games and in losing games come from the
same distribution, and that she wins 22 of the 31 games she plays, the
chance of her averaging at least 0.61 more assists in
wins than losses is roughly a quarter. (Option 3)
First, we should note that the area to the left of the red line (a
quarter) is the p-value of our hypothesis test. Generally, the p-value
is the probability of observing an outcome as or more extreme than the
observed, under the assumption that the null hypothesis is true. The
direction to look in depends on the alternate hypothesis; here, since
our alternative hypothesis is that the number of assists Kelsey Plum
makes in winning games is higher on average than in losing games, a
“more extreme” outcome is where the assists in winning games are higher
than in losing games, i.e. where \text{(assists in wins)} - \text{(assists in
losses)} is positive or where \text{(assists in losses)} - \text{(assists in
wins)} is negative. As mentioned in the solution to the first
subpart, our test statistic is \text{(assists
in losses)} - \text{(assists in wins)}, so a more extreme outcome
is one where this is negative, i.e. to the left of the observed
statistic.
Let’s first rule out the first two options.

Option 1: This option states that the probability
that the null hypothesis (the number of assists she makes in winning and
losing games comes from the same distribution) is true is roughly a
quarter. However, the p-value is not the probability
that the null hypothesis is true.
Option 2: The significance level is the formal name
for the p-value “cutoff” that we specify in our hypothesis test. There
is no cutoff mentioned in the problem. The observed
significance level is another name for the p-value, but Option 2 did not
contain the word observed.

Now, the only difference between Options 3 and 4 is the inclusion of
“at least” in Option 3. Remember, to compute a p-value we must compute
the probability of observing something as or more
extreme than the observed, under the null. The “or more” corresponds to
“at least” in Option 3. As such, Option 3 is the correct choice.",70.0,Medium
691,Wi,22,Final,,Problem 4,Problem 4.4,"True or False: The histogram drawn in the previous
subpart is a density histogram.

 True
 False","False
The area of a density histogram is 1. The area of the histogram drawn
in the previous subpart is much larger than 1. In fact, the area of this
histogram is in the hundreds or thousands; you can draw a rectangle
stretching from -1 to 1 on the x-axis
and 0 to 300 on the y-axis that has
area 2 \cdot 300 = 600, and this
rectangle is much smaller than the larger histogram.",82.0,Easy
692,Wi,22,Final,,Problem 5,Problem 5,"Recall, plum has 31 rows.
Consider the function df_choice, defined below.
def df_choice(df):
    return df[np.random.choice([True, False], df.shape[0], replace=True)]",,,
693,Wi,22,Final,,Problem 5,Problem 5.1,"Suppose we call df_choice(plum) once. What is the
probability that the result is an empty DataFrame?

 0
 1
 \frac{1}{2^{25}}
 \frac{1}{2^{30}}
 \frac{1}{2^{31}}
 \frac{2^{31} - 1}{2^{31}}
 \frac{31}{2^{30}}
 \frac{31}{2^{31}}
 None of the above","\frac{1}{2^{31}}
First, let’s understand what df_choice does. It takes in
one input, df. The line
np.random.choice([True, False], df.shape[0], replace=True)
evaluates to an array such that:

There is one element for every row in the input DataFrame or array
df (so if df has 31 rows, the output array
will have length 31)
Each element is equally likely to be True or
False, since the sequence we are selecting from is
[True, False] and we are selecting with replacement

So
np.random.choice([True, False], df.shape[0], replace=True)
is an array the same length as df, with each element
randomly set to True or False. Note that there
is a \frac{1}{2} chance the first
element is True, a \frac{1}{2} chance the second element is
True, and so on.
Then,
df[np.random.choice([True, False], df.shape[0], replace=True)]
is using Boolean indexing to keep only the rows in df where
the array
np.random.choice([True, False], df.shape[0], replace=True)
contains the value True. So, the function
df_choice returns a DataFrame containing
somewhere between 0 and df.shape[0] rows. Note that there
is a \frac{1}{2} chance that the new
DataFrame contains the first row from df, a \frac{1}{2} chance that the new DataFrame
contains the second row from df, and so on.
In this question, the only input ever given to df_choice
is plum, which has 31 rows.

In this subpart, we’re asked for the probability that
df_choice(plum) is an empty DataFrame. There are 31 rows,
and each of them have a \frac{1}{2}
chance of being included in the output, and so a \frac{1}{2} chance of being missing. So, the
chance that they are all missing is:
\begin{aligned} P(\text{empty DataFrame})
&= P(\text{all rows missing}) \\ &= P(\text{row 0 missing and
row 1 missing and ... and row 30 missing}) \\ &= P(\text{row 0
missing}) \cdot P(\text{row 1 missing}) \cdot ... \cdot P(\text{row 30
missing}) \\ &= \frac{1}{2} \cdot \frac{1}{2} \cdot ... \cdot
\frac{1}{2} \\ &= \boxed{\frac{1}{2^{31}}} \end{aligned}",83.0,Easy
694,Wi,22,Final,,Problem 5,Problem 5.2,"Suppose we call df_choice(plum) once. What is the
probability that the result is a DataFrame with 30 rows?

 0
 1
 \frac{1}{2^{25}}
 \frac{1}{2^{30}}
 \frac{1}{2^{31}}
 \frac{2^{31} - 1}{2^{31}}
 \frac{31}{2^{30}}
 \frac{31}{2^{31}}
 None of the above","\frac{31}{2^{31}}
In order for the resulting DataFrame to have 30 rows, exactly 1 row
must be missing, and the other 30 must be present.
To start, let’s consider one row in particular, say, row 7. The
probability that row 7 is missing is \frac{1}{2}, and the probability that rows 0
through 6 and 8 through 30 are all present is \frac{1}{2} \cdot \frac{1}{2} \cdot ... \cdot
\frac{1}{2} = \frac{1}{2^{30}} using the logic from the previous
subpart. So, the probability that row 7 is missing AND all other rows
are present is \frac{1}{2} \cdot
\frac{1}{2^{30}} = \frac{1}{2^{31}}.
Then, in order for there to be 30 rows, either row 0 must be missing,
or row 1 must be missing, and so on:
\begin{aligned} P(\text{exactly one row
missing}) &= P(\text{only row 0 is missing or only row 1 is missing
or ... or only row 30 is missing}) \\ &= P(\text{only row 0 is
missing}) + P(\text{only row 1 is missing}) + ... + P(\text{only row 30
is missing}) \\ &= \frac{1}{2^{31}} + \frac{1}{2^{31}} + ... +
\frac{1}{2^{31}} \\ &=
\boxed{\frac{31}{2^{31}}}  \end{aligned}",48.0,Hard
695,Wi,22,Final,,Problem 5,Problem 5.3,"Suppose we call df_choice(plum) once.
True or False: The probability that the result is a
DataFrame that consists of just row 0 from
plum (and no other rows) is equal to the probability you
computed in the first subpart of this problem.

 True
 False","True
An important realization to make is that all subsets
of the rows in plum are equally likely to be returned by
df_choice(plum), and they all have probability \frac{1}{2^{31}}. For instance, one subset of
plum is the subset where rows 2, 5, 8, and 30 are missing,
and the rest are all present. The probability that this subset is
returned by df_choice(plum) is \frac{1}{2^{31}}.
This is true because for each individual row, the probability that it
is present or missing is the same – \frac{1}{2} – so the probability of any
subset is a product of 31 \frac{1}{2}s,
which is \frac{1}{2^{31}}. (The answer
to the previous subpart was not \frac{1}{2^{31}} because it was asking about
multiple subsets – the subset where only row 0 was missing, and the
subset where only row 1 was missing, and so on).
So, the probability that df_choice(plum) consists of
just row 0 is \frac{1}{2^{31}}, and
this is the same as the answer to the first subpart (\frac{1}{2^{31}}); in both situations, we are
calculating the probability of one specific subset.",63.0,Medium
696,Wi,22,Final,,Problem 5,Problem 5.4,"Suppose we call df_choice(plum) once.
What is the probability that the resulting DataFrame has 0 rows, or 1
row, or 30 rows, or 31 rows?

 0
 1
 \frac{1}{2^{25}}
 \frac{1}{2^{30}}
 \frac{1}{2^{31}}
 \frac{2^{31} - 1}{2^{31}}
 \frac{31}{2^{30}}
 \frac{31}{2^{31}}
 None of the above","\frac{1}{2^{25}}
Here, we’re not being asked for the probability of one specific
subset (like the subset containing just row 0); rather, we’re being
asked for the probability of various different subsets, so our
calculation will be a bit more involved.
We can break our problem down into four pieces. We can find the
probability that there are 0 rows, 1 row, 30 rows, and 31 rows
individually, and add these probabilities up, since only one of them can
happen at a time (it’s impossible for a DataFrame to have both 1 and 30
rows at the same time; these events are “mutually exclusive”). It turns
out we’ve already calculated two of these probabilities:

From the first subpart, the probability that 0 rows are returned is
\frac{1}{2^{31}}. This corresponds to a
single subset, the subset where all rows are missing.
From the second subpart, the probability that 30 rows are returned
is \frac{31}{2^{31}}.

The other two possibilities are symmetric with the above
two!

The probability that 31 rows are returned is the same as the
probability that 0 rows are returned, since the probability of a row
being missing and a row being present is the same. This is \frac{1}{2^{31}}.
The probability that 1 row is returned is the same as the
probability that 1 row is missing, i.e. the probability that 30 rows are
returned. This, from the second subpart, is \frac{31}{2^{31}}.

Putting it all together, we have:
\begin{aligned} P(\text{number of returned
rows is 0, 1, 30, or 31}) &= P(\text{0 rows are returned}) +
P(\text{1 row is returned}) + P(\text{30 rows are returned}) +
P(\text{31 rows are returned}) \\ &= \frac{1}{2^{31}} +
\frac{31}{2^{31}} + \frac{31}{2^{31}} + \frac{1}{2^{31}} \\ &=
\frac{1 + 31 + 31 + 1}{2^{31}} \\ &= \frac{64}{2^{31}} \\ &=
\frac{2^6}{2^{31}} \\ &= \frac{1}{2^{31 - 6}} \\ &=
\boxed{\frac{1}{2^{25}}} \end{aligned}",35.0,Hard
697,Wi,22,Final,,Problem 6,Problem 6,"In addition to the plum DataFrame, we also have access
to the season DataFrame, which contains statistics on all
players in the WNBA in the 2021 season. The first few rows of
season are shown below. (Remember to keep the data
description from the top of the exam open in another tab!)

Each row in season corresponds to a single player. For
each player, we have: - 'Player' (str), their
name - 'Team' (str), the three-letter code of
the team they play on - 'G' (int), the number
of games they played in the 2021 season - 'PPG'
(float), the number of points they scored per game played -
'APG' (float), the number of assists (passes)
they made per game played - 'TPG' (float), the
number of turnovers they made per game played
Note that all of the numerical columns in season must
contain values that are greater than or equal to 0.",,,
698,Wi,22,Final,,Problem 6,Problem 6.1,"Which of the following is the best choice for the index of
season?

 'Player'
 'Team'
 'G'
 'PPG'","'Player'
Ideally, the index of a DataFrame is unique, so that we can use it to
“identify” the rows. Here, each row is about a player, so
'Player' should be the index. 'Player' is the
only column that is likely to be unique; it is possible that two players
have the same name, but it’s still a better choice of index
than the other three options, which are definitely not unique.",95.0,Easy
699,Wi,22,Final,,Problem 6,Problem 6.2,"Note: For the rest of the exam, assume that the
index of season is still 0, 1, 2, 3, …
Below is a histogram showing the distribution of the number of
turnovers per game for all players in season.

Suppose, throughout this question, that the mean
number of turnovers per game is 1.25. Which of the following is closest
to the median number of turnovers per game?

 0.5
 0.75
 1
 1.25
 1.5
 1.75","1
The median of a distribution is the value that is “halfway” through
the distribution, i.e. the value such that half of the values in the
distribution are larger than it and half the values in the distribution
are smaller than it.
Visually, we’re looking for the location on the x-axis where we can draw a vertical line that
splits the area of the histogram in half. While it’s impossible to tell
the exact median of the distribution, since we don’t know how the values
are distributed within the bars, we can get pretty close by using this
principle.
Immediately, we can rule out 0.5, 0.75, 1.5, and 1.75, since they are
too far from the “center” of the distribution (imagine drawing vertical
lines at any of those points on the x-axis; they don’t split the distribution’s
area in half). To decide between 1 and 1.25, we can use the fact that
the distribution is right-skewed, meaning that its mean is
larger than its median (intuitively, the mean is dragged in the
direction of the tail, which is to the right). This means that the
median should be less than the mean. We are given that the mean of the
distribution is 1.25, so the median should be 1.",73.0,Medium
700,Wi,22,Final,,Problem 6,Problem 6.3,"Sabrina Ionescu and Sami Whitcomb are both players on the New York
Liberty, and are both California natives.
In “original units”, Sabrina Ionescu had 3.5 turnovers per game. In
standard units, her turnovers per game is 3.
In standard units, Sami Whitcomb’s turnovers per game is -1. How many
turnovers per game did Sami Whitcomb have in original
units? Round your answer to 3 decimal places.
Note: You will need the fact from the previous
subpart that the mean number of turnovers per game is 1.25.","0.5
To convert a value x to standard
units (denoted by x_{\text{su}}), we
use the following formula:
x_{\text{su}} = \frac{x - \text{mean of
}x}{\text{SD of }x}
Let’s look at the first line given to us: In “original units”,
Sabrina Ionescu had 3.5 turnovers per game. In standard units, her
turnovers per game is 3.
Substituting the information we know into the above equation gives
us:
3 = \frac{3.5 - 1.25}{\text{SD of
}x}
In order to convert future values from original units to standard
units, we’ll need to know \text{SD of
}x, which we don’t currently but can obtain by rearranging the
above equation. Doing so yields
\text{SD of }x = \frac{3.5-1.25}{3} =
\frac{2.25}{3} = 0.75
Now, let’s look at the second line we’re given: In standard
units, Sami Whitcomb’s turnovers per game is -1. How many turnovers per
game did Sami Whitcomb have in original units? Round
your answer to 3 decimal places.
We have all the information we need to convert Sami Whitcomb’s
turnovers per game from standard units to original units! Plugging in
the values we know gives us:
\begin{aligned} x_{\text{su}} &=
\frac{x - \text{mean of }x}{\text{SD of }x} \\ -1 &= \frac{x -
1.25}{0.75} \\ -0.75 &= x - 1.25 \\ 1.25 - 0.75 &= x \\ x &=
\boxed{0.5} \end{aligned}
Thus, in original units, Sami Whitcomb averaged 0.5 turnovers per
game.",87.0,Easy
701,Wi,22,Final,,Problem 6,Problem 6.4,"What is the smallest possible number of turnovers
per game, in standard units? Round your answer to 3
decimal places.","-1.667
The smallest possible number of turnovers per game in original units
is 0 (which a player would have if they never had a turnover – that
would mean they’re really good!). To find the smallest possible
turnovers per game in standard units, all we need to do is convert 0
from original units to standard units. This will involve our work from
the previous subpart.
\begin{aligned} x_{\text{su}} &=
\frac{x - \text{mean of }x}{\text{SD of }x} \\ &= \frac{0 -
1.25}{0.75} \\ &= -\frac{1.25}{0.75} \\ &= -\frac{5}{3} =
\boxed{-1.667} \end{aligned}",82.0,Easy
702,Wi,22,Final,,Problem 7,Problem 7,"Let’s switch our attention to the relationship between the number of
points per game and the number of assists per game for all players in
season. Using season, we compute the following
information:

The mean points per game is 7, with a standard deviation of 5
The mean number of assists per game is 1.5, with a standard
deviation of 1.5
The correlation between points per game and assists per game is
0.65",,,
703,Wi,22,Final,,Problem 7,Problem 7.1,"Let’s start by using points per game (x) to predict assists per game (y).
Tina Charles had 27 points per game in 2021, the most of any player
in the WNBA. What is her predicted assists per game, according to the
regression line? Round your answer to 3 decimal places.","5.4
We need to find and use the regression line to find the predicted
y for an x of 27. There are two ways to proceed:

Use the regression line in standard units. To do this, we’d need to
convert 27 from original units to standard units, use the regression
line y_\text{su} = r \cdot x_\text{su},
and convert the output back to original units.
Use the regression line in original units. To do this, we’d need to
find the slope m and intercept b in the regression line y = mx + b, using the formulas m = r \cdot \frac{\text{SD of }y }{\text{SD of
}x} and b = \text{mean of }y - m \cdot
\text{mean of }x.

Both solutions work; for the sake of completeness, we’ll show both.
Recall, r is the correlation
coefficient between x and y, which we are told is 0.65.
Solution 1:
First, we need to convert 27 points per game to standard units. Doing
so yields
x_{\text{su}} = \frac{x - \text{mean of
}x}{\text{SD of }x} = \frac{27 - 7}{5} = 4
Per the regression line, y_\text{su} = r
\cdot x_\text{su}, we have y_\text{su}
= 0.65 \cdot 4 = 2.6, which is Tina Charles’ predicted assists
per game in standard units. All that’s left is to convert this value
back to original units:
\begin{aligned} y_{\text{su}} &=
\frac{y - \text{mean of }y}{\text{SD of }y} \\ 2.6 &= \frac{y -
1.5}{1.5} \\ 2.6 \cdot 1.5 + 1.5 &= y \\ y &= \boxed{5.4}
\end{aligned}
So, the regression line predicts Tina Charles will have 5.4 assists
per game (in original units).

Solution 2:
First, we need to find the slope m
and intercept b:
m = r \cdot \frac{\text{SD of }y
}{\text{SD of }x} = 0.65 \cdot \frac{1.5}{5} = 0.195
b = \text{mean of }y - m \cdot \text{mean
of }x = 1.5 - 0.195 \cdot 7 = 0.135
Then,
y = mx + b \implies y = 0.195 \cdot 27 +
0.135 = \boxed{5.4}
So, once again, the regression line predicts Tina Charles will have
5.4 assists per game.
Note: The numbers in this problem may seem ugly, but
students taking this exam had access to calculators since this exam was
online. It also turns out that the numbers were easier to work with in
Solution 1 over Solution 2; this was intentional.",81.0,Easy
704,Wi,22,Final,,Problem 7,Problem 7.2,"Tina Charles actually had 2.1 assists per game in the 2021
season.
What is the error, or residual, for the prediction in the previous
subpart? Round your answer to 3 decimal places.","-3.3
Residuals are defined as follows:
\text{residual} = \text{actual } y -
\text{predicted }y
2.1 - 5.4 = -3.3, which gives us our
answer.
Note: Many students answered 3.3. Pay attention to
the order of the calculation!",82.0,Easy
705,Wi,22,Final,,Problem 7,Problem 7.3,"Select all true statements below regarding the regression line
between points per game (x) and assists
per game (y).

 The point (0, 0) is guaranteed to be on the regression line when both
x and y are in standard units.
 The point (0, 0) is guaranteed to be on the regression line when both
x and y are in original units.
 The point (7, 1.5) is guaranteed to be on the regression line when
both x and y are in standard units.
 The point (7, 1.5) is guaranteed to be on the regression line when
both x and y are in original units.
 None of the above",,87.0,Easy
706,Wi,22,Final,,Problem 7,Problem 7.4,"So far, we’ve been using points per game (x) to predict assists per game (y). Suppose we found the regression line
(when both x and y are in original units) to be y = ax + b.
Now, let’s reverse x and y. That is, we will now use assists per game
(x) to predict points per game (y). The resulting regression line (when both
x and y are in original units) is y = cx + d.
Which of the following statements is guaranteed to be true?

 a = c
 a > c
 a < c
 Using just the information given in this problem, it is impossible to
determine the relationship between a
and c.","a <
c
The formula for the slope of the regression line is m = r \cdot \frac{\text{SD of }y}{\text{SD of
}x}. Note that the correlation coefficient r is symmetric, meaning that the correlation
between x and y is the same as the correlation between
y and x.
In the two regression lines mentioned in this problem, we have
\begin{aligned} a &= r \cdot
\frac{\text{SD of assists per game}}{\text{SD of points per game}} \\ c
&= r \cdot \frac{\text{SD of points per game}}{\text{SD of assists
per game}}  \end{aligned}
We’re told in the problem that the SD of points per game is 5 and the
SD of assists per game is 1.5. So, a = r \cdot
\frac{1.5}{5} and c = r \cdot
\frac{5}{1.5}; since \frac{1.5}{5} <
\frac{5}{1.5}, a < c.",74.0,Medium
707,Wi,22,Final,,Problem 8,Problem 8,,,,
708,Wi,22,Final,,Problem 8,Problem 8.1,"Recall that the mean points per game is 7, with a standard deviation
of 5. Also note that for all players, points per game must be greater
than or equal to 0.
Using Chebyshev’s inequality, we find that at least p\% of players scored 25 or fewer points per
game.
What is the value of p? Give your
answer as number between 0 and 100, rounded to 3 decimal places.","92.284\%
Recall, Chebyshev’s inequality states that the proportion of values
within z standard deviations of the
mean is at least 1 -
\frac{1}{z^2}.
To approach the problem, we’ll start by converting 25 points per game
to standard units. Doing so yields \frac{25 -
7}{5} = 3.6. This means that 25 is 3.6 standard deviations
above the mean. The value 3.6 standard deviations
below the mean is 7 - 3.6
\cdot 5 = -11, so when we use Chebyshev’s inequality with z = 3.6, we will get a lower bound on the
proportion of values between -11 and 25. However, as the question tells
us, points per game must be non-negative, so in this case the proportion
of values between -11 and 25 is the same as the proportion of values
between 0 and 25 (i.e. the proportion of values less than or equal to
25).
When z = 3.6, we have 1 - \frac{1}{z^2} = 1 - \frac{1}{3.6^2} =
0.922839, which as a percentage rounded to three decimal places
is 92.284\%. Thus, at least 92.284\% scored 25 or fewer points per
game.",46.0,Hard
709,Wi,22,Final,,Problem 8,Problem 8.2,"Note: This problem is out of scope; it
covers material no longer included in the course.
Note: This question uses the mathematical definition
of percentile, not np.percentile.
The array aces defined below contains the points per
game scored by all members of the Las Vegas Aces. Note that it contains
14 numbers that are in sorted order.
aces = np.array([0, 0, 1.05, 1.47, 1.96, 2, 3.25, 
                 10.53, 11.09, 11.62, 12.19, 
                 14.24, 14.81, 18.25])
As we saw in lab, percentiles are not unique. For instance, the
number 1.05 is both the 15th percentile and 16th percentile of
aces.
There is a positive integer q,
between 0 and 100, such that 14.24 is the qth percentile of aces, but
14.81 is the (q+1)th percentile of
aces.
What is the value of q? Give your
answer as an integer between 0 and 100.","85
For reference, recall that we find the pth percentile of a collection of n numbers as follows:

Sort the collection in increasing order.
Define h to be p\% of n:

h = \frac{p}{100} \cdot n

If h is an integer, define k = h. Otherwise, let k be the smallest integer greater than h.
Take the kth element of the
sorted collection (start counting from 1, not 0).


To start, it’s worth emphasizing that there are n = 14 numbers in aces total.
14.24 is at position 12 (when the positions are numbered 1 through
14).
Let’s try and find a value of p such
that 14.24 is the pth percentile. To do
so, we might try and find what “percentage” of the way through the
distribution 14.24 is; doing so gives \frac{12}{14} = 85.71\%. If we follow the
process outlined above with p = 85, we
get that h = \frac{85}{100} \cdot 14 =
11.9 and thus k = 12, meaning
that the 85th percentile is the number at position 12, which 14.24.
Let’s see what happens when we try the same process with p = 86. This time, we have h = \frac{86}{100} \cdot 14 = 12.04 and thus
k = 13, meaning that the 86th
percentile is the number at position 13, which is 14.81.
This means that the value of q is 85
– the 85th percentile is 14.24, while the 86th percentile is 14.81.",57.0,Medium
710,Wi,22,Final,,Problem 9,Problem 9,"For your convenience, we show the first few rows of
season again below.

In the past three problems, we presumed that we had access to the
entire season DataFrame. Now, suppose we only have access
to the DataFrame small_season, which is a random sample of
size 36 from season. We’re interested in
learning about the true mean points per game of all players in
season given just the information in
small_season.
To start, we want to bootstrap small_season 10,000 times
and compute the mean of the resample each time. We want to store these
10,000 bootstrapped means in the array boot_means.
Here is a broken implementation of this procedure.
boot_means = np.array([])                                           
for i in np.arange(10000):                                          
    resample = small_season.sample(season.shape[0], replace=False)  # Line 1
    resample_mean = small_season.get('PPG').mean()                  # Line 2
    np.append(boot_means, new_mean)                                 # Line 3
For each of the 3 lines of code above (marked by comments), specify
what is incorrect about the line by selecting one or more of the
corresponding options below. Or, select “Line _ is correct as-is” if you
believe there’s nothing that needs to be changed about the line in order
for the above code to run properly.",,,
711,Wi,22,Final,,Problem 9,Problem 9.1,"What is incorrect about Line 1? Select all that apply.

 Currently the procedure samples from small_season, when
it should be sampling from season
 The sample size is season.shape[0], when it should be
small_season.shape[0]
 Sampling is currently being done without replacement, when it should
be done with replacement
 Line 1 is correct as-is",,95.0,Easy
712,Wi,22,Final,,Problem 9,Problem 9.2,"What is incorrect about Line 2? Select all that apply.

 Currently it is taking the mean of the 'PPG' column in
small_season, when it should be taking the mean of the
'PPG' column in season
 Currently it is taking the mean of the 'PPG' column in
small_season, when it should be taking the mean of the
'PPG' column in resample
 .mean() is not a valid Series method, and should be
replaced with a call to the function np.mean
 Line 2 is correct as-is","Currently it is taking the mean of the
'PPG' column in small_season, when it should
be taking the mean of the 'PPG' column in
resample
The current implementation of Line 2 doesn’t use the
resample at all, when it should. If we were to leave Line 2
as it is, all of the values in boot_means would be
identical (and equal to the mean of the 'PPG' column in
small_season).
Option 1 is incorrect since our bootstrapping procedure is
independent of season. Option 3 is incorrect because
.mean() is a valid Series method.",98.0,Easy
713,Wi,22,Final,,Problem 9,Problem 9.3,"What is incorrect about Line 3? Select all that apply.

 The result of calling np.append is not being reassigned
to boot_means, so boot_means will be an empty
array after running this procedure
 The indentation level of the line is incorrect –
np.append should be outside of the for-loop
(and aligned with for i)
 new_mean is not a defined variable name, and should be
replaced with resample_mean
 Line 3 is correct as-is",,94.0,Easy
714,Wi,22,Final,,Problem 9,Problem 9.4,"Suppose we’ve now fixed everything that was incorrect about our
bootstrapping implementation.
Recall from earlier in the exam that, in season, the
mean number of points per game is 7, with a standard deviation of 5.
It turns out that when looking at just the players in
small_season, the mean number of points per game is 9, with
a standard deviation of 4. Remember that small_season is a
random sample of size 36 taken from season.
Which of the following histograms visualizes the empirical
distribution of the sample mean, computed using the bootstrapping
procedure above?


 Option 1
 Option 2
 Option 3
 Option 4","Option 3
The key to this problem is knowing to use the Central Limit Theorem.
Specifically, we know that if we collect many samples from a population
with replacement, then the distribution of the sample means will be
roughly normal with:

a mean that is equal to the mean of the population
a standard deviation that is \frac{\text{SD of population}}{\sqrt{\text{sample
size}}}

Here, the “population” is small_season,
because that is the sample we’re repeatedly (re)sampling from. While
season is actually the population, it is not seen at all in
the bootstrapping process, so it doesn’t directly influence the
distribution of the bootstrapped sample means.
The mean of small_season is 9, and so is the
distribution of bootstrapped sample means. The standard deviation of
small_season is 4, so the square root law, the standard
deviation of the distribution of bootstrapped sample means is \frac{4}{\sqrt{36}} = \frac{4}{6} =
\frac{2}{3}.
The answer now boils down to choosing the histogram that looks
roughly normally distributed with a mean of 9 and a standard deviation
of \frac{2}{3}. Options 1 and 4 can be
ruled out right away since their means seem to be smaller than 9. To
decide between Options 2 and 3, we can use the inflection point rule,
which states that in a normal distribution, the inflection points occur
at one standard deviation above and one standard deviation below the
mean. (An inflection point is when a curve changes from opening upwards
to opening downwards.) See the picture below for more details.

Option 3 is the only distribution that appears to be centered at 9
with a standard deviation of \frac{2}{3} (0.7 is close to \frac{2}{3}), so it must be the empirical
distribution of the bootstrapped sample means.",42.0,Hard
715,Wi,22,Final,,Problem 9,Problem 9.5,"We construct a 95% confidence interval for the true mean points per
game for all players by taking the middle 95% of the bootstrapped sample
means.
left_b = np.percentile(boot_means, 2.5)
right_b = np.percentile(boot_means, 97.5)
boot_ci = [left_b, right_b]
Select the most correct statement below.

 (left_b + right_b) / 2 is exactly equal to the mean
points per game in season.
 (left_b + right_b) / 2 is not necessarily equal to the
mean points per game in season, but is close.
 (left_b + right_b) / 2 is exactly equal to the mean
points per game in small_season.
 (left_b + right_b) / 2 is not necessarily equal to the
mean points per game in small_season, but is close.
 (left_b _+ right_b) / 2 is not close to either the mean
points per game in season or the mean points per game in
small_season.","(left_b + right_b) / 2 is not
necessarily equal to the mean points per game in
small_season, but is close.
Normal-based confidence intervals are of the form [\text{mean} - \text{something}, \text{mean} +
\text{something}]. In such confidence intervals, it is the case
that the average of the left and right endpoints is exactly the mean of
the distribution used to compute the interval.
However, the confidence interval we’ve created is not
normal-based, rather it is bootstrap-based! As such, we can’t
say that anything is exactly true; this rules out Options 1 and
3.
Our 95% confidence interval was created by taking the middle 95% of
bootstrapped sample means. The distribution of bootstrapped sample means
is roughly normal, and the normal distribution is
symmetric (the mean and median are both equal, and represent the
“center” of the distribution). This means that the middle of our 95%
confidence interval should be roughly equal to the mean of the
distribution of bootstrapped sample means. This implies that Option 4 is
correct; the difference between Options 2 and 4 is that Option 4 uses
small_season, which is the sample we bootstrapped from,
while Option 2 uses season, which was not accessed at all
in our bootstrapping procedure.",62.0,Medium
716,Wi,22,Final,,Problem 9,Problem 9.6,"Instead of bootstrapping, we could also construct a 95% confidence
interval for the true mean points per game by using the Central Limit
Theorem.
Recall that, when looking at just the players in
small_season, the mean number of points per game is 9, with
a standard deviation of 4. Also remember that small_season
is a random sample of size 36 taken from season.
Using only the information that we have about
small_season (i.e. without using any facts about
season), compute a 95% confidence interval for the true
mean points per game.
What are the left and right endpoints of your interval? Give your
answers as numbers rounded to 3 decimal places.","[7.667,
10.333]
In a normal distribution, roughly 95% of values are within 2 standard
deviations of the mean. The CLT tells us that the distribution of sample
means is roughly normal, and in subpart 4 of this problem we already
computed the SD of the distribution of sample means to be \frac{2}{3}.
So, our normal-based 95% confidence interval is computed as
follows:
\begin{aligned} &[\text{mean of
sample} - 2 \cdot \text{SD of distribution of sample means}, \text{mean
of sample} + 2 \cdot \text{SD of distribution of sample means}] \\
&= [9 - 2 \cdot \frac{4}{\sqrt{36}}, 9 + 2 \cdot
\frac{4}{\sqrt{36}}] \\ &= [9 - \frac{4}{3}, 9 + \frac{4}{3}] \\
&\approx \boxed{[7.667, 10.333]} \end{aligned}",87.0,Easy
717,Wi,22,Final,,Problem 9,Problem 9.7,"Recall that the mean points per game in season is 7,
which is not in the interval you found above (if it is, check your
work!).
Select the true statement below.

 The 95% confidence interval we created in the previous subpart did
not contain the true mean points per game, which means that the
distribution of the sample mean is not normal.
 The 95% confidence interval we created in the previous subpart did
not contain the true mean points per game, which means that the
distribution of points per game in small_season is not
normal.
 The 95% confidence interval we created in the previous subpart did
not contain the true mean points per game. This is to be expected,
because the Central Limit Theorem is only correct 95% of the time.
 The 95% confidence interval we created in the previous subpart did
not contain the true mean points per game, but if we collected many
original samples and constructed many 95% confidence intervals, then
roughly 95% of them would contain the true mean points per game.
 The 95% confidence interval we created in the previous subpart did
not contain the true mean points per game, but if we collected many
original samples and constructed many 95% confidence intervals, then
exactly 95% of them would contain the true mean points per game.","The 95% confidence interval we created in
the previous subpart did not contain the true mean points per game, but
if we collected many original samples and constructed many 95%
confidence intervals, then roughly 95% of them would contain the true
mean points per game.
In a confidence interval, the confidence level gives us a level of
confidence in the process used to create the confidence
interval. If we repeat the process of collecting a sample from the
population and using the sample to construct a c% confidence interval
for the population mean, then roughly c% of the
intervals we create should contain the population mean. Option 4 is the
only option that corresponds to this interpretation; the others are all
incorrect in different ways.",87.0,Easy
718,Wi,22,Final,,Problem 10,Problem 10,"Note: This problem is out of scope; it
covers material no longer included in the course.
The WNBA is interested in helping boost their players’ social media
presence, and considers various ways of making that happen.
Which of the following claims can be tested using a randomized
controlled trial? Select all that apply.

 Winning two games in a row causes a player to gain Instagram
followers.
 Drinking Gatorade causes a player to gain Instagram followers.
 Playing for the Las Vegas Aces causes a player to gain Instagram
followers.
 Deleting Twitter causes a player to gain Instagram followers.
 None of the above.",,77.0,Easy
719,Wi,23,Final,,Problem 1,Problem 1,"Let’s start by correcting the data in the ""Rating""
column. All the values in this column are strings of length 4. In
addition, some strings use commas in place of a dot to represent a
decimal point. Select the options below that evaluate to a Series
containing the values in the ""Rating"" column, appropriately
changed to floats.
Note: The Series method .str.replace
uses the string method .replace on every string in a
Series.

 float(games.get(""Rating"").str.replace("","", "".""))
 games.get(""Rating"").str.replace("","", ""."").apply(float)
 games.get(""Rating"").str.replace("","", """").apply(int)/100
 games.get(""Rating"").str.replace("","", """").apply(float)

Important! For the rest of this exam, we will assume
that the values in the ""Rating"" column have been correctly
changed to floats.","Option 2
Option 1: Let’s look at the code piece by piece to
understand why this does not work. games.get(""Rating"")
gives you the column ""Rating"" as a Series. As per the note,
.str.replace("","", ""."") can be used on a Series, and will
replace all commas with periods. The issue is with the use of the
float function; the float function can convert
a single string to a float, like float(""3.14""), but not an
entire Series of floats. This will cause an error making this option
wrong.
Option 2: Once again we are getting
""Rating"" as a Series and replacing the commas with periods.
We then apply float() to the Series, which will
successfully convert all of the values into floats.
Option 3: This piece of code attempts to replace
commas with nothing, which is correct for values using commas as decimal
separators. However, this approach ignores values that use dots as
decimal separators. Something like
games.get(""Rating"").str.replace("","", """").str.replace(""."", """").apply(int)/100
could correct this mistake.
Option 4: Again, we are getting
""Rating"" as a Series and replacing the commas with an empty
string. The values inside of ""Rating"" are then converted to
floats, which is fine, but remember, that the numbers are 100 times too
big. This means we have altered the actual value inappropriately, which
makes this option incorrect.",67.0,Medium
720,Wi,23,Final,,Problem 2,Problem 2,"You are unsure whether it would make sense to use
""BGG Rank"" as the index of the games
DataFrame, because you are unsure whether this column has duplicate
values. Perhaps, for example, two games are tied and both have a rank of
6.
Select all of the expressions below that evaluate to True when the
""BGG Rank"" column could be used as the index (no
duplicates), and False when it could not be used as the index
(duplicates). In other words, these are the expressions that can be used
to detect the presence of duplicate values.

 (games.get(""BGG Rank"") - np.arange(games.shape[0])).max() == 1
 games.groupby(""BGG Rank"").count().get(""Name"").max() == 1
 games.shape[0] - len(np.unique(games.get(""BGG Rank""))) == 0
 games.get(""BGG Rank"").max() - games.shape[0] == 0

Note: We will not set the index of
games, instead we’ll leave it with the default index.","Options 2 and 3
Option 2:
games.groupby(“BGG Rank”).count() gets all of the unique
“BGG Rank”’s and puts them into the index. Then by using
the aggregate function .count() we are able to turn all the
remaining columns into the number of times each “BGG Rank”
appears. Since all of the columns are the same we just need to get one
of them to access the counts. In this case we get the column “Name” by
doing .get(“Name”). Finally, when we do
.max() == 1 we are checking to see if the maximum count for
the number of unique “BGG Rank”’s is one, which would mean
there are no duplicates.
Option 3: Let’s work from the inside out for this
line of code: len(np.unique(games.get(“BGG Rank”))). Like
all the others we are getting a Series of “BGG Rank”.
np.unique() gives us an array with unique elements inside
of the Series. When we do len() we are figuring out how
many unique elements there are inside of “BGG Rank”. Recall
games.shape[0] gets us the number of rows in
games. This means that we are trying to see if the number
of rows is the same as the number of unique elements inside of
“BGG Rank”, and if they are then that means they are all
unique and should equal 0.
Option 1: games.get(“BGG Rank”) will
get you a Series of the “BGG Rank” column.
np.arange(games.shape[0]) will create a numpy array that
will go from zero to games.shape[0], which is the number of
rows in the games DataFrame. So it would look something
like: arr([0, 1, 2, . . ., n]), where n is the
number of rows in games. By doing:
games.get(“BGG Rank”) - np.arange(games.shape[0]) one is
decreasing each rank by an increasing factor of one each time. This
essentially gives a Series of numbers, but it doesn’t actually have
anything to do with uniqueness. We are simply finding if the difference
between “BGG Rank” and the numpy array leads to a maximum
of 1. So although the code works it does not tell us if there are
duplicates.
Option 4: games.get(“BGG Rank”).max()
will give us the maximum element inside of “BGG Rank”.
Note, games.shape[0] gets us the number of rows in
games. We should never make assumptions about what is
inside of “BGG Rank”. This means we don’t know if the
values line up nicely like: 1, 2, 3, . . . games.shape[0],
so the maximum element could be unique, but be bigger than
games.shape[0]. Knowing this, when we do the whole line for
Option 4 it is not guaranteed to be zero when “BGG Rank” is
unique or not, so it does not detect duplicates.",76.0,Easy
721,Wi,23,Final,,Problem 3,Problem 3,"Notice that ""Strategy Games"" and
""Thematic Games"" are two of the possible domains, and that
a game can belong to multiple domains.
Define the variables strategy and thematic follows.
strategy = games.get(""Domains"").str.contains(""Strategy Games"")
thematic = games.get(""Domains"").str.contains(""Thematic Games"")",,,
722,Wi,23,Final,,Problem 3,Problem 3.1,"What is the data type of strategy?

 bool
 str
 Series
 DataFrame","Series
strategy will give you a Series. This is
because games.get(“Domains”) will give you one column, a
Series, and then
.str.contains(“Strategy Games”) will convert those values
to True if it contains that string and False
otherwise, but it will not actually change the Series to a
DataFrame, a bool, or a str.",81.0,Easy
723,Wi,23,Final,,Problem 3,Problem 3.2,"Suppose we randomly select one of the ""Strategy Games""
from the games DataFrame.
What is the probability that the randomly selected game is
not also one of the ""Thematic Games""?
Write a single line of Python code that evaluates to this probability,
using the variables strategy and thematic in
your solution.
Note: For this question and others that require one
line of code, it’s fine if you need to write your solution on multiple
lines, as long as it is just one line of Python code. Please do write on
multiple lines to make sure your answer fits within the box
provided.","(games[(strategy == True) & (thematic == False)].shape[0] / games[strategy == True].shape[0])
or
1 - games[strategy & thematic].shape[0] / games[strategy].shape[0]
The problem is asking us to find the probability that a selected game
from “Strategy Games” will not be in
“Thematic Games”. Recall that this is the probability that
given “Strategy Games” will it not be in
“Thematic Games”, which would look like this:
P(“Thematic Games”
Compliment|“Strategy Games”). This means the formula would
look like: (“Thematic Games” Compliment
and
“Strategy Games”)/“Strategy Games”
This means one possible solution for this would be:
(games[(strategy == True) & (thematic == False)].shape[0] / games[strategy == True].shape[0]
This solution works because we are following the formula to find the
probability of thematic complement and strategy games over the number of
times “Strategy Games” are True. Doing
games[query_condition] gives us the games
DataFrame where strategy == True and
thematic == False. Another important thing is that for
(baby)pandas we always use the keyword & and not
and. Note that we are using .shape[0] to get
the number of rows or times that True shows up for
“Strategy Games” and the number of rows or times that
False shows up for “Thematic Games”
Compliment.
Another possible strategy would be using the complement rule: Which
would be: P(“Thematic Games” Compliment) = 1 -
P(“Thematic Games”). This would lead you to an answer like:
(1 - games[strategy & thematic].shape[0]) / games[strategy].shape[0].
games[strategy & thematic].shape[0]) finds you the
probability of P(“Thematic Games”), so when plugged into
the equation above we are able to find P(“Thematic Games”
Compliment).",43.0,Hard
724,Wi,23,Final,,Problem 3,Problem 3.3,"Many of the games in the games DataFrame belong to more
than one domain. We want to identify the number of games that belong to
only one domain. Select all of the options below that would correctly
calculate the number of games that belong to only one domain.
Hint: Don’t make any assumptions about the possible
domains.

 (games.get(""Domains"").str.split("" "").apply(len) == 2).sum()
 (games.get(""Domains"").apply(len) == 1).sum()
 (games[games.get(""Domains"").str.split("","").apply(len) == 1].groupby(""Domains"").count().get(""Name"").sum())
 games[games.get(""Domains"").str.split("","").apply(len) == 1].shape[0]","Options 3 and 4
Let’s take a closer look at why Option 3 and
Option 4 are correct.
Option 3: Option 3 first queries the
games DataFrame to only keep games with one
“Domains”.
games.get(“Domains”).str.split(“,”).apply(len) == 1 gets
the “Domains” column and splits all of them if they contain
a comma. If the value does have a comma then it will create a list. For
example let’s say the domain was
“Strategy Games”, “Thematic Games” then after doing
str.split(“,”) we would have the list:
[“Strategy Games”, “Thematic Games”]. Any row with a
list will evaluate to False. This means we are
only keeping values where there is one domain. The next
part .groupby(“Domains”).count().get(“Name”).sum() makes a
DataFrame with an index of the unique domains and the number of times
those appear. Note that all the other columns: “Name”,
“Mechanics”, “Play Time”,
“Complexity”, “Rating”, and
“BGG Rank” now evaluate to the same thing, the number of
times a unique domain appears. That means by doing
.get(“Name”).sum() we are adding up all the number of times
a unique domain appears, which would get us the number of games that
belong to only one domain.
Option 4: Option 4 starts off exactly like Option 3,
but instead of doing .groupby() it gets the number of rows
using .shape[0], which will give us the number of games
that belong to only one domain.
Option 1: Let’s step through why Option 1 is
incorrect.
games.get(“Domains”).str.split(“ ”).apply(len) == 2.sum()
gives you a Series of the “Domains” column,
then splits each domain by a space. We then get the length of that
list, check if the length is equal to 2, which would mean
there are two elements in the list, and finally get the sum
of all elements in the list who had two elements because of the split.
Remember that True evaluates to 1 and False
evaluates to 0, so we are getting the number of elements that were split
into two. It does not tell us the number of games that belong to only
one domain.
Option 2: Let’s step through why Option 2 is also
incorrect. (games.get(“Domains”).apply(len) == 1).sum()
checks to see if each element in the column “Domains” has
only one character. Remember when you apply len() to a
string then we get the number of characters in that string. This is
essentially counting the number of domains that have 1 letter. Thus, it
does not tell us the number of games that belong to only one domain.",86.0,Easy
725,Wi,23,Final,,Problem 4,Problem 4,"We want to create a bootstrapped 95% confidence interval for the
median ""Complexity"" of all cooperative games, given a
sample of 100 cooperative games.
Suppose coop_sample is a DataFrame containing 100 rows
of games, all of which are cooperative games. We’ll calculate the
endpoints left and right of our bootstrapped
95% confidence interval as follows.
medians = np.array([])
for i in np.arange(10000):
    resample = coop_sample.sample(100, replace=True)
    median = np.median(resample.get(""Complexity""))
    medians = np.append(medians, median)
left = np.percentile(medians, 2.5)
right = np.percentile(medians, 97.5)
Now consider the interval defined by the endpoints
left_2 and right_2, calculated as follows.
medians_2 = np.array([])
for i in np.arange(10000):
    shuffle = coop_sample.assign(shuffle=
    np.random.permutation(coop_sample.get(""Complexity"")))
    resample_2 = shuffle.sample(100, replace=True)
    median_2 = np.median(resample_2.get(""shuffle""))
    medians_2 = np.append(medians_2, median_2)
left_2 = np.percentile(medians_2, 2.5)
right_2 = np.percentile(medians_2, 97.5)
Which interval should be wider, [left, right] or
[left_2, right_2]?

 [left, right]
 [left_2, right_2]
 Both about the same.","Both about the same.
It’s important to understand what each code block above is doing in
order to answer this question. Let’s take a look at the original
medians code. We are sampling from the
coop_sample to create a shuffled coop_sample,
we then get the median of the column “Complexity” and append it to the
medians array. Finally, we find the left and right
percentiles of the medians array.
Now we will look at what medians_2 is doing. It looks
like we are adding a new column called “shuffle” to
coop_sample. The column “shuffle” is a
shuffled version of “Complexity”. Then we are taking the
shuffle DataFrame with the “shuffle” column
and sampling from “shuffle” to randomize it again. Then we
get the median of this shuffled column and find its percentiles.
Essentially, both of these blocks of code are taking the
“Complexity” column, shuffling it, finding the median of
the shuffled column, and then finding the confidence interval. Since it
is being done on the same column and in basically the same way both
intervals [left, right] and [left_2, right_2]
are about the same.",77.0,Easy
726,Wi,23,Final,,Problem 5,Problem 5,"As in the previous question, let coop_sample be a sample
of 100 rows of games, all corresponding to cooperative games.
Define samp and resamp as follows.
samp = coop_sample.get(""Complexity"")
resamp = coop_sample.sample(100, replace=True).get(""Complexity"")",,,
727,Wi,23,Final,,Problem 5,Problem 5.1,"Which of the following statements could evaluate to
True? Select all that are possible.

 len(samp.unique()) < len(resamp.unique())
 len(samp.unique()) == len(resamp.unique())
 len(samp.unique()) > len(resamp.unique())","Options 2 and 3
Option 2: This is correct because it is possible for
resamp to be shuffled in such a way that the number of
unique elements are not the same.
Option 3: This is correct because it is possible for
resamp to pull the same values more often making it less
unique than samp.
Option 1: The reason that this is incorrect is
because samp.unique() has the most possible unique elements
inside of it. When we shuffle it using
coop_sample.sample(100, replace = True) we could pull the
same value multiple times, making it less unique.",91.0,Easy
728,Wi,23,Final,,Problem 5,Problem 5.2,"Which of the following statements could evaluate to
True? Select all that are possible.

 np.count nonzero(samp == 1) < np.count nonzero(resamp == 1)
 np.count nonzero(samp == 1) == np.count nonzero(resamp == 1)
 np.count nonzero(samp == 1) > np.count nonzero(resamp == 1)","Options 1, 2, and 3
Option 1: It might be helpful to recall what exactly
the column “Complexity” holds. In this case it holds the
average complexity of the game on a scale of 1 to 5. The code is trying
to find if the number of ones in samp and
resamp are different. It is possible that when shuffling
due to replace = True that resamp has more
ones inside of it than samp.
Option 2: Once again it is possible that when
shuffled resamp has the same number of ones as
samp does.
Option 3: When we shuffle coop_sample
there is no guarantee that one will sample more ones and instead other
averages could be selected. This means it is possible for the number of
ones in samp can be greater than the number of ones in
resamp.",83.0,Easy
729,Wi,23,Final,,Problem 5,Problem 5.3,"Which of the following statements could evaluate to
True? Select all that are possible.

 samp.min() < resamp.min()
 samp.min() == resamp.min()
 samp.min() > resamp.min()","Options 1 and 2
Option 1: It is possible when shuffled that
samp’s original minimum is never sampled, making
resamp’s minimum to be greater than samp’s
min.
Option 2:: If samp’s original min is
sampled then it will be the same minimum that appears inside of
resamp.
Option 3: It is impossible for resamp’s
minimum to be less than samp’s minimum. This is because all
of resamp’s values come from samp. That means
there cannot be a smaller average inside of resamp that
never appears in samp.",83.0,Easy
730,Wi,23,Final,,Problem 5,Problem 5.4,"Which of the following statements could evaluate to
True? Select all that are possible.

 np.std(samp) < np.std(resamp)
 np.std(samp) == np.std(resamp)
 np.std(samp) > np.std(resamp)","Options 1, 2, and 3
Option 1: np.std() gives us the
standard deviation of the array we give it. When we do
np.std(samp) we are finding the standard deviation of
“Complexity”. When we do np.std(resamp) we are
finding the standard deviation of “Complexity”, which may
grab values multiple times. Since we are grabbing values multiple times
it is possible to have a standard deviation become smaller if we
continuously grab smaller values.
Option 2: If the resamp gets us the
same values as samp we would end up with the same standard
deviation, which would make
np.std(samp) == np.std(resamp).
Option 3: Similar to Option 1, we may grab many
values which are on the larger end, which could increase our standard
deviation.",79.0,Easy
731,Wi,23,Final,,Problem 6,Problem 6,"Choose the best tool to answer each of the following questions. Note
the following:

By “hypothesis testing,” we mean “standard” hypothesis testing,
i.e. hypothesis testing that doesn’t involve
permutation testing or bootstrapping.
By “bootstrapping,” we mean bootstrapping that
doesn’t involve hypothesis testing.",,,
732,Wi,23,Final,,Problem 6,Problem 6.1,"Are strategy games rated higher than non-strategy games?

 Hypothesis testing
 Permutation testing
 Bootstrapping","Permutation testing
Recall that we use a permutation test when we want to determine if
two samples are from the same population. The question is asking if
“strategy games” are rated higher than “non-strategy games” meaning we
have two samples and want to know if they come from the same or
different rating populations.
We would not use hypothesis testing here because we are not trying to
quantify how weird a test statistic between strategy games and
non-strategy games.
We would not use bootstrapping here because we are not given a single
sample that we want to re-sample from.",58.0,Medium
733,Wi,23,Final,,Problem 6,Problem 6.2,"What is the mean complexity of all games?

 Hypothesis testing
 Permutation testing
 Bootstrapping","Bootstrapping
Bootstrapping is the act of resampling from a sample. We use
bootstrapping because the original sample looks like the population, so
by resampling the sample we are able to quantify our uncertainty of the
mean complexity of all games. We can use bootstrapping to approximate
the distribution of the sample statistic, which is the mean.
We would not use hypothesis testing here because we do not have the
population distribution or a sample to test with.
We would not use permutation testing here because we are not trying
to find if two samples are from the same population.",89.0,Easy
734,Wi,23,Final,,Problem 6,Problem 6.3,"Are there an equal number of cooperative and non-cooperative
games?

 Hypothesis testing
 Permutation testing
 Bootstrapping","Hypothesis Testing
Recall hypothesis tests quantify how “weird” a result is. We use it
when we have a population distribution and one sample and we are trying
to see if that sample was drawn from the population. In this instance we
are trying to find if there are an equal number of cooperative and
non-cooperative games. The population distribution is our DataFrame and
we are trying to see if the cooperative games and non-cooperative games
in our sample come from the same population.
We would not use permutation testing here because there is no
numerical data in the column, and it can be answered by just the column
of categories.
We would not use bootstrapping because we are not re-sampling from
the sample we are given to find a test statistic.",75.0,Easy
735,Wi,23,Final,,Problem 6,Problem 6.4,"Are games with more than one domain more complex than games with one
domain?

 Hypothesis testing
 Permutation testing
 Bootstrapping","Permutation testing
Once again we would use permutation testing to solve this problem
because we have two samples: games with more than one domain and games
with one domain. We do not know the population distribution.
We would not use hypothesis testing because we were not given a
population distribution to test the sample against.
We would not use bootstrapping because we are not re-sampling from
the sample to find a test statistic.",72.0,Medium
736,Wi,23,Final,,Problem 7,Problem 7,"We use the regression line to predict a game’s ""Rating""
based on its ""Complexity"". We find that for the game
Wingspan, which has a ""Complexity"" that is 2
points higher than the average, the predicted ""Rating"" is 3
points higher than the average.",,,
737,Wi,23,Final,,Problem 7,Problem 7.1,"What can you conclude about the correlation coefficient r?

 r < 0
 r = 0
 r > 0
 We cannot make any conclusions about the value of r based on this information alone.","r >
0
To answer this problem, it’s useful to recall the regression line in
standard units:
\text{predicted } y_{\text{(su)}} = r
\cdot x_{\text{(su)}}
If a value is positive in standard units, it means that it is above
the average of the distribution that it came from, and if a value is
negative in standard units, it means that it is below the average of the
distribution that it came from. Since we’re told that Wingspan
has a ""Complexity"" that is 2 points higher than the
average, we know that x_{\text{(su)}}
is positive. Since we’re told that the predicted ""Rating""
is 3 points higher than the average, we know that \text{predicted } y_{\text{(su)}} must also
be positive. As a result, r must also
be positive, since you can’t multiply a positive number (x_{\text{(su)}}) by a negative number and end
up with another positive number.",74.0,Medium
738,Wi,23,Final,,Problem 7,Problem 7.2,"What can you conclude about the standard deviations of “Complexity”
and “Rating”?

 SD of ""Complexity"" < SD of ""Rating""
 SD of ""Complexity"" = SD of ""Rating""
 SD of ""Complexity"" > SD of ""Rating""
 We cannot make any conclusions about the relationship between these
two standard deviations based on this information alone.","SD of ""Complexity"" < SD of
""Rating""
Since the distance of the predicted ""Rating"" from its
average is larger than the distance of the ""Complexity""
from its average, it might be reasonable to guess that the values in the
""Rating"" column are more spread out. This is true, but
let’s see concretely why that’s the case.
Let’s start with the equation of the regression line in standard
units from the previous subpart. Remember that here, x refers to ""Complexity"" and
y refers to ""Rating"".
\text{predicted } y_{\text{(su)}} = r
\cdot x_{\text{(su)}}
We know that to convert a value to standard units, we subtract the
value by the mean of the column it came from, and divide by the standard
deviation of the column it came from. As such, x_{\text{(su)}} = \frac{x - \text{mean of }
x}{\text{SD of } x}. We can substitute this relationship in the
regression line above, which gives us
\frac{\text{predicted } y - \text{mean of
} y}{\text{SD of } y} = r \cdot \frac{x - \text{mean of } x}{\text{SD of
} x}
To simplify things, let’s use what we were told. We were told that
the predicted ""Rating"" was 3 points higher than average.
This means that the numerator of the left side, \text{predicted } y - \text{mean of } y, is
equal to 3. Similarly, we were told that the ""Complexity""
was 2 points higher than average, so x -
\text{mean of } x is 2. Then, we have:
\frac{3}{\text{SD of } y} =
\frac{2r}{\text{SD of }x}
Note that for convenience, we included r in the numerator on the right-hand
side.
Remember that our goal is to compare the SD of ""Rating""
(y) to the SD of
""Complexity"" (x). We now
have an equation that relates these two quantities! Since they’re both
currently on the denominator, which can be tricky to work with, let’s
take the reciprocal (i.e. “flip”) both fractions.
\frac{\text{SD of } y}{3} = \frac{\text{SD
of }x}{2r}
Now, re-arranging gives us
\text{SD of } y \cdot \frac{2r}{3} =
\text{SD of }x
Since we know that r is somewhere
between 0 and 1, we know that \frac{2r}{3} is somewhere between 0 and \frac{2}{3}. This means that \text{SD of } x is somewhere between 0 and
two-thirds of the value of \text{SD of }
y, which means that no matter what, \text{SD of } x < \text{SD of } y.
Remembering again that here ""Complexity"" is our x and ""Rating"" is our y, we have that the SD of
""Complexity"" is less than the SD of
""Rating"".",42.0,Hard
739,Wi,23,Final,,Problem 8,Problem 8,"Suppose that for children’s games, ""Play Time"" and
""Rating"" are negatively linearly associated due to children
having short attention spans. Suppose that for children’s games, the
standard deviation of ""Play Time"" is twice the standard
deviation of ""Rating"", and the average
""Play Time"" is 10 minutes. We use linear regression to
predict the ""Rating"" of a children’s game based on its
""Play Time"". The regression line predicts that Don’t
Break the Ice, a children’s game with a ""Play Time"" of
8 minutes will have a ""Rating"" of 4. Which of the following
could be the average ""Rating"" for children’s games?

 2
 2.8
 3.1
 4","3.1
Let’s recall the formulas for the regression line in original units,
since we’re given information in original units in this question (such
as the fact that for a ""Play Time"" of 8
minutes, the predicted ""Rating"" is 4
stars). Remember that throughout this question, ""Play Time""
is our x and ""Rating"" is
our y.
The regression line is of the form y = mx +
b, where
m = r \cdot \frac{\text{SD of }
y}{\text{SD of }x}, b = \text{mean of }y - m \cdot \text{mean of }
x
There’s a lot of information provided to us in the question – let’s
think about what it means in the context of our xs and ys.

The first piece is that r is
negative, so -1 \leq r < 0.
The second piece is that \text{SD of } x =
2 \cdot (\text{SD of } y). Equivalently, we can say that \frac{\text{SD of } y}{\text{SD of } x} =
\frac{1}{2}. This form is convenient, because it’s close to the
definition of the slope of the regression line, m. Using this fact, the slope of the
regression line is m = r \cdot \frac{\text{SD
of } y}{\text{SD of }x} = r \cdot \frac{1}{2} = \frac{r}{2}.
The \text{mean of } x is 10. This
means that the intercept of the regression line, b, is b =
\text{mean of }y - m \cdot \text{mean of } x = \text{mean of }y -
\frac{r}{2} \cdot 10 = \text{mean of }y - 5r.
If x is 8, the predicted y is 4.

Given all of this information, we need to find possible values for
the \text{mean of } y. Substituting our
known values for m and b into y = mx +
b gives us
y = \frac{r}{2} x + \text{mean of }y -
5r
Now, using the fact that if if x =
8, the predicted y is 4, we
have
\begin{align*}4 &= \frac{r}{2} \cdot 8
+ \text{mean of }y - 5r\\4 &= 4r - 5r + \text{mean of }y\\ 4 + r
&= \text{mean of} y\end{align*}
Cool! We now know that the \text{mean of }
y is 4 + r. We know that r must satisfy the relationship -1 \leq r < 0. By adding 4 to all pieces
of this inequality, we have that 3 \leq r + 4
< 4, which means that 3 \leq
\text{mean of } y < 4. Of the four options provided, only one
is greater than or equal to 3 and less than 4, which is 3.1.",55.0,Medium
740,Wi,23,Final,,Problem 9,Problem 9,"The function perm_test should take three inputs:

df, a DataFrame.
labels, a string. The name of a column in df that
contains two distinct values, which signify the groups in a permutation
test.
data, a string. The name of a column in df that
contains numerical data.

The function should return an array of 1000 simulated differences of
group means, under the assumption of the null hypothesis in a
permutation test, namely that data in both groups come from the same
population.
The smaller of the two group labels should be first in the
subtraction. For example, if the two values in the labels
column are ""dice game"" and ""card game"", we
would compute the difference as the mean of the ""card game""
group minus the mean of the ""dice game"" group, because
""card game"" comes before ""dice game""
alphabetically. Note that groupby orders
the rows in ascending order by default.
An incorrect implementation of perm_test is provided
below.
1 def perm_test(df, labels, data):
2   diffs = np.array([])
3   for i in np.arange(1000):
4       df.assign(shuffled=np.random permutation(df.get(data)))
5       means = df.groupby(labels).mean().get(data)
6       diff = means.iloc[0] - means.iloc[1]
7       diffs = np.append(diffs, diff)
8       return diffs
Three lines of code above are incorrect. Your job is to identify
which lines of code are incorrect, and describe briefly in English how
you would fix them. You don’t need to explain why the current code is
wrong, just how to fix it.",,,
741,Wi,23,Final,,Problem 9,Problem 9.1,"The first line that is incorrect is line number: _______
Explain in one sentence how to change this line. Do not write
code.","Line 4; We need to save this as
df.
Recall that df.assign() does not save the added column
to the original df, which means that we need to save line 4
to a variable called df.",55.0,Medium
742,Wi,23,Final,,Problem 9,Problem 9.2,"The second line that is incorrect is line number: _______
Explain in one sentence how to change this line. Do not write
code.","Line 5; We need to get
""shuffled"" instead of data.
Recall a permutation test is simulating if samples come from the same
population. This means we need to shuffle the data and use it to see if
that would change our result/view. This means in line 5 we want to use
the shuffled data, so we need to do .get(“shuffled”)
instead of .get(“data”).",50.0,Medium
743,Wi,23,Final,,Problem 9,Problem 9.3,"The third line that is incorrect is line number: _______
Explain in one sentence how to change this line. Do not write
code.","Line 8; Move it outside of the
for-loop (unindent).
If we have return inside of the for-loop it
will terminate after it goes through the code once! This means all we
have to do is unindent return, moving it outside of the
for-loop.",67.0,Medium
744,Wi,23,Final,,Problem 9,Problem 9.4,"Write one line of code that creates an array called
simulated_diffs containing 1000 simulated differences in
group means for this permutation test. You should call your
perm_test function here!","simulated diffs = perm_test(with dice[with dice.get(""Domains"").str.contains(""Children’s Games"")], ""isDice"", ""Play Time"")

The inputs to perm_test, in order, are:

A DataFrame containing all relevant information.
The name of the column in that DataFrame that contains group
labels.
The name of the column in that DataFrame that contains numbers.

Here, the only relevant information is information on
""Children's Games"", so the first argument to
perm_test must be a DataFrame in which the
""Domains"" column contains ""Children's Games"",
as described in the question.
Then, since we’re testing whether the distribution of
""Play Time"" is different for dice games and non-dice games,
we know that the column with group labels is ""isDice""
(which is defined in the call to .assign that is provided
to us in the question), and the column with numerical information is
""Play Time"".",60.0,Medium
745,Wi,23,Final,,Problem 9,Problem 9.5,"Suppose we’ve stored the observed value of the test statistic for
this permutation test in the variable obs_diff. Fill in the
blank below to find the p-value for this permutation test.

 <
 <=
 >
 >=",">=
We want to find if the simulated_diffs are more or as
extreme as the obs_diff. To be as or more extreme that
means it needs an equal sign. The other part of this is it cannot be
smaller because then it is not as extreme, which means
the answer must be >=.",57.0,Medium
746,Wi,23,Final,,Problem 10,Problem 10,"It’s your first time playing a new game called Brunch Menu.
The deck contains 96 cards, and each player will be dealt a hand of 9
cards. The goal of the game is to avoid having certain cards, called
Rotten Egg cards, which come with a penalty at the end of the
game. But you’re not sure how many of the 96 cards in the game are
Rotten Egg cards. So you decide to use the Central Limit
Theorem to estimate the proportion of Rotten Egg cards in the deck based
on the 9 random cards you are dealt in your hand.",,,
747,Wi,23,Final,,Problem 10,Problem 10.1,"You are dealt 3 Rotten Egg cards in your hand of 9 cards. You then
construct a CLT-based 95% confidence interval for the proportion of
Rotten Egg cards in the deck based on this sample. Approximately, how
wide is your confidence interval?
Choose the closest answer, and use the following facts:

The standard deviation of a collection of 0s and 1s is \sqrt{(\text{Prop. of 0s}) \cdot (\text{Prop of
1s})}.
\sqrt{18} is about \frac{17}{4}.


 \frac{17}{9}
 \frac{17}{27}
 \frac{17}{81}
 \frac{17}{96}","\frac{17}{27}
A Central Limit Theorem-based 95% confidence interval for a
population proportion is given by the following:
\left[ \text{Sample Proportion} - 2 \cdot
\frac{\text{Sample SD}}{\sqrt{\text{Sample Size}}}, \text{Sample
Proportion} + 2 \cdot \frac{\text{Sample SD}}{\sqrt{\text{Sample Size}}}
\right]
Note that this interval uses the fact that (about) 95% of values in a
normal distribution are within 2 standard deviations of the mean. It’s
key to divide by \sqrt{\text{Sample
Size}} when computing the standard deviation because the
distribution that is roughly normal is the distribution of the sample
mean (and hence, sample proportion), not the distribution of the sample
itself.
The width of the above interval – that is, the right endpoint minus
the left endpoint – is
\text{width} = 4 \cdot \frac{\text{Sample
SD}}{\sqrt{\text{Sample Size}}}
From the provided hint, we have that
\text{Sample SD} = \sqrt{(\text{Prop. of
0s}) \cdot (\text{Prop of 1s})} = \sqrt{\frac{3}{9} \cdot \frac{6}{9}} =
\frac{\sqrt{18}}{9}
Then, since we know that the sample size is 9 and that \sqrt{18} is about \frac{17}{4}, we have
\text{width} =  4 \cdot \frac{\text{Sample
SD}}{\sqrt{\text{Sample Size}}} = 4 \cdot
\frac{\frac{\sqrt{18}}{9}}{\sqrt{9}} = 4 \cdot \frac{\sqrt{18}}{9 \cdot
3} = 4 \cdot \frac{\frac{17}{4}}{27} = \frac{17}{27}",51.0,Medium
748,Wi,23,Final,,Problem 10,Problem 10.2,"Which of the following are limitations of trying to use the Central
Limit Theorem for this particular application? Select all that
apply.

 The CLT is for large random samples, and our sample was not very
large.
 The CLT is for random samples drawn with replacement, and our sample
was drawn without replacement.
 The CLT is for normally distributed data, and our data may not have
been normally distributed.
 The CLT is for sample means and sums, not sample proportions.","Options 1 and 2
Option 1: We use Central Limit Theorem (CLT) for
large random samples, and a sample of 9 is considered to be very small.
This makes it difficult to use CLT for this problem.
Option 2: Recall CLT happens when our sample is
drawn with replacement. When we are handed nine cards we are never
replacing cards back into our deck, which means that we are sampling
without replacement.
Option 3: This is wrong because CLT states that a
large sample is approximately a normal distribution even if the data
itself is not normally distributed. This means it doesn’t matter if our
data had not been normally distributed if we had a large enough sample
we could use CLT.
Option 4: This is wrong because CLT does apply to
the sample proportion distribution. Recall that proportions can be
treated like means.",77.0,Easy
749,Wi,23,Final,,Problem 11,Problem 11,"In recent years, there has been an explosion of board games that
teach computer programming skills, including CoderMindz,
Robot Turtles, and Code Monkey Island. Many such games
were made possible by Kickstarter crowdfunding campaigns.
Suppose that in one such game, players must prove their understanding
of functions and conditional statements by answering questions about the
function wham, defined below. Like players of this game,
you’ll also need to answer questions about this function.
1 def wham(a, b):
2   if a < b:
3       return a + 2
4   if a + 2 == b:
5       print(a + 3)
6       return b + 1
7   elif a - 1 > b:
8       print(a)
9       return a + 2
10  else:
11      return a + 1",,,
750,Wi,23,Final,,Problem 11,Problem 11.1,"What is printed when we run print(wham(6, 4))?","6 8
When we call wham(6, 4), a gets assigned to
the number 6 and b gets assigned to the number 4. In the
function we look at the first if-statement. The
if-statement is checking if a, 6, is less than
b, 4. We know 6 is not less than 4, so we skip this section
of code. Next we see the second if-statement which checks
if a, 6, plus 2 equals b, 4. We know 6 + 2 = 8, which is not equal to 4. We then
look at the elif-statement which asks if a, 6,
minus 1 is greater than b, 4. This is True! 6 - 1 = 5 and 5 > 4. So we
print(a), which will spit out 6 and then we will
return a + 2. a + 2 is 6 + 2. This means the function
wham will print 6 and return 8.",81.0,Easy
751,Wi,23,Final,,Problem 11,Problem 11.2,"Give an example of a pair of integers a and
b such that wham(a, b) returns
a + 1.","Any pair of integers a,
b with a = b or with
a = b + 1
The desired output is a + 1. So we want to look at the
function wham and see which condition is necessary to get
the output a + 1. It turns out that this can be found in
the else-block, which means we need to find an
a and b that will not satisfy any of the
if or elif-statements.
If a = b, so for example a points to 4 and
b points to 4 then: a is not less than
b (4 < 4), a + 2 is not equal to
b (4 + 2 = 6 and 6 does
not equal 4), and a - 1 is not greater than b
(4 - 1= 3) and 3 is not greater than
4.
If a = b + 1 this means that a is greater
than b, so for example if b is 4 then
a is 5 (4 + 1 = 5). If we
look at the if-statements then a < b is not
true (5 is greater than 4), a + 2 == b is also not true
(5 + 2 = 7 and 7 does not equal 4), and
a - 1 > b is also not true (5
- 1 = 4 and 4 is equal not greater than 4). This means it will
trigger the else statement.",94.0,Easy
752,Wi,23,Final,,Problem 11,Problem 11.3,"Which of the following lines of code will never be executed, for any
input?

 3
 6
 9
 11","6
For this to happen: a + 2 == b then a must
be less than b by 2. However if a is less than
b it will trigger the first if-statement. This
means this second if-statement will never run, which means
that the return on line 6 never happens.",79.0,Easy
753,Wi,23,Final,,Problem 12,Problem 12,"In the game Spot It, players race to identify an object that
appears on two different cards. Each card contains images of eight
objects, and exactly one object is common to both cards.",,,
754,Wi,23,Final,,Problem 12,Problem 12.1,"Suppose the objects appearing on each card are stored in an array,
and our task is to find the object that appears on both cards. Complete
the function find_match that takes as input two arrays of 8
objects each, with one object in common, and returns the name of the
object in both arrays.
For example, suppose we have two arrays defined as follows.
objects1 = np.array([""dragon"", ""spider"", ""car"", ""water droplet"", ""spiderweb"", ""candle"", ""ice cube"", ""ladybug""])
objects2 = np.array([""zebra"", ""lock"", ""dinosaur"", ""eye"", ""fire"", ""shamrock"", ""spider"", ""carrot""])
Then find_match(objects1, objects2) should evaluate to
""spider"". Your function must include a for loop, and it
must take at most three lines of code (not counting the
line with def).","def find_match(array1, array2):
    for obj in array1:
        if obj in array2:
            return obj

We first need to define the function find_match(). We
can gather since we are feeding in two groups of objects we are giving
find_match() two parameters, which we have called
array1 and array2. The next step is utilizing
a for-loop. We want to look at all the objects inside of
array1 and then check using an if-statement if
that object exists in array2. If it does, we can stop the
loop and return the object!",67.0,Medium
755,Wi,23,Final,,Problem 12,Problem 12.2,"Now suppose the objects appearing on each card are stored in a
DataFrame with 8 rows and one column called ""object"".
Complete the function find_match_again that takes as input
two such DataFrames with one object in common and returns the nameof the
object in both DataFrames.
Your function may not call the previous function
find_match, and it must take exactly one line of
code (not counting the line with def).","def find_match_again(df1, df2)
    return df1.merge(df2, on = “object”).get(“object”).iloc[0]

Once again we need to define our function and then have two
parameters for the two DataFrames. Recall the method
.merge() which will combine two DataFrames and only give us
the elements that are shared. The directions tell us that both
DataFrames have a single column called “object”. Since we
want to combine the DataFrames on that column we do have:
df1.merge(df2, on = “object”). Once we have merged the
DataFrames we should have only 1 row with the index and the column
“object”. To isolate the element inside of this DataFrame
we can first get a Series by doing .get(“object”) and then
do .iloc[0] to get the element inside of the Series.",46.0,Hard
756,Wi,23,Final,,Problem 13,Problem 13,"Dylan, Harshi, and Selim are playing a variant of a dice game called
Left, Center, Right (LCR) in which there are 9 chips
(tokens) and 9 dice. Each player starts off with 3 chips. Each die has
the following six sides: L, C, R, Dot, Dot, Dot.
During a given player’s turn, they must roll a number of dice equal
to the number of chips they currently have. Each die determines what to
do with one chip:

L means give the chip to the player on their left.
R means give the chip to the player on their right.
C means put the chip in the center of the table. This chip is now
out of play.
Dot means do nothing (or keep the chip).

Since the number of dice rolled is the same as the number of chips
the player has, the dice rolls determine exactly what to do with each
chip. There is no strategy at all in this simple game.
Dylan will take his turn first (we’ll call him Player 0), then at the
end of his turn, he’ll pass the dice to his left and play will continue
clockwise around the table. Harshi (Player 1) will go next, then Selim
(Player 2), then back to Dylan, and so on.

Note that if someone has no chips when it’s their turn, they are
still in the game and they still take their turn, they just roll 0 dice
because they have 0 chips. The game ends when only one person is holding
chips, and that person is the winner. If 300 turns have been taken (100
turns each), the game will end and we’ll declare it a tie.
The function simulate_lcr below simulates one full game
of Left, Center, Right and returns the number of turns taken in
that game. Some parts of the code are not provided. You will need to
fill in the code for the parts marked with a blank. The parts marked
with ... are not provided, but you don’t need to fill them
in because they are very similar to other parts that you do need to
complete.
Hint: Recall that in Python, the % operator gives
the remainder upon division. For example 12 % 5 is 2.
def simulate_lcr():
    # stores the number of chips for players 0, 1, 2 (in that order)
    player_chips = np.array([3,3,3])

    # maximum of 300 turns allotted for the game
    for i in np.arange(300):

        # which player's turn it is currently (0, 1, or 2)
        current_player = __(a)__

        # stores what the player rolled on their turn
        roll = np.random.choice([""L"", ""C"", ""R"", ""Dot"", ""Dot"", ""Dot""], __(b)__)

        # count the number of instances of L, C, and R
        L_count = __(c)__
        C_count = ...
        R_count = ...

        if current_player == 0:
            # update player_chips based on what player 0 rolled
            player_chips = player_chips + np.array(__(d)__)
        elif current_player == 1:
            # update player_chips based on what player 1 rolled
            player_chips = player_chips + ...
        else:
            # update player_chips based on what player 2 rolled
            player_chips = player_chips + ...

        # if the game is over, return the number of turns played
        if __(e)__:
            return __(f)__

    # if no one wins after 300 turns, return 300
    return 300",,,
757,Wi,23,Final,,Problem 13,Problem 13.1,What goes in blank (a)?,"i % 3
We are trying to find which player’s turn it is within the
for-loop. We know that each player: Dylan, Harshi, and
Selim will play a maximum of 100 turns. Notice that the
for-loop goes from 0 to 299. This means we need to
manipulate the i somehow to figure out whose turn it is.
The hint here is extremely helpful. The maximum remainder we want to
have is 2 (recall the players are called Player 0, Player 1, and Player
2). This means we can utilize % to give us the remainder of
i / 3, which would tell us which player’s turn it is.",41.0,Hard
758,Wi,23,Final,,Problem 13,Problem 13.2,What goes in blank (b)?,"player_chips[current_player]
Recall np.random.choice() must be given an array and can
then optionally be given a size to get multiple values
instead of one. We know that player_chips is an array of
the chips for each player. To access a specific player’s chips we can
use [current_player] because the 1st index of
player_chips corresponds to Player 0, the 2nd index
corresponds to Player 1, and the 3rd index corresponds to Player 2.",51.0,Medium
759,Wi,23,Final,,Problem 13,Problem 13.3,What goes in blank (c)?,"`np.count_nonzero(roll == “L”)
We know that if we do roll == “L” then we get an array
which changes the index of each element in roll to
True if that element equals “L” and
False otherwise. We can then use
np.count_nonzero() to count the number of True
values there are.",61.0,Medium
760,Wi,23,Final,,Problem 13,Problem 13.4,What goes in blank (d)?,"[-(L_count + C_count + R_count), L_count, R_count]
Recall the rules of the games:

L means give the chip to the player on their left.
R means give the chip to the player on their right.
C means put the chip in the center of the table. This chip is now
out of play.
Dot means do nothing (or keep the chip)

If we are Player 0 the person to our left is Player 1 and the person
to our right is Player 2. We want to update player_chips to
appropriately give the players to our left and right chips. This means
we can add our own array with the element at index 1 be our
L_count and the element at index 2 be our
R_count. We need to also subtract the tokens we are giving
away and C_count, so in index 0 we have:
-(L_count + C_count + R_count).",51.0,Medium
761,Wi,23,Final,,Problem 13,Problem 13.5,What goes in blank (e)?,"np.count_nonzero(player_chips) == 1
We want to stop the game early if only one person has chips. To do
this we can use np.count_nonzero(player_chips) to count the
number of elements inside player_chips that have chips. If
the player does not have chips then their index would have 0 inside of
it.",61.0,Medium
762,Wi,23,Final,,Problem 13,Problem 13.6,What goes in blank (f)?,"i + 1
To find the number of turns played we simply need to add 1 to
i. We do this because i starts at 0!",53.0,Medium
763,Wi,23,Final,,Problem 14,Problem 14,"Suppose the function simulate_lcr from the last question
has been correctly implemented, and we want to use it to see how many
turns a game of Left, Center, Right usually takes.
Note: You can answer this question even if you
couldn’t answer the previous one.
Consider the code and histogram below.
turns = np.array([])
for i in np.arange(10000):
    turns = np.append(turns, simulate_lcr())
(bpd.DataFrame().assign(turns=turns).plot(kind=""hist"", density = True, ec=""w"", bins = np.arange(0, 66, 6)))",,,
764,Wi,23,Final,,Problem 14,Problem 14.1,"Does this histogram show a probability distribution or an empirical
distribution?

 Probability Distribution
 Empirical Distribution","Empirical Distribution
An empirical distribution is derived from observed data, in this
case, the results of 10,000 simulated games of Left, Center, Right. It
represents the frequencies of outcomes (number of turns taken in each
game) as observed in these simulations.",54.0,Medium
765,Wi,23,Final,,Problem 14,Problem 14.2,"What is the probability of a game of Left, Center, Right
lasting 30 turns or more? Choose the closest answer below.

 0.01
 0.06
 0.10
 0.60","0.06
We’re being asked to find the proportion of values in the histogram
that are greater than or equal to 30, which is equal to the area of the
histogram to the right of 30. Immediately, we can rule out 0.01 and
0.60, because the area to the right of 30 is more than 1% of the total
area and less than 60% of the total area.
The problem then boils down to determining whether the area to the
right of 30 is 0.06 or 0.10. While you could solve this by finding the
areas of the three bars individually and adding them together, there’s a
quicker solution. Notice that the x-axis gridlines – the vertical lines in the
background in white – appear every 10 units (at x = 0, x =
10, x = 20, x = 30, and so on) and the y-axis gridlines – the horizontal lines in
the background in white – appear every 0.01 units (at y = 0, y =
0.01, y = 0.02, and so on).
There’s a “box” in the grid between x =
30 and x = 40, and between y = 0 and y =
0.01. The area of that box is (40 - 30)
\cdot 0.01 = 0.1, which means that if a bar book up the entire
box, then 10% of the values in this distribution would fall into that
bar’s bin.
So, to decide whether the area to the right of 30 is closer to 0.06
or 0.1, we can estimate whether the three bars to the right of 30 would
fill up the entire box described above (that is, the box from 30 to 40
on the x-axis and 0 to 0.1 on the y-axis), or whether it would be much emptier.
Visually, if you broke off the area that is to the right of 40 in the
histogram and put it in the box we’ve just described, then quite a bit
of the box would still be empty. As such, the area to the right of 30 is
less than the area of the box, so it’s less than 0.1, and so the only
valid option is 0.06.",50.0,Medium
766,Wi,23,Final,,Problem 14,Problem 14.3,"Suppose a player with n chips takes
their turn. What is the probability that they will have to put at least
one chip into the center? Give your answer as a mathematical expression
involving n.","1 -
(\frac{5}{6})^n
Recall that the die used to play this game has six sides: L, C, R,
Dot, Dot, Dot. The chance of getting C is \frac{1}{6}. So we can take the compliment of
that to get \frac{5}{6}, which is the
probability of not putting at least one chip into the center and then
doing (\frac{5}{6})^n. Once again we
must use the complement rule as to convert it back to the probability of
putting at least one chip into the center. This gives us the answer:
1 - (\frac{5}{6})^n",56.0,Medium
767,Wi,23,Final,,Problem 14,Problem 14.4,"Suppose a player with n chips takes
their turn. What is the probability that they will end their turn with
n chips? Give your answer as a
mathematical expression involving n.","\left( \frac{1}{2}
\right)^n
Recall, when it is a player’s turn, they roll one die for each of the
n chips they have. The die that they
roll has six faces. In three of those faces (L, C, and R), they end up
losing a chip, and in the other three of those faces (dot, dot, and
dot), they keep the chip. So, for each chip, there is a \frac{3}{6} = \frac{1}{2} chance that they
get to keep it after the turn. Since each die roll is independent, there
is a \frac{1}{2} \cdot \frac{1}{2} \cdot ...
\cdot \frac{1}{2} = \left( \frac{1}{2} \right)^n chance that they
get to keep all n chips. (Note that
there is no way to earn more chips during a turn, so that’s not
something we need to consider.)",69.0,Medium
768,Wi,23,Final,,Problem 15,Problem 15,"At a recent game night, you played several new board games and liked
them so much that you now want to buy copies for yourself.
The DataFrame stores is shown below in full. Each row
represents a game you want to buy and a local board game store where
that game is available for purchase. If a game is not available at a
certain store, there will be no row corresponding to that store and that
game.",,,
769,Wi,23,Final,,Problem 15,Problem 15.1,"The DataFrame prices has five rows. Below we merge
stores with prices and display the output in
full.
merged = stores.merge(prices, on=""Game"")
merged


In the space below, specify what the DataFrame prices
could look like. The column labels should go in the top
row, and the row labels (index) should go in the leftmost row. You may
not need to use all the columns provided, but you are told that
prices has five rows, so you should use all rows
provided.
Note: There are several correct answers to this
question.","We can use the merged DataFrame to figure out the prices
that correlate to each game in stores. We see in
merged the price for Tickets to Ride should be
47.99, so we create a row for that game. We repeat this process to find
the remaining rows. Since we know that prices have 5 rows
we then make a game and price up. Note that in the solution above the
last row (index 4) has “Sushi Go” and 9.99.
These can be any game or any price that is not listed in indexes 0 to 4.
This is because prices has 5 rows and when we use
.merge() since the game “Sushi Go” is not in
stores it will not be added.",84.0,Easy
770,Wi,23,Final,,Problem 15,Problem 15.2,"What goes in blank (a)?

 min()
 max()
 count()
 mean()","count()
The problem asks us which store would allow us to buy as many games
as possible. The provided code is
merge.groupby(“Store”).__a__. We want to use the aggregate
method that allows us to find the number of games in each store. The
aggregation method for this would be count().",87.0,Easy
771,Wi,23,Final,,Problem 15,Problem 15.3,What goes in blank (b)?,"index[0]
Recall groupby() will cause the unique values from the
column “Store” to be in the index. The remaining part of
the code sorts the DataFrame so that the store with the most games is at
the top. This means the row at index 0 has the store and most number of
games inside of the DataFrame. To grab the element at the 1st index we
simply do index[0].",53.0,Medium
772,Wi,23,Final,,Problem 15,Problem 15.4,"Suppose you go to the store where_to_go and buy one copy
of each of the available games that you enjoyed at game night. How much
money will you spend? Write one line of code that
evaluates to the answer, using the merged DataFrame and no
others.","merged[merged.get(“Store”) == where_to_go].get(“Price”).sum()
We want to figure out how much money we would spend if we went to
where_to_go, which is the store where we can buy as many
games as possible. We can simply query the merged DataFrame to only
contain the rows where the store is equal to where_to_go.
We then can simply get the “Price” column and add all of
the values up by doing .sum() on the Series.",74.0,Medium
773,Wi,23,Final,,Problem 16,Problem 16,"We collect data on the play times of 100 games of Chutes and
Ladders (sometimes known as Snakes and Ladders) and want
to use this data to perform a hypothesis test.",,,
774,Wi,23,Final,,Problem 16,Problem 16.1,"Which of the following pairs of hypotheses can we test using this
data?
Option 1: Null Hypothesis: In a random sample of
Chutes and Ladders games, the average play time is 30 minutes.
Alternative Hypothesis: In a random sample of Chutes
and Ladders games, the average play time is not 30 minutes.
Option 2: Null Hypothesis: In a random sample of
Chutes and Ladders games, the average play time is not 30 minutes.
Alternative Hypothesis: In a random sample of Chutes
and Ladders games, the average play time is 30 minutes
Option 3: Null Hypothesis: A game of Chutes and
Ladders takes, on average, 30 minutes to play. Alternative
Hypothesis: A game of Chutes and Ladders does not take, on
average, 30 minutes to play.
Option 4: Null Hypothesis: A game of Chutes and
Ladders does not take, on average, 30 minutes to play.
Alternative Hypothesis: A game of Chutes and Ladders
takes, on average, 30 minutes to play.

 Option 1
 Option 2
 Option 3
 Option 4","Option 3
Option 3: is the correct answer because the Null
Hypothesis can be applicable to the real world, and thus simulated, and
has the test statistic “equal” to our prediction of 30 minutes. The
Alternative Hypothesis is also correctly different from the Null
Hypothesis by saying the test statistic is “not equal” to our prediction
of 30 minutes.
Option 1: We want the Null Hypothesis or Alternative
Hypothesis to be applicable to the real world, which means that by
having the start “In a random sample…” we are discrediting this in the
real world.
Option 2: Like Option 1, we want the Null Hypothesis
or Alternative Hypothesis to be applicable to the real world, which
means that by having the start “In a random sample…” we are discrediting
this in the real world.
Option 4: This answer is wrong because the Null
Hypothesis should be focused on figuring out the positive test
statistic, in this case average. In other words, let u be
the average time to play Chutes and Ladders and let
u<sub>0<\sub> be 30 minutes. The Null
Hypothesis should be u =
u<sub>0<\sub> and the Alternative Hypothesis
should be something different, in this case: u !=
u<sub>0<\sub>.",65.0,Medium
775,Wi,23,Final,,Problem 16,Problem 16.2,"We use our collected data to construct a 95% CLT-based confidence
interval for the average play time of a game of Chutes and
Ladders. This 95% confidence interval is [26.47, 28.47]. For the
100 games for which we collected data, what is the mean and standard
deviation of the play times?","mean = 27.47 and SD = 5
One of the key properties of the normal distribution is that about
95% of values lie within 2 standard deviations of the mean. The Central
Limit Theorem states that the distribution of the sample mean is roughly
normal, which means that to create this CLT-based 95% confidence
interval, we used the 2 standard deviations rule.
What we’re given, then, is the following:
\begin{align*} \text{Sample Mean} + 2
\cdot \text{SD of Distribution of Possible Sample Means} &= 28.47 \\
\text{Sample Mean} - 2 \cdot \text{SD of Distribution of Possible Sample
Means} &= 26.47 \end{align*}
The sample mean is halfway between 26.47 and 28.47, which is 27.47.
Substituting this into the first equation gives us
\begin{align*}27.47 + 2 \cdot \text{SD of
Distribution of Possible Sample Means} &= 28.47\\2 \cdot \text{SD of
Distribution of Possible Sample Means} &= 1 \\ \text{Distribution of
Possible Sample Means} &= 0.5\end{align*}
It can be tempting to conclude that the sample standard deviation is
0.5, but it’s not – the SD of the sample mean’s distribution is 0.5.
Remember, the SD of the sample mean’s distribution is given by the
square root law:
\text{SD of Distribution of Possible
Sample Means} = \frac{\text{Population SD}}{\sqrt{\text{Sample Size}}}
\approx \frac{\text{Sample SD}}{\sqrt{\text{Sample Size}}}
We don’t know the population SD, so we’ve used the sample SD as an
estimate. As such, we have that
\text{SD of Distribution of Possible
Sample Means} = 0.5 = \frac{\text{Sample SD}}{\sqrt{\text{Sample Size}}}
= \frac{\text{Sample SD}}{\sqrt{100}}
So, \text{Sample SD} = 0.5 \cdot \sqrt{100}
= 0.5 \cdot 10 = 5.",64.0,Medium
776,Wi,23,Final,,Problem 16,Problem 16.3,"Does the CLT say that the distribution of play times of the 100 games
is roughly normal?

 Yes
 No","No
The Central Limit Theorem states that the distribution of the sample
mean or the sample sum is roughly normal. The distribution of play times
is a sample of size 100 drawn from the population of play times; the
Central Limit Theorem doesn’t say anything about a population or any one
sample.",45.0,Hard
777,Wi,23,Final,,Problem 16,Problem 16.4,"Of the two hypotheses you selected in part (a), which one is better
supported by the data?

 Null Hypothesis
 Alternative Hypothesis","Alternative Hypothesis
To test the null hypothesis, we check whether 30 is in the confidence
interval we constructed. 30 is not between 26.47 and 28.47, so we reject
the null hypothesis that the average play time is 30 minutes.",87.0,Easy
778,Wi,24,Final,,Problem 1,Problem 1,"Which of the following columns would be an appropriate index for the
olympians DataFrame?

 ""Name""
 ""Sport""
 ""Team""
 None of these.","None of these.
To decide what an appropriate index would be, we need to keep in mind
that in each row, the index should have a unique value – that is, we
want the index to uniquely identify rows of the DataFrame. In this case,
there will obviously be repeats in ""team"" and
""sport"", since these will appear multiple times for each
Olympic event. Although the name is unique for each athlete, the same
athlete could compete in multiple Olympics (for example, Michael Phelps
competed in both 2008 and 2004). So, none of these options is a valid
index.",74.0,Medium
779,Wi,24,Final,,Problem 2,Problem 2,"Frank X. Kugler has Olympic medals in three sports (wrestling,
weightlifting, and tug of war), which is more sports than any other
Olympic medalist. Furthermore, his medals for all three of these sports
are included in the olympians DataFrame. Fill in the blanks
below so that the expression below evaluates to
""Frank X. Kugler"".
                (olympians.groupby(__(a)__).__(b)__
                          .reset_index()
                          .groupby(__(c)__).__(d)__
                          .sort_values(by=""Age"", ascending=False)
                          .index[0])",,,
780,Wi,24,Final,,Problem 2,Problem 2.1,What goes in blank (a)?,"[""Name"", ""Sport""] or
[""Sport"", ""Name""]
The question wants us to find the name (Frank X. Kugler) who has
records that correspond to three distinct sports. We know that the same
athlete might have multiple records for a distinct sport if they
participated in the same sport for multiple years. Therefore we should
groupby ""Name"" and ""Sport"" to create a
DataFrame with unique Name-Sport pairs. This is a DataFrame that
contains the athletes and their sports (for each athlete, their
corresponding sports are distinct). If an athlete participated in 2
sports, for example, they would have 2 rows corresponding to them in the
DataFrame, 1 row for each distinct sport.",75.0,Easy
781,Wi,24,Final,,Problem 2,Problem 2.2,What goes in blank (b)?,".sum() or .mean()
or .min(), etc.
Any aggregation method applied on df.groupby() would
work. We only want to remove cases in which an athlete participates in
the same sport for multiple years, and get unique name-sport pairs.
Therefore, we don’t care about the aggregated numeric value. Notice
.unique() is not correct because it is not an aggregation
method used on dataframe after grouping by. If you use
.unique(), it will give you “AttributeError:
‘DataFrameGroupBy’ object has no attribute ‘unique’”. However,
.unique() can be used after Series.groupby().
For more info: [link]
(https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.SeriesGroupBy.unique.html)",96.0,Easy
782,Wi,24,Final,,Problem 2,Problem 2.3,What goes in blank (c)?,"""Name""
Now after resetting the index, we have ""Name"" and
""Sport"" columns containing unique name-sport pairs. The
objective is to count how many different sports each Olympian has medals
in. To do that, we groupby ""Name"" and later use the
.count() method. This would give a new DataFrame that has a
count of how many times each name shows up in our previous DataFrame
with unique name-sport pairs.",81.0,Easy
783,Wi,24,Final,,Problem 2,Problem 2.4,What goes in blank (d)?,".count()
The .count() method is applied to each group. In this
context, .count() will get the number of entries for each
Olympian across different sports, since the previous steps ensured each
sport per Olympian is uniquely listed (due to the initial groupby on
both ""Name"" and ""Sport""). It does not matter
what we sort_values by, because the
.groupby('Name').count() method will just put a count of
each ""Name"" in all of the columns, regardless of the column
name or what value was originally in it.",64.0,Medium
784,Wi,24,Final,,Problem 3,Problem 3,"In olympians, ""Weight"" is measured in
kilograms. There are 2.2 pounds in 1 kilogram. If we converted
""Weight"" to pounds instead, which of the following
quantities would increase?

 The mean of the ""Weight"" distribution.
 The standard deviation of the ""Weight"" distribution.
 The proportion of ""Weight"" values within 3 standard
deviations
 The correlation between ""Height"" and
""Weight"".
 The slope of the regression line predicting ""Weight""
from
 The slope of the regression line predicting ""Height""
from","Options 1, 2, and 5 are correct.

The mean of the new distribution where weight is in pounds would be
given by 2.2 \cdot \text{mean} in
kilograms. Since the mean in kilograms is positive, this quantity must
then increase if we multiply it by 2.2. Intuitively this should make
sense as we are basically scaling all the (positive) values by 2.2 (a
positive constant), so we expect the values to increase and thus the
mean to increase. So, option 1 is correct.
The standard deviation of the new distribution where weight is in
pounds would be given by 2.2  \cdot
\text{standard deviation} in kilograms. Since the standard
deviation in kilograms is positive, this quantity must then increase if
we multiply it by 2.2. Intuitively this should make sense as we are
scaling all the values by 2.2, so the difference between larger and
smaller values will be greater once they are scaled. Thus, we expect the
spread of our distribution to be larger, and the standard deviation to
increase. So, option 2 is correct.
We are basically interested in the proportion of x_{i} that satisfy \vert \frac{x_{i} \cdot μ}{σ} \vert<3,
with x_{i} being the values of
""Weight"", μ being the mean in kg and σ being the standard
deviation in kg. Once we scale everything by 2.2 to convert from
kilograms to pounds, we have that
\vert
\frac{2.2x_{i}-2.2μ}{2.2σ}\vert<3 → \frac{2.2}{2.2}\vert
\frac{x~i~*μ}{σ}\vert<3 → \vert
\frac{x_{i} \cdot μ}{σ}\vert<3.
Notice that after we scaled it to pounds, the equation still ends up the
exact same as the one in kg. Thus, the proportion of
""Weight"" within 3 standard deviations of the mean stays the
same. Intuitively this should make sense because we are scaling
everything by the same amount, so the proportion of points that are a
specific number of standard deviations away from the mean should be the
same, since the standard deviation and mean get scaled as well. So,
option 3 is incorrect.
Recall that the correlation coefficient, r, is the average value of
the product of two variables, when both variables are measured in
standard units. In kilograms, ""Weight""(kg) in standard
units is \frac{x~i~−μ}{σ}. Similar to
option 3, ""Weight""(pounds) in standard units is \frac{2.2x_{i}-2.2μ}{2.2σ} = \frac{2.2}{2.2}
\frac{x_{i} \cdot μ}{σ} = \frac{x_{i} \cdot μ}{σ}. Again, notice
that the equation in pounds ends up the exact same as in kilograms. The
same applies for ""Height"" in standard units. Since none of
the variables change when measured in standard units, r doesn’t change.
So, option 4 is incorrect.
Intuitively, this makes sense when we imagine a scatterplot with the
y-axis (value being predicted) representing ""Weight"" and
the x-axis representing ""Height"". We expect that the taller
someone is, the heavier they will be. So we can expect a positive
regression line slope between Weight and Height. When we convert Weight
from kg to pounds, we are scaling every value in ""Weight"",
making their values increase. When we scale the the weight values
(y-values) to become bigger, we are making the regression slope even
steeper, because an increase in Height (x) now corresponds to an even
larger increase in Weight(y). So, option 5 is correct.
Similar to the last problem, we can also imagine a scatterplot
except this time with ""Weight"" on the x-axis and
""Height"" on the y-axis. Since we are increasing the values
of ""Height"", we can imagine stretching the x-axis’ values
without changing the y-values, which makes the line more flat and
therefore decreases the slope. So, option 6 is incorrect. Another
approach to both 5 and 6 is to utilize the correlation coefficient
r, which is equal to the slope times \frac{σ~y~}{σ~x~}. We know that multiplying a
set values by a value greater than one increases the spread of the data
which increases standard deviation. In option 5, ""Weight""
is in the y-axis, and increasing the numerator of the fraction \frac{σ~y~}{σ~x~} increases r. In
option 6, ""Weight"" is in the x-axis, and increasing the
denominator the fraction \frac{σ~y~}{σ~x~} decreases r.",80.0,Easy
785,Wi,24,Final,,Problem 4,Problem 4,"The Olympics are held every two years, in even-numbered years,
alternating between the Summer Olympics and Winter Olympics. Summer
Olympics are held in years that are a multiple of 4
(such as 2024), and Winter Olympics are held in years that are not a
multiple of 4 (such as 2022 or 2026).
We want to add a column to olympics that contains either
""Winter"" or ""Summer"" by applying a function
called season as follows:
    olympians.assign(Season=olympians.get(""Year"").apply(season))
Which of the following definitions of season is correct?
Select all that apply.
Notes:

We’ll ignore the fact that the 2020 Olympics were rescheduled to
2021 due to Covid.
Recall that the % operator in Python gives the
remainder upon division. For example, 8 % 3 evaluates to
2.

Way 1:
        def season(year):
            if year % 4 == 0:
                return ""Summer""
            return ""Winter""
Way 2:
        def season(year):
            return ""Winter""
            if year % 4 == 0:
                return ""Summer""
Way 3:
        def season(year):
            if year % 2 == 0:
                return ""Winter""
            return ""Summer""
Way 4:
        def season(year):
            if int(year / 4) != year / 4:
                return ""Winter""
            return ""Summer""

 Way 1
 Way 2
 Way 3
 Way 4","Way 1 and Way 4

Way 1: This function first checks if the year is divisible by 4, and
returns “Summer” if it is. If the year isn’t divisible by 4, then the
code inside that if statement won’t execute, and we move on to the next
line of code, which just returns “Winter”, as we wanted. So, way 1 is
correct.
Way 2 looks similar to way 1, but has one key difference: the
return ""Winter"" line is before the if
statement. Since nothing after a return statement gets executed
(assuming that the return statement gets executed), no matter what the
year is this function will always return “Winter”. So, way 2 is
incorrect.
Way 3 doesn’t account for the fact that all years which are
multiples of 4 are also multiples of 2. So even though it gets the 2022
Winter Olympics correct, it will also return “Winter” for 2020 since
2020 % 2 evaluates to 0. So, way 3 is
incorrect.
Way 4 uses similar logic to way 1, just using a different method to
check for divisibility. Instead of using the modulo operator, we check
if casting year / 4 to an integer using int
changes its value. If the two aren’t equal, then we know that
year / 4 wasn’t an integer before casting, which means that
the year isn’t divisible by 4 and we should return
""Winter"". If the code inside the if statement doesn’t
execute, then we know that the year is divisible by 4, so we return
“Summer”. So, way 4 is correct.",89.0,Easy
786,Wi,24,Final,,Problem 5,Problem 5,"In figure skating, skaters move around an ice rink performing a
series of skills, such as jumps and spins. Ylesia has been training for
the Olympics, and she has a set routine that she plans to perform.
Let’s say that Ylesia performs a skill successfully if she does not
fall during that skill. Each skill comes with its own probability of
success, as some skills are harder and some are easier. Suppose that the
probabilities of success for each skill in Ylesia’s Olympic routine are
stored in an array called skill_success.
For example, if Ylesia’s Olympic routine happened to only contain
three skills, skill_success might be the array with values
0.92, 0.84, 0.92. However, her routine can contain any number of
skills.",,,
787,Wi,24,Final,,Problem 5,Problem 5.1,"Ylesia wants to simulate one Olympic routine to see how many times
she might fall. Fill in the function count_falls below,
which takes as input an array skill_success and returns as
output the number of times Ylesia falls during her Olympic routine.

        def count_falls(skill_success):
            falls = 0
            for p in skill_success:
                result = np.random.multinomial(1, __(a)__)
                falls = __(b)__
            return falls","(a): [p, 1-p], (b):
falls + result[1] OR (a): [1-p, p], (b):
falls + result[0]

(a) First, we should think about what
np.random.multinomial is trying to do here. It’s trying to make an array
of how many times each scenario happened. There are 2 possible scenarios
here: Ylesia succeeds or Ylesia fails. In this code, p is the
probability that Ylesia succeeds a skill, and therefore the probabilty
that Ylesia does not succeed (she fails) will be 1-p. So to properly
simulate how many times she falls, we should put [p, 1-p]
in blank (a).
(b) Our answer from (a) will make an array stored
in result, with index 0 being how many times she succeeded
(corresponds to p), and index 1 being how many times she fell
(corresponds to 1-p). Since index 1 corresponds to the scenario in which
she falls, in order to correctly increase the number of falls, we add
falls by result[1]. Therefore, blank (b) is
falls + result[1].

Likewise, you can change the order with (a): [1-p, p]
and (b): falls + result[0] and it would still correctly
simulate how many times she falls.",59.0,Medium
788,Wi,24,Final,,Problem 5,Problem 5.2,"Fill in the blanks below so that prob_no_falls evaluates
to the exact probability of Ylesia performing her entire routine without
falling.
        prob_no_falls = __(a)__
        for p in skill_success:
            prob_no_falls = __(b)__
        prob_no_falls","(a): 1, (b):
prob_no_falls * p

(a) We start with the initial value of
prob_no_falls. This should be set to 1 because we’re
computing a probability product, and starting with 1 ensures the initial
value doesn’t affect the multiplication of subsequent
probabilities.
(b) Inside the for-loop, we want to update
prob_no_falls by multiplying it by each probability of
success (p) in skill_success. This is because
the probability of Ylesia not falling throughout multiple independent
skills is the product of her not falling during each skill.",72.0,Medium
789,Wi,24,Final,,Problem 5,Problem 5.3,"Fill the blanks below so that approx_prob_no_falls
evaluates to an estimate of the probability that Ylesia performs her
entire routine without falling, based on 10,000 trials. Feel free to use
the function you defined in part (a) as part of your solution.
        results = np.array([])
        for i in np.arange(10000):
            results = np.append(results, __(a)__)
        approx_prob_no_falls = __(b)__
        approx_prob_no_falls","(a): count_falls(skill_success),
(b): np.count_nonzero(results == 0) / 10000, though there
are many other correct solutions

(a) For this question, we are doing a simulation
where we calculate the probability of Ylesia not falling during her
routine based on 10,000 trials. To do so, we want to find out the number
of times that Yelsia did not fall any skill during her routine out of
the 10,000 trials. Based on the given codes, we have an array where we
are appending something into that array for each trial. We can utilize
the function defined in part a to calculate the number of times Ylesia
falls during a single trial so blank a will be
count_falls(skill_success).
(b) After 10,000 iterations, we have an array of
the number of falls for each trial. Then, we want to count the number of
times that we get 0 in that array, which means Ylesia did not fall.
Lastly, to get the probability, we will need to divide by the total
number of trials which is 10,000. This gives us the answer for blank b:
np.count_nonzero(results == 0) / 10000.",66.0,Medium
790,Wi,24,Final,,Problem 6,Problem 6,"Suppose we sample 400 rows of olympians
at random without replacement, then generate a 95% CLT-based confidence
interval for the mean age of Olympic medalists based on this sample.",,,
791,Wi,24,Final,,Problem 6,Problem 6.1,"The CLT is stated for samples drawn with replacement, but in
practice, we can often use it for samples drawn without replacement.
What is it about this situation that makes it reasonable to still use
the CLT despite the sample being drawn without replacement?

 The sample is much smaller than the population.
 The statistic is the sample mean.
 The CLT is less computational than bootstrapping, so we don’t need to
sample with replacement like we would for bootstrapping.
 The population is normally distributed.
 The sample standard deviation is similar to the population standard
deviation.","The sample is much smaller than the
population.
The Central Limit Theorem (CLT) states that regardless of the shape
of the population distribution, the sampling distribution of the sample
mean will be approximately normally distributed if the sample size is
sufficiently large. The key factor that makes it reasonable to still use
the CLT, is the sample size relative to the population size. When the
sample size is much smaller than the population size, as in this case
where 400 rows of Olympians are sampled from a likely much larger
population of Olympians, the effect of sampling without replacement
becomes negligible.",47.0,Hard
792,Wi,24,Final,,Problem 6,Problem 6.2,"Suppose our 95% CLT-based confidence interval for the mean age of
Olympic medalists is [24.9, 26.1]. What
was the mean of ages in our sample?","Mean = 25.5
We calculate the mean by first determining the width of our interval:
26.1 - 24.9 = 1.2, then we divide this width in half to get 0.6 which
represents the distance from the mean to each side of the confidence
interval. Using this we can find the mean in two ways: 24.9 + 0.6 = 25.5
OR 26.1 - 0.6 = 25.5.",90.0,Easy
793,Wi,24,Final,,Problem 6,Problem 6.3,"Suppose our 95% CLT-based confidence interval for the mean age of
Olympic medalists is [24.9, 26.1]. What
was the standard deviation of ages in our sample?","Standard deviation = 6
We can calculate the sample standard deviation (sample SD) by using
the 95% confidence interval equation:
\text{sample mean} - 2 * \frac{\text{sample
SD}}{\sqrt{\text{sample size}}}, \text{sample mean} + 2 *
\frac{\text{sample SD}}{\sqrt{\text{sample size}}}.
Choose one of the end points and start plugging in all the
information you have/calculated:
25.5 - 2*\frac{\text{sample
SD}}{\sqrt{400}} = 24.9 → \text{sample
SD} = \frac{(25.5 - 24.9)}{2}*\sqrt{400} = 6.",55.0,Medium
794,Wi,24,Final,,Problem 7,Problem 7,"In our sample, we have data on 1210 medals for the
sport of gymnastics. Of these, 126 were awarded to
American gymnasts, 119 were awarded to Romanian
gymnasts, and the remaining 965 were awarded to
gymnasts from other nations.
We want to do a hypothesis test with the following hypotheses.
Null: American and Romanian gymnasts win an equal
share of Olympic gymnastics medals.
Alternative: American gymnasts win more Olympic
gymnastics medals than Romanian gymnasts.",,,
795,Wi,24,Final,,Problem 7,Problem 7.1,"Which test statistic could we use to test these hypotheses?

 total variation distance between the distribution of medals by
country and the uniform distribution
 proportion of medals won by American gymnasts
 difference in the number of medals won by American gymnasts and the
number of medals won by Romanian gymnasts
 absolute difference in proportion of medals won by American gymnasts
and proportion of medals won by Romanian gymnasts","difference in the number of medals won by
American gymnasts and the number of medals won by Romanian gymnasts
To test this pair of hypotheses, we need a test statistic that is
large when the data suggests that we reject the null hypothesis, and
small when we fail to reject the null. Now let’s look at each
option:
- Option 1: Total variation distance across all the
countries won’t tell us about the differences in medals between
Americans and Romanians. In this case, it only tells how different the
proportions are across all countries in comparison to the uniform
distribution, the mean proportion. - Option 2: This
test statistic doesn’t take into account the number of medals Romanians
won. Imagine a situation where Romanians won half of all the medals and
Americans won the other half, and no other country won any medals. In
here, they won the same amount of medals and the test statistic would be
1/2. Now imagine if Americans win half the medals, some other country
won the other half, and Romanians won no medals. In this case, the
Americans won a lot more medals than Romanians but the test statistic is
still 1/2. A good test statistic should point to one hypothesis when
it’s large and the other hypothesis when it’s small. In this test
statistic, 1/2 points to both hypotheses, making it a bad test
statistic.
- Option 3: In this test statistic, when Americans win
an equal amount of medals as Romanians, the test statistic would be 0, a
very small number. When Americans win way more medals than Romanians,
the test statistic is large, suggesting that we reject the null
hypothesis in favor of the alternative. You might notice that when
Romanians win way more medals than Americans, the test statistic would
be negative, suggesting that we fail to reject the null hypothesis that
they won equal medals. But recall that failing to reject the null
doesn’t necessarily mean we think the null is true, it just means that
under our null hypothesis and alternative hypothesis, the null is
plausible. The important thing is that the test statistic points to the
alternative hypothesis when it’s large, and points to the null
hypothesis when it’s small. This test statistic does just that, so
option 3 is the correct answer.
- Option 4: Since this statistic is an absolute value,
large values signify a large difference in the two proportions, while
small values signify a small difference in two proportions. However, it
doesn’t tell which country wins more because a large value could mean
that either country has a higher proportion of medals than the other.
This hypothesis would only be helpful if our alternative hypothesis was
that the number of medals the Americans/ Romanians win are
different.",73.0,Medium
796,Wi,24,Final,,Problem 7,Problem 7.2,"Below are four different ways of testing these hypotheses. In each
case, fill in the calculation of the observed statistic in the variable
observed, such that p_val represents the
p-value of the hypothesis test.
Way 1:
        many_stats = np.array([])
        for i in np.arange(10000):
            result = np.random.multinomial(245, [0.5, 0.5]) / 245
            many_stats = np.append(many_stats, result[0] - result[1])
        observed = __(a)__
        p_val = np.count_nonzero(many_stats >= observed)/len(many_stats)
Way 2:
        many_stats = np.array([])
        for i in np.arange(10000):
            result = np.random.multinomial(245, [0.5, 0.5]) / 245
            many_stats = np.append(many_stats, result[0] - result[1])
        observed = __(b)__
        p_val = np.count_nonzero(many_stats <= observed)/len(many_stats)
Way 3:
        many_stats = np.array([])
        for i in np.arange(10000):
            result = np.random.multinomial(245, [0.5, 0.5]) / 245
            many_stats = np.append(many_stats, result[0])
        observed = __(c)__ 
        p_val = np.count_nonzero(many_stats >= observed)/len(many_stats)
Way 4:
        many_stats = np.array([])
        for i in np.arange(10000):
            result = np.random.multinomial(245, [0.5, 0.5]) / 245
            many_stats = np.append(many_stats, result[0])
        observed = __(d)__ 
        p_val = np.count_nonzero(many_stats <= observed)/len(many_stats)","Way 1: 126/245 - 119/245 or
7/245
First, let’s look at what this code is doing. The line
result = np.random.multinomial(245, [0.5, 0.5]) / 245 makes
an array of length 2, where each of the 2 elements contains the amount
of the 245 total medals corresponding to the amount of medals won by
American gymnasts and Romanian gymnasts respectively. We then divide
this array by 245 to turn them into proportions out of 245 (which is the
sum of 126+119). This array of proportions is then assigned to
result. For example, one of our 10000 repetitions could
assign np.array([124/245, 121/245]) to result.
The following line,
many_stats = np.append(many_stats, result[0] - result[1]),
appends the difference between the first proportion in
result and the second proportion in result to
many_stats. Using our example, this would append 124/245 -
121/245 (which equals 3/245) to many_stats. To determine
how we calculate the observed statistic, we have to consider how we are
calculating the p-value. In order to calculate the p-value, we need to
determine how frequent it is to see a result as extreme as our observed
statistic, or more extreme in the direction of the alternative
hypothesis. The alternative hypothesis states that American gymnasts win
more medals than Romanian gymnasts, meaning that we are looking for
results in many_stats where the difference is equal to or
greater than (more extreme than) 126/245 - 119/245 (which equals 7/245).
That is, the final line of code in Way 1 is using
np.count_nonzero to find the amount of differences in
many_stats greater than 7/245. Therefore, observed must
equal 7/245.",56.0,Medium
797,Wi,24,Final,,Problem 7,Problem 7.3,"The four p-values calculated in Ways 1 through 4 are:

 exactly the same
 similar, but not necessarily exactly the same
 not necessarily similar","similar, but not necessarily the same
All of these differences in test statistics and different p-values
all are different, however, they are all geared towards testing through
the same null and alternative hypothesis. Although they are all
different methods, they are all trying to prove the same conclusion.",71.0,Medium
798,Wi,24,Final,,Problem 8,Problem 8,,,,
799,Wi,24,Final,,Problem 8,Problem 8.1,"In Olympic hockey, the number of goals a team scores is linearly
associated with the number of shots they attempt. In addition, the
number of goals a team scores has a mean of 10 and a standard deviation
of 5, whereas the number of attempted shots has a mean of 30 and a
standard deviation of 10.
Suppose the regression line to predict the number of goals based on
the number of shots predicts that for a game with 20 attempted shots, 6
goals will be scored. What is the correlation between the number of
goals and the number of attempted shots? Give your answer as an exact
fraction or decimal.","\frac{4}{5}
Recall that the formula of the regression line in standard units is
y_{su}=r \cdot x_{su}. Since we are
predicting # of goals from the # of shots, let x_{su} represent # of shots in standard units
and y_{su} represent # of goals in
standard units. Using the formula for standard units with information in
the problem, we find x_{su}=\frac{20-30}{10}=(-1) and y_{su}=\frac{6-10}{5}=(-\frac{4}{5}). Hence,
(-\frac{4}{5})=r \cdot (-1) and r=\frac{4}{5}.",74.0,Medium
800,Wi,24,Final,,Problem 8,Problem 8.2,"In Olympic baseball, the number of runs a team scores is linearly
associated with the number of hits the team gets. The number of runs a
team scores has a mean of 8 and a standard deviation of 4, while the
number of hits has a mean of 24 and a standard deviation of 6. Consider
the regression line that predicts the number of runs scored based on the
number of hits.

What is the maximum possible predicted number of runs for a team
that gets 27 hits?
What is the correlation coefficient in the case where the
predicted number of runs for a team with 25 hits is as large as
possible?","i) 10
Consider the standard unit regression line again, y_{su}=r \cdot x_{su}. Since we are
predicting # of runs from the # of hits, let xsu represent # of hits in
standard units and ysu represent # of runs in standard units. In part b,
we are hoping to find the maximal y_{su} given the x_{su}. Via formula for standard units, we
know x_{su}=\frac{27-24}{6}=\frac{1}{2}. Because
x_{su} is positive, we know that to
achieve the maximum prediction in y_{su}, the correlation r must also be
positive and its largest possible value. Since the value of r must be
between -1 and 1, we know that to satisfy the prior condition, r=1.
Plugging everything back, we find that y_{su}=1 \cdot \frac{1}{2}. We reverse our
operations to find the actual predicted # of runs y (not in standard
units). \frac{1}{2}=\frac{y-8}{4} and
so y = 10.",63.0,Medium
801,Wi,24,Final,,Problem 9,Problem 9,"In 2024, the Olympics will include breaking (also known as
breakdancing) for the first time. The breaking competition will include
16 athletes, who will compete in a single-elimination
tournament.
In the first round, all 16 athletes will compete against an opponent
in a face-to-face “battle"". The 8 winners, as determined by the judges,
will move on to the next round. Elimination continues until the final
round contains just 2 competitors, and the winner of this final battle
wins the tournament.
The table below shows how many competitors participate in each
round:


After the 2024 Olympics, suppose we make a DataFrame called
breaking containing information about the performance of
each athlete during each round. breaking will have one row
for each athlete’s performance in each round that they participated.
Therefore, there will be 16+8+4+2 =
30 rows in breaking.
In the ""name"" column of breaking, we will
record the athlete’s name (which we’ll assume to be unique), and in the
other columns we’ll record the judges’ scores in the categories on which
the athletes will be judged (creativity, personality, technique,
variety, performativity, and musicality).",,,
802,Wi,24,Final,,Problem 9,Problem 9.1,"How many rows of breaking correspond to the winner of
the tournament? Give your answer as an integer.","4
Since the winner of the tournament must have won during the 1st, 2nd,
3rd, and final rounds, there will be a total of four rows in
breaking corresponding to this winner.",94.0,Easy
803,Wi,24,Final,,Problem 9,Problem 9.2,"How many athletes’ names appear exactly twice in the
""name"" column of breaking? Give your answer as
an integer.","4
For an athlete to appear on exactly two rows in
breaking, they must get through the 1st round but get
eliminated in the 2nd round. There are a total of 8 athletes in the 2nd
round, of which 4 are eliminated.",82.0,Easy
804,Wi,24,Final,,Problem 9,Problem 9.3,"If we merge breaking with itself on the
""name"" column, how many rows will the resulting DataFrame
have? Give your answer as an integer.
Hint: Parts (a) and (b) of this question are relevant to
part (c).","74
Let’s consider 4 separate cases, the athletes who make it into the
final round, the athletes who are eliminated in the 3rd round, the
athletes who are eliminated in the 2nd round, and the athletes who are
eliminated in the 1st round. There are two athletes in the final round,
and both of their names appear four times. When merging
breaking with itself, these two athletes will appear 16
times each in the merged dataframe (4 \cdot
4). Two athletes are eliminated in the 3rd round, and their names
each appear 3 times in breaking. When merging
breaking with itself, these two athletes will appear 9
times each in the merged dataframe (3 \cdot
3). Four athletes are eliminated in the 2nd round, and their
names each appear twice in breaking. When merging
breaking with itself, these four athletes will appear 4
times each in the merged dataframe (2 \cdot
2). Eight athletes are eliminated in the 1st round, and their
names each appear once in breaking. When merging
breaking with itself, these eight athletes will appear 1
time each in the merged dataframe (1 \cdot
1). Now, we can add these numbers all up, 16 \cdot 2 + 9 \cdot 2 + 4 \cdot 4 + 8 \cdot 1 =
74.",39.0,Hard
805,Wi,24,Final,,Problem 9,Problem 9.4,"Recall that the number of competitors in each round is 16, 8, 4, 2. Write one line of code that
evaluates to the array np.array([16, 8, 4, 2]). You
must use np.arange in your solution, and
you may not use np.array or the DataFrame
breaking.","2 ** np.arange(4, 0, -1)
We notice that 16 = 2^4, 8 = 2^3, 4 =
2^2, and 2 = 2^1. We can use
this pattern and write an expression in the form of
2 ** (something). Notice, the exponent terms 4, 3, 2, and 1
are evenly distributed values, and [4, 3, 2, 1] can be generated by
np.arange as follows: np.arange(4, 0, -1). 4 is the
starting number, 0 is the ending number (exclusive), and -1 is the step
size.",38.0,Hard
806,Wi,24,Final,,Problem 10,Problem 10,"We want to use the sample of data in olympians to
estimate the mean age of Olympic beach volleyball players.",,,
807,Wi,24,Final,,Problem 10,Problem 10.1,"Which of the following distributions must be normally distributed in
order to use the Central Limit Theorem to estimate this parameter?

 The age distribution of all Olympic athletes.
 The age distribution of Olympic beach volleyball players.
 The age distribution of Olympic beach volleyball in our sample.
 None of the above.","None of the Above
The central limit theorem states that the distribution of possible
sample means and sample sums is approximately normal,
no matter the distribution of the population. Options A, B, and C are
not probability distributions of the sum or mean of a large random
sample draw with replacement.",72.0,Medium
808,Wi,24,Final,,Problem 10,Problem 10.2,"(10 pts) Next we want to use bootstrapping to estimate this
parameter. Which of the following code implementations correctly
generates an array called sample_means containing 10,000 bootstrapped sample means?
Way 1:
    sample_means = np.array([])
    for i in np.arange(10000):
        bv = olympians[olympians.get(""Sport"") == ""Beach Volleyball""]
        one_mean = (bv.sample(bv.shape[0], replace=True)
                      .get(""Age"").mean())
        sample_means = np.append(sample_means, one_mean)
Way 2:
    sample_means = np.array([])
    for i in np.arange(10000):
        bv = olympians[olympians.get(""Sport"") == ""Beach Volleyball""]
        one_mean = (olympians.sample(olympians.shape[0], replace=True)
                             .get(""Age"").mean())
        sample_means = np.append(sample_means, one_mean)
Way 3:
    sample_means = np.array([])
    for i in np.arange(10000):
        resample = olympians.sample(olympians.shape[0], replace=True)
        bv = resample[resample.get(""Sport"") == ""Beach Volleyball""]
        one_mean = bv.get(""Age"").mean()
        sample_means = np.append(sample_means, one_mean)
Way 4:
    sample_means = np.array([])
    bv = olympians[olympians.get(""Sport"") == ""Beach Volleyball""]
    for i in np.arange(10000):
        one_mean = (bv.sample(bv.shape[0], replace=True)
                      .get(""Age"").mean())
        sample_means = np.append(sample_means, one_mean)
Way 5:
    sample_means = np.array([])
    bv = olympians[olympians.get(""Sport"") == ""Beach Volleyball""]
    one_mean = (bv.sample(bv.shape[0], replace=True)
                  .get(""Age"").mean())
    for i in np.arange(10000):
        sample_means = np.append(sample_means, one_mean)

 Way 1
 Way 2
 Way 3
 Way 4
 Way 5","Way 1 and Way 4

Way 1 first creates a DataFrame bv, which is a subset
DataFrame of olympians but filtered to only have
""Beach Volleyball"". It then samples from bv
with replacement, and counts the mean of that sample and stores it in
the variable one_sample. It does this 10,000 times (due to
the for loop), each time creating a dataframe bv, sampling
from it, calculating a mean, and then appending one_sample
to the array sample_means. This is a correct way to
bootstrap.
Way 2 is incorrect because it calculates one_mean using a sample
from the entire olympians DataFrame, instead of the bv
DataFrame with only the ""Beach Volleyball"" players. This
will result in a sample of players of any sport, and the mean of those
ages will be calculated.
Way 3 is incorrect because it queries for rows where the data in the
""Sport"" column equals ""Beach Volleyball"" after
sampling from the DataFrame instead of before. This would lead to a mean
that is not representative of a sample of volleyball players’ ages
because we are sampling from all the rows with all different sports,
most likely resulting in a smaller sample size of volleyball players.
There would also be an inconsistent number of
""Beach Volleyball"" players in each sample.
Way 4 is essentially the same as Way 1, except it creates the
DataFrame bv before the for loop. The DataFrame
bv will always be the same, so it doesn’t really matter if
we make bv before or after the for loop.
Way 5 is incorrect because the one_mean is calculated
only once, but is appended to the sample_means array 10,000
times. As a result, the same mean is being appended, instead of a
different mean being calculated and appended each iteration.",88.0,Easy
809,Wi,24,Final,,Problem 10,Problem 10.3,"For most of the answer choices in part (b), we do not have enough
information to predict how the standard deviation of
sample_means would come out. There is one answer choice,
however, where we do have enough information to compute the standard
deviation of sample_means. Which answer choice is this, and
what is the standard deviation of sample_means for this
answer choice?

 Way 1
 Way 2
 Way 3
 Way 4
 Way 5","Way 5
Way 5 results in a sample_means array with the same mean appended
10,000 times. As a result the standard deviation would be 0 because the
entire array would be the same value repeated.",57.0,Medium
810,Wi,24,Final,,Problem 10,Problem 10.4,"There are 68 rows of olympians
corresponding to beach volleyball players. Assume that in part (b), we
correctly generated an array called sample_means containing
10,000 bootstrapped sample mean ages based on this original sample of 68
ages. The standard deviation of the original sample of 68 ages is
approximately how many times larger than the standard deviation of
sample_means? Give your answer to the nearest integer.","8
Recall SD of sample_means = \frac{\text{Population SD}}{\sqrt{\text{sample
size}}}. The sample size equals 68. Based on this equation, the
population SD is \sqrt{68} times larger
than the SD of distribution of possible sample means. \sqrt{68} rounded to the nearest integer is
8.",46.0,Hard
811,Wi,24,Final,,Problem 11,Problem 11,"Aladár Gerevich is a Hungarian fencer who is one of only two men to
win Olympic medals 28 years apart. He earned 10 Olympic medals in total
throughout his career: 7 gold, 1 silver, and 2 bronze. The table below
shows the distribution of medal types for Aladár Gerevich, as well as a
few other athletes who also earned 10 Olympic medals.",,,
812,Wi,24,Final,,Problem 11,Problem 11.1,"Which type of data visualization is most appropriate to compare two
athlete’s medal distributions?

 overlaid histogram
 overlaid bar chart
 overlaid line plot","overlaid bar chart
Here, we are plotting the data of 2 athletes, comparing the medal
distributions. Gold, silver, and bronze medals are categorical
variables, while the proportion of these won is a quantitative value. A
bar chart is the only kind of plot that involves categorical data with
quantitative data. Since there are 2 athletes, the most appropriate plot
is an overlaid bar chart. The overlapping bars would help compare the
difference in their distributions.",73.0,Medium
813,Wi,24,Final,,Problem 11,Problem 11.2,"Among the other athletes in the table above, whose medal distribution
has the largest total variation distance (TVD) to Aladár Gerevich’s
distribution?

 Katie Ledecky
 Alexander Dityatin
 Franziska van Almsick","Franziska van Almsick
The Total Variation Distance (TVD) of two categorical distributions
is the sum of the absolute differences of their proportions, all divided
by 2. We can apply the TVD formula to these distributions: The TVD
between Katie Ledecky and Aladar Gerevich is given by \frac{1}{2} \cdot (|0.7 - 0.7| + |0.1 - 0.3| + |0.2
- 0|) = \frac{0.4}{2} = 0.2. The TVD between Alexander Dityatin
and Aladar Gerevich is given by \frac{1}{2}
\cdot (|0.7 - 0.3| + |0.1 - 0.6| + |0.2 - 0.1|) = \frac{1}{2} =
0.5. And finally, the TVD between Franziska van Almsick and
Aladar Gerevich is given by \frac{1}{2} \cdot
(|0.7 - 0| + |0.1 - 0.4| + |0.2 - 0.6|) = \frac{1.4}{2} = 0.7.
So, Franziska van Almsick has the largest TVD to Gerevich’s
distribution.",92.0,Easy
814,Wi,24,Final,,Problem 11,Problem 11.3,"Suppose Pallavi earns 10 Olympic medals in such a way that the TVD
between Pallavi’s medal distribution and Aladár Gerevich’s medal
distribution is as large as possible. What is Pallavi’s medal
distribution?","x=0, y=1, z=0
Intuitively, can maximize the TVD between the distributions by
putting all of Pallavi’s medals in the category which Gerevich won the
least of, so x = 0, y = 1, z = 0. Moving any of these medals to another
category would decrease the TVD, since that would mean that all of
Pallavi’s medal proportions would get closer to Gerevich’s (Silver is
decreasing, getting closer, and Gold and Bronze are increasing, which
makes them closer as well).",72.0,Medium
815,Wi,24,Final,,Problem 11,Problem 11.4,"More generally, suppose medal_dist is an array of length
three representing an athlete’s medal distribution. Which of the
following expressions gives the maximum possible TVD between
medal_dist and any other distribution?

 medal_dist.max()
 medal_dist.min()
 1 - medal_dist.max()
 1 - medal_dist.min()
 np.abs(1 - medal_dist).sum()/2","1 - medal_dist.min()
Similar to part c, we know that the TVD is maximized by placing all
the medals of competitor A into the category in which competitor B has
the lowest proportion of medals. If we place all of competitor A’s
medals into this bin, the difference between the two distributions for
this variable will be 1 - medal_dist.min() In the other
bins, competitor A has no medals (making all their values 0), and
competitor B has the remainder of their medals, which is
1 - medal_dist.min(). So, in total, the TVD is given by
\frac{1}{2} \cdot 2 \cdot
1 - medal_dist.min() =
1 - medal_dist.min().",56.0,Medium
816,Wi,24,Final,,Problem 12,Problem 12,"Consider the DataFrame
olympians.drop(columns=[""Medal"", ""Year"", ""Count""]).",,,
817,Wi,24,Final,,Problem 12,Problem 12.1,"State a question we could answer using the data in this DataFrame and
a permutation test.","There are many possible answers for this
question. Some examples: “Are male olympians from Team USA significantly
taller than male olympians from other countries?”, “Are olympic swimmers
heavier than olympic figure skaters?”, “On average, are male athletes
heavier than female athletes?” Any question asking for a difference in a
numerical variable across olympians from different categories would
work, as long as it is not about the dropped columns.
Recall that a permutation test is basically trying to test if two
variables come from the same distribution or if the difference between
those two variables are so significant that we can’t possibly say that
they’re from the same distribution. In general, this means the question
would have to involve the age, height, or weight column because they are
numerical data.",80.0,Easy
818,Wi,24,Final,,Problem 12,Problem 12.2,"State the null and alternative hypotheses for this permutation
test.","Also many possible answers depending on your
answer for the first question.
For example, if our question was “Are olympic swimmers heavier than
olympic figure skaters?”, then the null hypothesis could be “Olympic
swimmers weigh the same as olympic figure skaters” and the alternative
could be “Olympic swimmers weigh more than figure skaters.”",80.0,Easy
819,Wi,24,Final,,Problem 13,Problem 13,"In our sample, we have data on 163 medals for the
sport of table tennis. Based on our data, China seems to really dominate
this sport, earning 81 of these medals.
That’s nearly half of the medals for just one country! We want to do
a hypothesis test with the following hypotheses to see if this pattern
is true in general, or just happens to be true in our sample.
Null: China wins half of Olympic table tennis
medals.
Alternative: China does not win half of Olympic
table tennis medals.",,,
820,Wi,24,Final,,Problem 13,Problem 13.1,"Why can these hypotheses be tested by constructing a confidence
interval?

 Since proportions are means, so we can use the CLT.
 Since the test aims to determine whether a parameter is equal to a
fixed value.
 Since we need to get a sense of how other samples would come out by
bootstrapping.
 Since the test aims to determine if our sample came from a known
population distribution.","Since the test aims to determine whether a
parameter is equal to a fixed value
The goal of a confidence interval is to provide a range of values
that, given the data, are considered plausible for the parameter in
question. If the null hypothesis’ fixed value does not fall within this
interval, it suggests that the observed data is not very compatible with
the null hypothesis. Thus in our case, if a 95% confidence interval for
the proportion of medals won by China does not include ~0.5, then
there’s statistical evidence at the 5% significance level to suggest
that China does not win exactly half of the medals. So again in our
case, confidence intervals work to test this hypothesis because we are
attempting to find out whether or half of the medals (0.5) lies within
our interval at the 95% confidence level.",44.0,Hard
821,Wi,24,Final,,Problem 13,Problem 13.2,"Suppose we construct a 95% bootstrapped CI for the proportion of
Olympic table tennis medals won by China. Select all true
statements.

 The true proportion of Olympic table tennis medals won by China has a
95% chance of falling within the bounds of our interval.
 If we resampled our original sample and calculated the proportion of
Olympic table tennis medals won by China in our resample, there is
approximately a 95% chance our interval would contain this number.
 95% of Olympic table tennis medals are won by China.
 None of the above.","If we resampled our original sample and
calculated the proportion of Olympic table tennis medals won by China in
our resample, there is approximately a 95% chance our interval would
contain this number.
The second option is the only correct answer because it accurately
describes the process and interpretation of a bootstrap confidence
interval. A 95% bootstrapped confidence interval means that if we
repeatedly sampled from our original sample and constructed the interval
each time, approximately 95% of those intervals would contain the true
parameter. This statement does not imply that the true proportion has a
95% chance of falling within any single interval we construct; instead,
it reflects the long-run proportion of such intervals that would contain
the true proportion if we could repeat the process indefinitely. Thus,
the confidence interval gives us a method to estimate the parameter with
a specified level of confidence based on the resampling procedure.",73.0,Medium
822,Wi,24,Final,,Problem 13,Problem 13.3,"True or False: In this scenario, it would also be appropriate to
create a 95% CLT-based confidence interval.

 True
 False","True
The statement is true because the Central Limit Theorem (CLT) applies
to the sampling distribution of the proportion, given that the sample
size is large enough, which in our case, with 163 medals, it is. The CLT
asserts that the distribution of the sample mean (or proportion, in our
case) will approximate a normal distribution as the sample size grows,
allowing the use of standard methods to create confidence intervals.
Therefore, a CLT-based confidence interval is appropriate for estimating
the true proportion of Olympic table tennis medals won by China.",71.0,Medium
823,Wi,24,Final,,Problem 13,Problem 13.4,"True or False: If our 95% bootstrapped CI came out to be [0.479, 0.518], we would reject the null
hypothesis at the 0.05 significance level.

 True
 False","False
This is false, we would fail to reject the null hypothesis because
the interval [0.479, 0.518] includes
the value of 0.5, which corresponds to the null hypothesis that China
wins half of the Olympic table tennis medals. If the confidence interval
contains the hypothesized value, there is not enough statistical
evidence to reject the null hypothesis at the specified significance
level. In this case, the data does not provide sufficient evidence to
conclude that the proportion of medals won by China is different from
0.5 at the 0.05 significance level.",92.0,Easy
824,Wi,24,Final,,Problem 13,Problem 13.5,"True or False: If we instead chose to test these hypotheses at the
0.01 significance level, the confidence interval we’d create would be
wider.

 True
 False","True
Lowering the significance level means that you require more evidence
to reject the null hypothesis, thus seeking a higher confidence in your
interval estimate. A higher confidence level corresponds to a wider
interval because it must encompass a larger range of values to ensure
that it contains the true population parameter with the increased
probability. Thus as we lower the significance level, the interval we
create will be wider, making this statement true.",79.0,Easy
825,Wi,24,Final,,Problem 13,Problem 13.6,"True or False: If we instead chose to test these hypotheses at the
0.01 significance level, we would be more likely to conclude a
statistically significant result.

 True
 False","False
This statement is false. A small significance level lowers the chance
of getting a statistically significant result; our value for 0.01
significance has to be outside a 99% confidence interval to be
statistically significant. In addition, the true parameter was already
contained within the tighter 95% confidence interval, so we failed to
reject the null hypothesis at the 0.05 significance level. This
guarantees failing to reject the null hypotehsis at the 0.01
significance level since we know that whatever is contained in a 95%
confidence interval has to also be contained in a 99% confidence
interval. Thus, this answer is false.",62.0,Medium
826,Wi,24,Final,,Problem 14,Problem 14,,,,
827,Wi,24,Final,,Problem 14,Problem 14.1,"Suppose that in Olympic ski jumping, ski jumpers jump off of a ramp
that’s shaped like a portion of a normal curve. Drawn from left to
right, a full normal curve has an inflection point on the ascent, then a
peak, then another inflection point on the descent. A ski jump ramp
stops at the point that is one third of the way between the inflection
point on the ascent and the peak, measured horizontally. Below is an
example ski jump ramp, along with the normal curve that generated
it.

Fill in the blank below so that the expression evaluates to the area
of a ski jump ramp, if the area under the normal curve that generated it
is 1.
    from scipy import stats
    stats.norm.cdf(______)
What goes in the blank?","-2/3
We know that the normal distribution is symmetric about the mean, and
that the mean is the “peak” described in the graph. The inflection
points occur one standard deviation above and below the mean (the peak),
so a point which is one third of the way in between the first inflection
point and the peak is -(1-\frac{1}{3}) =
-\frac{2}{3} standard deviations from the mean. We can then use
stats.norm.cdf(-2/3) to calculate the area under the curve
to the left of this point.",51.0,Medium
828,Wi,24,Final,,Problem 14,Problem 14.2,"Suppose that in Olympic downhill skiing, skiers compete on mountains
shaped like normal distributions with mean 50 and standard deviation 8.
Skiers start at the peak and ski down the right side of the mountain, so
their x-coordinate increases.
Keenan is an Olympic downhill skier, but he’s only been able to
practice on a mountain shaped like a normal distribution with mean 65
and standard deviation 12. In his practice, Keenan always crouches down
low when he reaches the point where his x-coordinate is 92, which helps him ski
faster. When he competes at the Olympics, at what x-coordinate should he crouch down low,
corresponding to the same relative location on the mountain?","68
Since we know that both slopes are normal distributions (just scaled
and shifted), we can derive this answer by writing Keenan’s crouch point
in terms of standard deviations from the mean. He typically crouches at
92 feet, whose distance from the mean (in standard deviations) is given
by \frac{92 - 65}{12} = 2.25. So, all
we need to do is find what number is 2.25 standard deviations from the
mean in the Olympic mountain. This is given by 50 + (2.25 * 8) = 68",72.0,Medium
829,Wi,24,Final,,Problem 14,Problem 14.3,"Aaron is another Olympic downhill skier. When he competes on the
normal curve mountain with mean 50 and standard deviation 8, he crouches
down low when his x-coordinate is 54. If the total area of the mountain is 1,
approximately how much of the mountain’s area is ahead of Aaron at the
moment he crouches down low?

 0.1
 0.2
 0.3
 0.4","0.3
We know that when Aaron reaches the mean (50), exactly 0.5 of the
mountain’s area is behind him, since the mean and median are equal for
normal distributions like this one. We also see that 54 is one half of a
standard deviation away from the mean. So, all we have to do is find out
what proportion of the area is within half a standard deviation of the
mean. Using the 68-95-99.7 rule, we know that 68% of the values lie
within one standard deviation of the mean to both the right and left
side. So, this means 34% of the values are within one standard deviation
on one side and at least 17% are within half a standard deviation on one
side. Since the area is 1, the area would be 0.17. So, by the time Aaron
reaches an x-coordinate of 54, 0.5 + 0.17 =
0.67 of the mountain is behind him. From here, we simply
calculate the area in front by 1 - 0.67 =
0.33, so we conclude that approximately 0.3 of the area is in
front of Aaron.
Note: As a clafrification, the 0.17 is an estimate, specifically, an
underestimate, due to the shape of the normal distribution. Thse area
under a normal distribution is not proportional to how many standard
deviations far away from the mean you are.",50.0,Medium
830,Wi,24,Final,,Problem 15,Problem 15,"Birgit Fischer-Schmidt is a German canoe paddler who set many
records, including being the only woman to win Olympic medals 24 years
apart.
Below is a DataFrame with information about all 12 Olympic
medals she has won. There are only 10 rows but 12 medals
represented, because there are some years in which she won more than one
medal of the same type.",,,
831,Wi,24,Final,,Problem 15,Problem 15.1,"Suppose we randomly select one of Birgit’s Olympic medals, and see
that it is a gold medal. What is the probability that the medal was
earned in the year 2000? Give your answer as a fully simplified
fraction.","\frac{1}{4}
Reading the prompt we can see that we are solving for a conditional
probability. Let A be the given condition that the medal is gold and let
B be the event that a medal is from 2000. Looking at the DataFrame we
can see that 8 total gold medals are earned (make sure you pay attention
to the count column). Out of these 8 medals, 2 of them are from the year
2000. Thus, we obtain the probability \frac{2}{8} or \frac{1}{4}.",70.0,Medium
832,Wi,24,Final,,Problem 15,Problem 15.2,"Suppose we randomly select one of Birgit’s Olympic medals. What is
the probability it is gold or earned while representing East Germany?
Give your answer as a fully simplified fraction.","\frac{3}{4}
Here we can recognize that we are solving for the probability of a
union of two events. Let A be the event that the medal is gold. Let B be
the event that it is earned while representing East Germany. The
probability formula for a union is P(A \cup B)
= P(A) + P(B) - P(A \cap B). Looking at the DataFrame, we know
P(A)=\frac{8}{12}, P(B)=\frac{4}{12}, and P(A \cap B)=\frac{3}{12}. Plugging all of
this into the formula, we get \frac{8}{12}+\frac{4}{12}-\frac{3}{12}=\frac{9}{12}=\frac{3}{4}.
Thus, the correct answer is \frac{3}{4}.",73.0,Medium
833,Wi,24,Final,,Problem 15,Problem 15.3,"Suppose we randomly select two of Birgit’s Olympic medals, without
replacement. What is the probability both were earned in 1988? Give your
answer as a fully simplified fraction.","\frac{1}{22}
In this problem, we are sampling 2 medals without replacement. Let A
be the event that the first medal is from 1988 and let P(B) be the event that the second medal is
from 1988. P(A) is \frac{3}{12} since there are 3 medals from
1988 out of the total 12 medals. However, in the second trial, since we
are sampling without replacement, the medal that we just sampled is no
longer in our pool. Thus, P(B) is now
\frac{2}{11} since there are now 2
medals from 1988 from the remaining 11 total medals. The joint
probability P(A \cap B) is P(A)P(B) so, plugging these values in, we get
\frac{3}{12} \cdot \frac{2}{12} = \frac{3}{66}
= \frac{1}{22}. Thus, the answer is \frac{1}{22}.",57.0,Medium
834,Wi,24,Final,,Problem 16,Problem 16,"Suppose males is a DataFrame of all male Olympic
weightlifting medalists with a column called ""Weight""
containing their body weight. Similarly, females is a
DataFrame of all female Olympic weightlifting medalists. It also has a
""Weight"" column with body weights.
The males DataFrame has 425 rows and
the females DataFrame has 105 rows, since
women’s weightlifting became an Olympic sport much later than men’s.
Below, density histograms of the distributions of
""Weight"" in males and females are
shown on the same axes:",,,
835,Wi,24,Final,,Problem 16,Problem 16.1,"Estimate the number of males included in the third bin (from 80 to 100).
Give your answer as an integer, rounded to the nearest multiple of 10.","110
We can estimate the number of males included in the third bin (from
80 to 100) by multiplying the area of that particular histogram bar by
the total number of males. The bar has a height of around 0.013 and a
width of 20, so the bar has an area of 0.013
\cdot 20 = 0.26, which means 26% of males fall in that bin. Since
there are 425 total males, 0.26 \cdot 425 ≈
110 males are in that bin, rounded to the nearest multiple of
10.",81.0,Easy
836,Wi,24,Final,,Problem 16,Problem 16.2,"Using the males DataFrame, write one line of code that
evaluates to the exact number of males included in the third bin (from
80 to 100).","males[(males.get(""Weight"")>=80) & (males.get(""Weight"")<100)].shape[0]
To get the exact number of males in the third bin (from 80 to 100)
using code, we can query the males DataFrame to only include rows where
""Weight"" is greater than or equal to 80, and
""Weight"" is less than 100. Remember, a histogram bin
includes the lower bound (in this case, 80) and excludes the upper bound
(in this case, 100). Remember to put parentheses around each condition,
or else the order of operations will change your intended conditions.
After querying, we use .shape[0] to get the number of rows
in that dataframe, therefore getting the number of males with a weight
greater than or equal to 80, and less than 100.",75.0,Easy
837,Wi,24,Final,,Problem 16,Problem 16.3,"Among Olympic weightlifting medalists who weigh less than 60 kilograms, what proportion are male?

 less than 0.25
 between 0.25 and 0.5
 between 0.5 and 0.75
 more than 0.75","between 0.5
and 0.75
We can answer this question by calculating the approximate number of
males and females in the first bin (from 40 to 60). Be careful, we
cannot simply compare the areas of the bars because the number of male
weightlifting medalists is different from the number of female
weightlifting medalists. The approximate number of male weightlifting
medalists is equal to 0.008 \cdot 20 \cdot 425
= 68, while the approximate number of female weightlifting
medalists is equal to 0.021 \cdot 20 \cdot 105
= 44. The proportion of male weightlifting medalists who weigh
less than 69 kg is approximately \frac{68}{68
+ 44} = 0.607, which falls in the category of “between 0.5 and
0.75”.",54.0,Medium
838,Sp,22,Final,,Problem 1,Problem 1,"Complete the expression below so that it evaluates to the name of the
product for which the average assembly cost per package is lowest.
(ikea.assign(assembly_per_package = ___(a)___)
     .sort_values(by='assembly_per_package').___(b)___)",,,
839,Sp,22,Final,,Problem 1,Problem 1.1,What goes in blank (a)?,"ikea.get('assembly_cost')/ikea.get('packages')
This column, as its name suggests, contains the average assembly cost
per package, obtained by dividing the total cost of each product by the
number of packages that product comes in. This code uses the fact that
arithmetic operations between two Series happens element-wise.",91.0,Easy
840,Sp,22,Final,,Problem 1,Problem 1.2,What goes in blank (b)?,"get('product').iloc[0]
After adding the 'assembly_per_package' column and
sorting by that column in the default ascending order, the product with
the lowest 'assembly_per_package' will be in the very first
row. To access the name of that product, we need to get the
column containing product names and use iloc to access an
element of that Series by integer position.",66.0,Medium
841,Sp,22,Final,,Problem 2,Problem 2,"Complete the expression below so that it evaluates to a DataFrame
indexed by 'category' with one column called
'price' containing the median cost of the products in each
category.
ikea.___(a)___.get(___(b)___)",,,
842,Sp,22,Final,,Problem 2,Problem 2.1,What goes in blank (a)?,"groupby('category').median()
The question prompt says that the resulting DataFrame should be
indexed by 'category', so this is a hint that we may want
to group by 'category'. When using groupby, we
need to specify an aggregation function, and since we’re looking for the
median cost of all products in a category, we use the
.median() aggregation function.",90.0,Easy
843,Sp,22,Final,,Problem 2,Problem 2.2,What goes in blank (b)?,"['price']
The question prompt also says that the resulting DataFrame should
have just one column, the 'price' column. To keep only
certain columns in a DataFrame, we use .get, passing in a
list of columns to keep. Remember to include the square brackets,
otherwise .get will return a Series instead of a
DataFrame.",76.0,Easy
844,Sp,22,Final,,Problem 3,Problem 3,"In the ikea DataFrame, the first word of each string in
the 'product' column represents the product line. For
example the HEMNES line of products includes several different products,
such as beds, dressers, and bedside tables.
The code below assigns a new column to the ikea
DataFrame containing the product line associated with each product.
(ikea.assign(product_line = ikea.get('product')
                                .apply(extract_product_line)))",,,
845,Sp,22,Final,,Problem 3,Problem 3.1,"What are the input and output types of the
extract_product_line function?

 takes a string as input, returns a string
 takes a string as input, returns a Series
 takes a Series as input, returns a string
 takes a Series as input, returns a Series","takes a string as input, returns a
string
To use the Series method .apply, we first need a Series,
containing values of any type. We pass in the name of a function to
.apply and essentially, .apply calls the given
function on each value of the Series, producing a Series with the
resulting outputs of those function calls. In this case,
.apply(extract_product_line) is called on the Series
ikea.get('product'), which contains string values. This
means the function extract_product_line must take strings
as inputs. We’re told that the code assigns a new column to the
ikea DataFrame containing the product line associated with
each product, and we know that the product line is a string, as it’s the
first word of the product name. This means the function
extract_product_line must output a string.",72.0,Medium
846,Sp,22,Final,,Problem 3,Problem 3.2,"Complete the return statement in the
extract_product_line function below.
def extract_product_line(x):
    return _________
What goes in the blank?","x.split(' ')[0]
This function should take as input a string x,
representing a product name, and return the first word of that string,
representing the product line. Since words are separated by spaces, we
want to split the string on the space character ' '.
It’s also correct to answer x.split()[0] without
specifying to split on spaces, because the default behavior of the
string .split method is to split on any whitespace, which
includes any number of spaces, tabs, newlines, etc. Since we’re only
extracting the first word, which will be separated from the rest of the
product name by a single space, it’s equivalent to split using single
spaces and using the default of any whitespace.",84.0,Easy
847,Sp,22,Final,,Problem 4,Problem 4,"Recall that we have the complete set of currently available discounts
in the DataFrame offers.
The DataFrame with_offers is created as follows.
    (with_offers = ikea.take(np.arange(6))
                       .merge(offers, left_on='category',  
                                      right_on='eligible_category'))",,,
848,Sp,22,Final,,Problem 4,Problem 4.1,How many rows does with_offers have?,"11
First, consider the DataFrame ikea.take(np.arange(6)),
which contains the first six rows of ikea. We know the
contents of these first six rows from the preview of the DataFrame at
the start of this exam. To merge with offers, we need to
look at the 'category' of each of these six rows and see
how many rows of offers have the same value in the
'eligible_category' column.
The first row of ikea.take(np.arange(6)) is a
'bed'. There are two 'bed' offers, so that
will create two rows in the output DataFrame. Similarly, the second row
of ikea.take(np.arange(6)) creates two rows in the output
DataFrame because there are two 'outdoor' offers. The third
row creates just one row in the output, since there is only one
'dresser' offer. Continuing row-by-row in this way, we can
sum the number of rows created to get: 2+2+1+2+2+2 = 11.
Pandas Tutor is a really helpful tool to visualize the merge process.
Below is a color-coded visualization of this merge, generated by the
code
here.",41.0,Hard
849,Sp,22,Final,,Problem 4,Problem 4.2,"How many rows of with_offers have a value of 20 in the
'percent_off' column?","2
There is just one offer with a value of 20 in the
'percent_off' column, and this corresponds to an offer on a
'bed'. Since there are two rows of
ikea.take(np.arange(6)) with a 'category' of
'bed', each of these will match with the 20 percent-off
offer, creating two rows of with_offers with a value of 20
in the 'percent_off' column.
The
visualization
from Pandas Tutor below confirms our answer. The two rows with a
value of 20 in the 'percent_off' column are both shown in
rows 0 and 2 of the output DataFrame.",70.0,Medium
850,Sp,22,Final,,Problem 4,Problem 4.3,"If you can use just one offer per product, you’d want to use the one
that saves you the most money, which we’ll call the best offer.
True or False: The expression below evaluates to a
Series indexed by 'product' with the name of the best offer
for each product that appears in the with_offers
DataFrame.
with_offers.groupby('product').max().get('offer')

 True
 False","False
Recall that groupby applies the aggregation function
separately to each column. Applying the .max() aggregate on
the 'offer' column for each group gives the name that is
latest in alphabetical order because it contains strings, whereas
applying the .max() aggregate on the
'percent_off' column gives the largest numerical value.
These don’t necessarily go together in with_offers.
In particular, the element of
with_offers.groupby('product').max().get('offer')
corresponding to the LAPPLAND TV storage unit will be
'get_organized_promo'. This happens because the two rows of
with_offers corresponding to the LAPPLAND TV storage unit
have values of 'get_organized_promo' and
'birthday coupon', but 'get_organized_promo'
is alphabetically later, so it’s considered the max by
.groupby. However, the 'birthday coupon' is
actually a better offer, since it’s 15 percent off, while the
'get_organized_promo' is only 10 percent off. The
expression does not actually find the best offer for each product, but
instead finds the latest alphabetical offer for each product.
We can see this directly by looking at the output of Pandas Tutor
below, generated by
this
code.",69.0,Medium
851,Sp,22,Final,,Problem 4,Problem 4.4,"You want to add a column to with_offers containing the
price after the offer discount is applied.
with_offers = with_offers.assign(after_price = _________)
with_offers
Which of the following could go in the blank? Select all that
apply.

 with_offers.get('price') - with_offers.get('percent_off')/100
 with_offers.get('price')*(100 - with_offers.get('percent_off'))/100
 with_offers.get('price') - with_offers.get('price')*with_offers.get('percent_off')/100
 with_offers.get('price')*(100 - with_offers.get('percent_off')/100)","with_offers.get('price')*(100 - with_offers.get('percent_off'))/100,
(with_offers.get('price') - with_offers.get('price')*with_offers.get('percent_off')/100)
Notice that all the answer choices use
with_offers.get('price'), which is a Series of prices, and
with_offers.get('percent_off'), which is a Series of
associated percentages off. Using Series arithmetic, which works
element-wise, the goal is to create a Series of prices after the
discount is applied.
For example, the first row of with_offers corresponds to
an item with a price of 254 dollars, and a discount of 20 percent off,
coming from the snooze sale. This means its price after the discount
should be 80 percent of the original value, which is 254*0.8 = 203.2 dollars.
Let’s go through each answer in order, working with this example.
The first answer choice takes the price and subtracts the percent off
divided by 100. For our example, this would compute the discounted price
as 254 - 20/100 = 253.8 dollars, which
is incorrect.
The second answer choice multiplies the price by the quantity 100
minus the percent off, then divides by 100. This works because when we
subtract the percent off from 100 and divide by 100, the result
represents the proportion or fraction of the cost we must pay, and so
multiplying by the price gives the price after the discount. For our
example, this comes out to 254*(100-20)/100=
203.2 dollars, which is correct.
The third answer choice is also correct. This corresponds to taking
the original price and subtracting from it the dollar amount off, which
comes from multiplying the price by the percent off and dividing by 100.
For our example, this would be computed as 254
- 254*20/100 = 203.2 dollars, which is correct.
The fourth answer multiplies the price by the quantity 100 minus the
percent off divided by 100. For our example, this would compute the
discounted price as 254*(100 - 20/100) =
25349.2 dollars, a number that’s nearly one hundred times the
original price!
Therefore, only the second and third answer choices are correct.",79.0,Easy
852,Sp,22,Final,,Problem 5,Problem 5,"Recall that an IKEA fan created an app for people to log the amount
of time it takes them to assemble an IKEA product. We have this data in
app_data.",,,
853,Sp,22,Final,,Problem 5,Problem 5.1,"Suppose that when someone downloads the app, the app requires them to
choose a username, which must be different from all other registered
usernames.
True or False: If app_data had included
a column with the username of the person who reported each product
build, it would make sense to index app_data by
username.

 True
 False","False
Even though people must have distinct usernames, one person can build
multiple different IKEA products and log their time for each build. So
we don’t expect every row of app_data to have a distinct
username associated with it, and therefore username would not be
suitable as an index, since the index should have distinct values.",,
854,Sp,22,Final,,Problem 5,Problem 5.2,"What does the code below evaluate to?
(app_data.take(np.arange(4))
         .sort_values(by='assembly_time')
         .get('assembly_time')
         .iloc[0])
Hint: The 'assembly_time' column contains
strings.","'1 hr, 45 min'
The code says that we should take the first four rows of
app_data (which we can see in the preview at the start of
this exam), sort the rows in ascending order of
'assembly_time', and extract the very first entry of the
'assembly_time' column. In other words, we have to find the
'assembly_time' value that would be first when sorted in
ascending order. As given in the hint, the 'assembly_time'
column contains strings, and strings get sorted alphabetically, so we
are looking for the assembly time, among the first four, that would come
first alphabetically. This is '1 hr, 45 min'.
Note that first alphabetically does not correspond to the least
amount of time. '1 hr, 5 min' represents less time than
'1 hr, 45 min', but in alphabetical order
'1 hr, 45 min' comes before '1 hr, 5 min'
because both start with the same characters '1 hr, ' and
comparing the next character, '4' comes before
'5'.",,
855,Sp,22,Final,,Problem 5,Problem 5.3,"Complete the implementation of the to_minutes function
below. This function takes as input a string formatted as
'x hr, y min' where x and y
represent integers, and returns the corresponding number of minutes,
as an integer (type int in Python).
For example, to_minutes('3 hr, 5 min') should return
185.
def to_minutes(time):
    first_split = time.split(' hr, ')
    second_split = first_split[1].split(' min')
    return _________
What goes in the blank?","int(first_split[0])*60+int(second_split[0])
As the last subpart demonstrated, if we want to compare times, it
doesn’t make sense to do so when times are represented as strings. In
the to_minutes function, we convert a time string into an
integer number of minutes.
The first step is to understand the logic. Every hour contains 60
minutes, so for a time string formatted like x hr, y min'
the total number of minutes comes from multiplying the value of
x by 60 and adding y.
The second step is to understand how to extract the x
and y values from the time string using the string methods
.split. The string method .split takes as
input some separator string and breaks the string into pieces at each
instance of the separator string. It then returns a list of all those
pieces. The first line of code, therefore, creates a list called
first_split containing two elements. The first element,
accessed by first_split[0] contains the part of the time
string that comes before ' hr, '. That is,
first_split[0] evaluates to the string x.
Similarly, first_split[1] contains the part of the time
string that comes after ' hr, '. So it is formatted like
'y min'. If we split this string again using the separator
of ' min', the result will be a list whose first element is
the string 'y'. This list is saved as
second_split so second_split[0] evaluates to
the string y.
Now we have the pieces we need to compute the number of minutes,
using the idea of multiplying the value of x by 60 and
adding y. We have to be careful with data types here, as
the bolded instructions warn us that the function must return an
integer. Right now, first_split[0] evaluates to the string
x and second_split[0] evaluates to the string
y. We need to convert these strings to integers before we
can multiply and add. Once we convert using the int
function, then we can multiply the number of hours by 60 and add the
number of minutes. Therefore, the solution is
int(first_split[0])*60+int(second_split[0]).
Note that failure to convert strings to integers using the
int function would lead to very different behavior. Let’s
take the example time string of '3 hr, 5 min' as input to
our function. With the return statement as
int(first_split[0])*60+int(second_split[0]), the function
would return 185 on this input, as desired. With the return statement as
first_split[0]*60+second_split[0], the function would
return a string of length 61, looking something like this
'3333...33335'. That’s because the * and
+ symbols do have meaning for strings, they’re just
different meanings than when used with integers.",71.0,Medium
856,Sp,22,Final,,Problem 5,Problem 5.4,"You want to add to app_data a column called
'minutes' with integer values representing the number of
minutes it took to assemble each product.
app_data = app_data.assign(minutes = _________)
app_data
Which of the following should go in the blank?

 to_minutes('assembly_time')
 to_minutes(app_data.get('assembly_time'))
 app_data.get('assembly_time').apply(to_minutes)
 app_data.get('assembly_time').apply(to_minutes(time))","app_data.get('assembly_time').apply(to_minutes)
We want to create a Series of times in minutes, since it’s to be
added to the app_data DataFrame using .assign.
The correct way to do that is to use .apply so that our
function, which works for one input time string, can be applied
separately to every time string in the 'assembly_time'
column of app_data. Remember that .apply takes
as input one parameter, the name of the function to apply, with no
arguments. The correct syntax is
app_data.get('assembly_time').apply(to_minutes).",98.0,Easy
857,Sp,22,Final,,Problem 6,Problem 6,"We want to use app_data to estimate the average amount
of time it takes to build an IKEA bed (any product in the
'bed' category). Which of the following strategies would be
an appropriate way to estimate this quantity? Select all that apply.

 Query to keep only the beds. Then resample with replacement many
times. For each resample, take the mean of the 'minutes'
column. Compute a 95% confidence interval based on those means.
 Query to keep only the beds. Group by 'product' using
the mean aggregation function. Then resample with replacement many
times. For each resample, take the mean of the 'minutes'
column. Compute a 95% confidence interval based on those means.
 Resample with replacement many times. For each resample, first query
to keep only the beds and then take the mean of the
'minutes' column. Compute a 95% confidence interval based
on those means.
 Resample with replacement many times. For each resample, first query
to keep only the beds. Then group by 'product' using the
mean aggregation function, and finally take the mean of the
'minutes' column. Compute a 95% confidence interval based
on those means.","Option 1
Only the first answer is correct. This is a question of parameter
estimation, so our approach is to use bootstrapping to create many
resamples of our original sample, computing the average of each
resample. Each resample should always be the same size as the original
sample. The first answer choice accomplishes this by querying first to
keep only the beds, then resampling from the DataFrame of beds only.
This means resamples will have the same size as the original sample.
Each resample’s mean will be computed, so we will have many resample
means from which to construct our 95% confidence interval.
In the second answer choice, we are actually taking the mean twice.
We first average the build times for all builds of the same product when
grouping by product. This produces a DataFrame of different products
with the average build time for each. We then resample from this
DataFrame, computing the average of each resample. But this is a
resample of products, not of product builds. The size of the resample is
the number of unique products in app_data, not the number
of reported product builds in app_data. Further, we get
incorrect results by averaging numbers that are already averages. For
example, if 5 people build bed A and it takes them each 1 hour, and 1
person builds bed B and it takes them 10 hours, the average amount of
time to build a bed is \frac{5*1+10}{6} =
2.5. But if we average the times for bed A (1 hour) and average
the times for bed B (5 hours), then average those, we get \frac{1+5}{2} = 3, which is not the same.
More generally, grouping is not a part of the bootstrapping process
because we want each data value to be weighted equally.
The last two answer choices are incorrect because they involve
resampling from the full app_data DataFrame before querying
to keep only the beds. This is incorrect because it does not preserve
the sample size. For example, if app_data contains 1000
reported bed builds and 4000 other product builds, then the only
relevant data is the 1000 bed build times, so when we resample, we want
to consider another set of 1000 beds. If we resample from the full
app_data DataFrame, our resample will contain 5000 rows,
but the number of beds will be random, not necessarily 1000. If we query
first to keep only the beds, then resample, our resample will contain
exactly 1000 beds every time. As an added bonus, since we only care
about beds, it’s much faster to resample from a smaller DataFrame of
beds only than it is to resample from all app_data with
plenty of rows we don’t care about.",71.0,Medium
858,Sp,22,Final,,Problem 7,Problem 7,"Laura built the LAPPLAND TV storage unit in 2 hours and 30 minutes,
and she thinks she worked at an average speed. If you want to see
whether the average time to build the TV storage unit is indeed 2 hours
and 30 minutes using the sample of assembly times in
app_data, which of the following tools
could you use to help you? Select all that apply.

 hypothesis testing
 permutation testing
 bootstrapping
 Central Limit Theorem
 confidence interval
 regression","hypothesis testing, bootstrapping, Central
Limit Theorem, confidence interval
The average time to build the LAPPLAND TV storage unit is an unknown
population parameter. We’re trying to figure out if this parameter could
be equal to the specific value of 2 hours and 30 minutes. We can use the
framework we learned in class to set this up as a hypothesis test via
confidence interval. When we have a null hypothesis of the form “the
parameter equals the specific value” and an alternative hypothesis of
“it does not,” this framework applies, and conducting the hypothesis
test is equivalent to constructing a confidence interval for the
parameter and seeing if the specific value falls in the interval.
There are two ways in which we could construct the confidence
interval. One is through bootstrapping, and the other is through the
Central Limit Theorem, which applies in this case because our statistic
is the mean.
The only listed tools that could not be used here are permutation
testing and regression. Permutation testing is used to determine whether
two samples could have come from the same population, but here we only
have one sample. Permutation testing would be helpful to answer a
question like, “Does it take the same amount of time to assemble the
LAPPLAND TV storage as it does to assemble the HAUGA TV storage
unit?”
Regression is used to predict one numerical quantity based on
another, not to estimate a parameter as we are doing here. Regression
would be appropriate to answer a question like, “How does the assembly
time for the LAPPLAND TV storage unit change with the assembler’s
age?”",78.0,Easy
859,Sp,22,Final,,Problem 8,Problem 8,"For this question, let’s think of the data in app_data
as a random sample of all IKEA purchases and use it to test the
following hypotheses.
Null Hypothesis: IKEA sells an equal amount of beds
(category 'bed') and outdoor furniture (category
'outdoor').
Alternative Hypothesis: IKEA sells more beds than
outdoor furniture.
The DataFrame app_data contains 5000 rows, which form
our sample. Of these 5000 products,

1000 are beds,
1500 are outdoor furniture, and
2500 are in another category.",,,
860,Sp,22,Final,,Problem 8,Problem 8.1,"Which of the following could be used as the test
statistic for this hypothesis test? Select all that apply.

 Among 2500 beds and outdoor furniture items, the absolute difference
between the proportion of beds and the proportion of outdoor
furniture.
 Among 2500 beds and outdoor furniture items, the proportion of
beds.
 Among 2500 beds and outdoor furniture items, the number of beds.
 Among 2500 beds and outdoor furniture items, the number of beds plus
the number of outdoor furniture items.","Among 2500 beds and outdoor furniture
items, the proportion of beds.  Among 2500 beds and outdoor
furniture items, the number of beds.
Our test statistic needs to be able to distinguish between the two
hypotheses. The first option does not do this, because it includes an
absolute value. If the absolute difference between the proportion of
beds and the proportion of outdoor furniture were large, it could be
because IKEA sells more beds than outdoor furniture, but it could also
be because IKEA sells more outdoor furniture than beds.
The second option is a valid test statistic, because if the
proportion of beds is large, that suggests that the alternative
hypothesis may be true.
Similarly, the third option works because if the number of beds (out
of 2500) is large, that suggests that the alternative hypothesis may be
true.
The fourth option is invalid because out of 2500 beds and outdoor
furniture items, the number of beds plus the number of outdoor furniture
items is always 2500. So the value of this statistic is constant
regardless of whether the alternative hypothesis is true, which means it
does not help you distinguish between the two hypotheses.",78.0,Easy
861,Sp,22,Final,,Problem 8,Problem 8.2,"Let’s do a hypothesis test with the following test statistic: among
2500 beds and outdoor furniture items, the proportion of outdoor
furniture minus the proportion of beds.
Complete the code below to calculate the observed value of the test
statistic and save the result as obs_diff.
    outdoor = (app_data.get('category')=='outdoor') 
    bed = (app_data.get('category')=='bed')
    obs_diff = ( ___(a)___ - ___(b)___ ) / ___(c)___
The table below contains several Python expressions. Choose the
correct expression to fill in each of the three blanks. Three
expressions will be used, and two will be unused.","Reading the table from top to bottom, the
five expressions should be used in the following blanks: None, (b), (a),
(c), None.
The correct way to define obs_diff is

    outdoor = (app_data.get('category')=='outdoor') 
    bed = (app_data.get('category')=='bed')
    obs_diff = (app_data[outdoor].shape[0] - app_data[bed].shape[0]) / app_data[outdoor | bed].shape[0]

The first provided line of code defines a boolean Series called
outdoor with a value of True corresponding to
each outdoor furniture item in app_data. Using this as the
condition in a query results in a DataFrame of outdoor furniture items,
and using .shape[0] on this DataFrame gives the number of
outdoor furniture items. So app_data[outdoor].shape[0]
represents the number of outdoor furniture items in
app_data. Similarly, app_data[bed].shape[0]
represents the number of beds in app_data. Likewise,
app_data[outdoor | bed].shape[0] represents the total
number of outdoor furniture items and beds in app_data.
Notice that we need to use an or condition (|) to
get a DataFrame that contains both outdoor furniture and beds.
We are told that the test statistic should be the proportion of
outdoor furniture minus the proportion of beds. Translating this
directly into code, this means the test statistic should be calculated
as

    obs_diff = app_data[outdoor].shape[0]/app_data[outdoor | bed].shape[0] - app_data[bed].shape[0]) / app_data[outdoor | bed].shape[0]

Since this is a difference of two fractions with the same
denominator, we can equivalently subtract the numerators first, then
divide by the common denominator, using the mathematical fact \frac{a}{c} - \frac{b}{c} =
\frac{a-b}{c}.
This yields the answer

    obs_diff = (app_data[outdoor].shape[0] - app_data[bed].shape[0]) / app_data[outdoor | bed].shape[0]

Notice that this is the observed value of the test statistic
because it’s based on the real-life data in the app_data
DataFrame, not simulated data.",90.0,Easy
862,Sp,22,Final,,Problem 8,Problem 8.3,"Which of the following is a valid way to generate one value of the
test statistic according to the null model? Select all that apply.
Way 1:
multi = np.random.multinomial(2500, [0.5,0.5]) 
(multi[0] - multi[1])/2500
Way 2:
outdoor = np.random.multinomial(2500, [0.5,0.5])[0]/2500 
bed = np.random.multinomial(2500, [0.5,0.5])[1]/2500 
outdoor - bed 
Way 3:
choice = np.random.choice([0, 1], 2500, replace=True) 
choice_sum = choice.sum( ) 
(choice_sum - (2500 - choice_sum))/2500
Way 4:
choice = np.random.choice(['bed', 'outdoor'], 2500, replace=True) 
bed = np.count_nonzero(choice=='bed')
outdoor = np.count_nonzero(choice=='outdoor')
outdoor/2500 - bed/2500
Way 5:
outdoor = (app_data.get('category')=='outdoor') 
bed = (app_data.get('category')=='bed')
samp = app_data[outdoor|bed].sample(2500, replace=True) 
samp[samp.get('category')=='outdoor'].shape[0]/2500 -  samp[samp.get('category')=='bed'].shape[0]/2500)
Way 6:
outdoor = (app_data.get('category')=='outdoor') 
bed = (app_data.get('category')=='bed')
samp = (app_data[outdoor|bed].groupby('category').count( ).reset_index( ).sample(2500, replace=True))    
samp[samp.get('category')=='outdoor'].shape[0]/2500 - samp[samp.get('category')=='bed'].shape[0]/2500

 Way 1
 Way 2
 Way 3
 Way 4
 Way 5
 Way 6","Way 1, Way 3, Way 4, Way 6
Let’s consider each way in order.
Way 1 is a correct solution. This code begins by defining a variable
multi which will evaluate to an array with two elements
representing the number of items in each of the two categories, after
2500 items are drawn randomly from the two categories, with each
category being equally likely. In this case, our categories are beds and
outdoor furniture, and the null hypothesis says that each category is
equally likely, so this describes our scenario accurately. We can
interpret multi[0] as the number of outdoor furniture items
and multi[1] as the number of beds when we draw 2500 of
these items with equal probability. Using the same mathematical fact
from the solution to",,
863,Sp,22,Final,,Problem 8,Problem 8.2,", we can calculate the difference in
proportions as the difference in number divided by the total, so it is
correct to calculate the test statistic as
(multi[0] - multi[1])/2500.
Way 2 is an incorrect solution. Way 2 is based on a similar idea as
Way 1, except it calls np.random.multinomial twice, which
corresponds to two separate random processes of selecting 2500 items,
each of which is equally likely to be a bed or an outdoor furniture
item. However, is not guaranteed that the number of outdoor furniture
items in the first random selection plus the number of beds in the
second random selection totals 2500. Way 2 calculates the proportion of
outdoor furniture items in one random selection minus the proportion of
beds in another. What we want to do instead is calculate the difference
between the proportion of outdoor furniture and beds in a single random
draw.
Way 3 is a correct solution. Way 3 does the random selection of items
in a different way, using np.random.choice. Way 3 creates a
variable called choice which is an array of 2500 values.
Each value is chosen from the list [0,1] with each of the
two list elements being equally likely to be chosen. Of course, since we
are choosing 2500 items from a list of size 2, we must allow
replacements. We can interpret the elements of choice by
thinking of each 1 as an outdoor furniture item and each 0 as a bed. By
doing so, this random selection process matches up with the assumptions
of the null hypothesis. Then the sum of the elements of
choice represents the total number of outdoor furniture
items, which the code saves as the variable choice_sum.
Since there are 2500 beds and outdoor furniture items in total,
2500 - choice_sum represents the total number of beds.
Therefore, the test statistic here is correctly calculated as the number
of outdoor furniture items minus the number of beds, all divided by the
total number of items, which is 2500.
Way 4 is a correct solution. Way 4 is similar to Way 3, except
instead of using 0s and 1s, it uses the strings 'bed' and
'outdoor' in the choice array, so the
interpretation is even more direct. Another difference is the way the
number of beds and number of outdoor furniture items is calculated. It
uses np.count_nonzero instead of sum, which wouldn’t make
sense with strings. This solution calculates the proportion of outdoor
furniture minus the proportion of beds directly.
Way 5 is an incorrect solution. As described in the solution to",,,
864,Sp,22,Final,,Problem 8,Problem 8.2,", app_data[outdoor|bed] is a DataFrame
containing just the outdoor furniture items and the beds from
app_data. Based on the given information, we know
app_data[outdoor|bed] has 2500 rows, 1000 of which
correspond to beds and 1500 of which correspond to furniture items. This
code defines a variable samp that comes from sampling this
DataFrame 2500 times with replacement. This means that each row of
samp is equally likely to be any of the 2500 rows of
app_data[outdoor|bed]. The fraction of these rows that are
beds is 1000/2500 = 2/5 and the
fraction of these rows that are outdoor furniture items is 1500/2500 = 3/5. This means the random
process of selecting rows randomly such that each row is equally likely
does not make each item equally likely to be a bed or outdoor furniture
item. Therefore, this approach does not align with the assumptions of
the null hypothesis.
Way 6 is a correct solution. Way 6 essentially modifies Way 5 to make
beds and outdoor furniture items equally likely to be selected in the
random sample. As in Way 5, the code involves the DataFrame
app_data[outdoor|bed] which contains 1000 beds and 1500
outdoor furniture items. Then this DataFrame is grouped by
'category' which results in a DataFrame indexed by
'category', which will have only two rows, since there are
only two values of 'category', either
'outdoor' or 'bed'. The aggregation function
.count() is irrelevant here. When the index is reset,
'category' becomes a column. Now, randomly sampling from
this two-row grouped DataFrame such that each row is equally likely to
be selected does correspond to choosing items such that each
item is equally likely to be a bed or outdoor furniture item. The last
line simply calculates the proportion of outdoor furniture items minus
the proportion of beds in our random sample drawn according to the null
model.

Difficulty: ⭐️⭐️⭐️


The average score on this problem was 59%.",,59.0,Medium
865,Sp,22,Final,,Problem 8,Problem 8.4,"Suppose we generate 10,000 simulated values of the test statistic
according to the null model and store them in an array called
simulated_diffs. Complete the code below to calculate the
p-value for the hypothesis test.
    np.count_nonzero(simulated_diffs _________ obs_diff)/10000
What goes in the blank?

 <
 <=
 >
 >=","<=
To answer this question, we need to know whether small values or
large values of the test statistic indicate the alternative hypothesis.
The alternative hypothesis is that IKEA sells more beds than outdoor
furniture. Since we’re calculating the proportion of outdoor furniture
minus the proportion of beds, this difference will be small (negative)
if the alternative hypothesis is true. Larger (positive) values of the
test statistic mean that IKEA sells more outdoor furniture than beds. A
value near 0 means they sell beds and outdoor furniture equally.
The p-value is defined as the proportion of simulated test statistics
that are equal to the observed value or more extreme, where extreme
means in the direction of the alternative. In this case, since small
values of the test statistic indicate the alternative hypothesis, the
correct answer is <=.",43.0,Hard
866,Sp,22,Final,,Problem 9,Problem 9,"You are browsing the IKEA showroom, deciding whether to purchase the
BILLY bookcase or the LOMMARP bookcase. You are concerned about the
amount of time it will take to assemble your new bookcase, so you look
up the assembly times reported in app_data. Thinking of the
data in app_data as a random sample of all IKEA purchases,
you want to perform a permutation test to test the following
hypotheses.
Null Hypothesis: The assembly time for the BILLY
bookcase and the assembly time for the LOMMARP bookcase come from the
same distribution.
Alternative Hypothesis: The assembly time for the
BILLY bookcase and the assembly time for the LOMMARP bookcase come from
different distributions.",,,
867,Sp,22,Final,,Problem 9,Problem 9.1,"Suppose we query app_data to keep only the BILLY
bookcases, then average the 'minutes' column. In addition,
we separately query app_data to keep only the LOMMARP
bookcases, then average the 'minutes' column. If the null
hypothesis is true, which of the following statements about these two
averages is correct?

 These two averages are the same.
 Any difference between these two averages is due to random
chance.
 Any difference between these two averages cannot be ascribed to
random chance alone.
 The difference between these averages is statistically
significant.","Any difference between these two averages
is due to random chance.
If the null hypothesis is true, this means that the time recorded in
app_data for each BILLY bookcase is a random number that
comes from some distribution, and the time recorded in
app_data for each LOMMARP bookcase is a random number that
comes from the same distribution. Each assembly time is a
random number, so even if the null hypothesis is true, if we take one
person who assembles a BILLY bookcase and one person who assembles a
LOMMARP bookcase, there is no guarantee that their assembly times will
match. Their assembly times might match, or they might be different,
because assembly time is random. Randomness is the only reason that
their assembly times might be different, as the null hypothesis says
there is no systematic difference in assembly times between the two
bookcases. Specifically, it’s not the case that one typically takes
longer to assemble than the other.
With those points in mind, let’s go through the answer choices.
The first answer choice is incorrect. Just because two sets of
numbers are drawn from the same distribution, the numbers themselves
might be different due to randomness, and the averages might also be
different. Maybe just by chance, the people who assembled the BILLY
bookcases and recorded their times in app_data were slower
on average than the people who assembled LOMMARP bookcases. If the null
hypothesis is true, this difference in average assembly time should be
small, but it very likely exists to some degree.
The second answer choice is correct. If the null hypothesis is true,
the only reason for the difference is random chance alone.
The third answer choice is incorrect for the same reason that the
second answer choice is correct. If the null hypothesis is true, any
difference must be explained by random chance.
The fourth answer choice is incorrect. If there is a difference
between the averages, it should be very small and not statistically
significant. In other words, if we did a hypothesis test and the null
hypothesis was true, we should fail to reject the null.",77.0,Easy
868,Sp,22,Final,,Problem 9,Problem 9.2,"For the permutation test, we’ll use as our test statistic the average
assembly time for BILLY bookcases minus the average assembly time for
LOMMARP bookcases, in minutes.
Complete the code below to generate one simulated value of the test
statistic in a new way, without using
np.random.permutation.
billy = (app_data.get('product') == 
        'BILLY Bookcase, white, 31 1/2x11x79 1/2')
lommarp = (app_data.get('product') == 
          'LOMMARP Bookcase, dark blue-green, 25 5/8x78 3/8')
billy_lommarp = app_data[billy|lommarp]
billy_mean = np.random.choice(billy_lommarp.get('minutes'), billy.sum(), replace=False).mean()
lommarp_mean = _________
billy_mean - lommarp_mean
What goes in the blank?

 billy_lommarp[lommarp].get('minutes').mean()
 np.random.choice(billy_lommarp.get('minutes'), lommarp.sum(), replace=False).mean()
 billy_lommarp.get('minutes').mean() - billy_mean
 (billy_lommarp.get('minutes').sum() - billy_mean * billy.sum())/lommarp.sum()","(billy_lommarp.get('minutes').sum() - billy_mean * billy.sum())/lommarp.sum()
The first line of code creates a boolean Series with a True value for
every BILLY bookcase, and the second line of code creates the analogous
Series for the LOMMARP bookcase. The third line queries to define a
DataFrame called billy_lommarp containing all products that
are BILLY or LOMMARP bookcases. In other words, this DataFrame contains
a mix of BILLY and LOMMARP bookcases.
From this point, the way we would normally proceed in a permutation
test would be to use np.random.permutation to shuffle one
of the two relevant columns (either 'product' or
'minutes') to create a random pairing of assembly times
with products. Then we would calculate the average of all assembly times
that were randomly assigned to the label BILLY. Similarly, we’d
calculate the average of all assembly times that were randomly assigned
to the label LOMMARP. Then we’d subtract these averages to get one
simulated value of the test statistic. To run the permutation test, we’d
have to repeat this process many times.
In this problem, we need to generate a simulated value of the test
statistic, without randomly shuffling one of the columns. The code
starts us off by defining a variable called billy_mean that
comes from using np.random.choice. There’s a lot going on
here, so let’s break it down. Remember that the first argument to
np.random.choice is a sequence of values to choose from,
and the second is the number of random choices to make. And we set
replace=False, so that no element that has already been
chosen can be chosen again. Here, we’re making our random choices from
the 'minutes' column of billy_lommarp. The
number of choices to make from this collection of values is
billy.sum(), which is the sum of all values in the
billy Series defined in the first line of code. The
billy Series contains True/False values, but in Python,
True counts as 1 and False counts as 0, so billy.sum()
evaluates to the number of True entries in billy, which is
the number of BILLY bookcases recorded in app_data. It
helps to think of the random process like this:

Collect all the assembly times of any BILLY or LOMMARP bookcase in a
large bag.
Pull out a random assembly time from this bag.
Repeat step 2, drawing as many times as there are BILLY bookcases,
without replacement.

If we think of the random times we draw as being labeled BILLY, then
the remaining assembly times still leftover in the bag represent the
assembly times randomly labeled LOMMARP. In other words, this is a
random association of assembly times to labels (BILLY or LOMMARP), which
is the same thing we usually accomplish by shuffling in a permutation
test.
From here, we can proceed the same way as usual. First, we need to
calculate the average of all assembly times that were randomly assigned
to the label BILLY. This is done for us and stored in
billy_mean. We also need to calculate the average of all
assembly times that were randomly assigned the label LOMMARP. We’ll call
that lommarp_mean. Thinking of picking times out of a large
bag, this is the average of all the assembly times left in the bag. The
problem is there is no easy way to access the assembly times that were
not picked. We can take advantage of the fact that we can easily
calculate the total assembly time of all BILLY and LOMMARP bookcases
together with billy_lommarp.get('minutes').sum(). Then if
we subtract the total assembly time of all bookcases randomly labeled
BILLY, we’ll be left with the total assembly time of all bookcases
randomly labeled LOMMARP. That is,
billy_lommarp.get('minutes').sum() - billy_mean * billy.sum()
represents the total assembly time of all bookcases randomly labeled
LOMMARP. The count of the number of LOMMARP bookcases is given by
lommarp.sum() so the average is
(billy_lommarp.get('minutes').sum() - billy_mean * billy.sum())/lommarp.sum().
A common wrong answer for this question was the second answer choice,
np.random.choice(billy_lommarp.get('minutes'), lommarp.sum(), replace=False).mean().
This mimics the structure of how billy_mean was defined so
it’s a natural guess. However, this corresponds to the following random
process, which doesn’t associate each assembly with a unique label
(BILLY or LOMMARP):

Collect all the assembly times of any BILLY or LOMMARP bookcase in a
large bag.
Pull out a random assembly time from this bag.
Repeat, drawing as many times as there are BILLY bookcases, without
replacement.
Collect all the assembly times of any BILLY or LOMMARP bookcase in a
large bag.
Pull out a random assembly time from this bag.
Repeat step 5, drawing as many times as there are LOMMARP bookcases,
without replacement.

We could easily get the same assembly time once for BILLY and once
for LOMMARP, while other assembly times could get picked for neither.
This process doesn’t split the data into two random groups as
desired.",12.0,Hard
869,Sp,22,Final,,Problem 10,Problem 10,"IKEA is piloting a new rewards program where customers can earn free
Swedish meatball plates from the in-store cafe when they purchase
expensive items. Meatball plates are awarded as follows. Assume the item
price is always an integer.","Way 2, Way 3, Way 4
Way 1 does not work because it resets the variable
meatball_plates with each iteration instead of adding to a
running total. At the end of the loop, meatball_plates
evaluates to the number of meatball plates earned by the last price in
prices instead of the total number of meatball plates
earned by all purchases. A quick way to see this is incorrect is to
notice that the only possible return values are 0, 1, and 2, but it’s
possible to earn more than 2 meatball plates with enough purchases.
Way 2 works. As in Way 1, it loops through each price in
prices. When evaluating each price, it checks if the price
is at least 200 dollars, and if so, increments the total number of
meatball plates by 1. Then it checks if the price is at least 100
dollars, and if so, increments the count of meatball plates again. This
works because for prices that are at least 200 dollars, both
if conditions are satisfied, so the meatball plate count
goes up by 2, and for prices that are between 100 and 199 dollars, only
one of the if conditions is satisfied, so the count
increases by 1. For prices less than 100 dollars, the count doesn’t
change.
Way 3 works without using any iteration at all. It uses
np.count_nonzero to count the number of prices that are at
least 100 dollars, then it similarly counts the number of prices that
are at least 200 dollars and adds these together. This works because
prices that are at least 200 dollars are also at least 100 dollars, so
they contribute 2 to the total. Prices that are between 100 and 199
contribute 1, and prices below 100 dollars don’t contribute at all.
Way 4 also works and calculates the number of meatball plates without
iteration. The expression prices >= 200 evaluates to a
boolean Series with True for each price that is at least 200 dollars.
Summing this Series gives a count of the number of prices that are at
least 200 dollars, since True counts as 1 and False counts as 0 in
Python. Each such purchase earns 2 meatball plates, so this count of
purchases 200 dollars and up gets multiplied by 2. Similarly,
(100 <= prices) & (prices <= 199) is a Series
containing True for each price that is at least 100 dollars and at most
199 dollars, and the sum of that Series is the number of prices between
100 and 199 dollars. Each such purchase contributes one additional
meatball plate, so the number of such purchases gets multiplied by 1 and
added to the total.",64.0,Medium
870,Sp,22,Final,,Problem 11,Problem 11,"The histogram below shows the distribution of the number of products
sold per day throughout the last 30 days, for two different IKEA
products: the KURA reversible bed, and the ANTILOP highchair.",,,
871,Sp,22,Final,,Problem 11,Problem 11.1,"For how many days did IKEA sell between 20 (inclusive) and 30
(exclusive) KURA reversible beds per? Give an integer
answer or write “not enough information”, but not
both.","15
Remember that for a density histogram, the proportion of data that
falls in a certain range is the area of the histogram between those
bounds. So to find the proportion of days for which IKEA sold between 20
and 30 KURA reversible beds, we need to find the total area of the bins
[20, 25) and [25, 30). Note that the left endpoint of each
bin is included and the right bin is not, which works perfectly with
needing to include 20 and exclude 30.
The bin [20, 25) has a width of 5
and a height of about 0.047. The bin [25,
30) has a width of 5 and a height of about 0.053. The heights are
approximate by eye, but it appears that the [20, 25) bin is below the horizontal line at
0.05 by the same amount which the [25,
30) is above that line. Noticing this spares us some annoying
arithmetic and we can calculate the total area as
\begin{aligned}
        \text{total area} &= 5*0.047 + 5*0.053 \\
                    &= 5*(0.047 + 0.053) \\
                    &= 5*(0.1) \\
                    &= 0.5
\end{aligned}
Therefore, the proportion of days where IKEA sold between 20 and 30
KURA reversible beds is 0.5. Since there are 30 days represented in this
histogram, that corresponds to 15 days.",54.0,Medium
872,Sp,22,Final,,Problem 11,Problem 11.2,"For how many days did IKEA sell more KURA reversible beds than
ANTILOP highchairs? Give an integer answer or write
“not enough information”, but not both.","not enough information
We cannot compare the number of KURA reversible beds sold on a
certain day with the number of ANTILOP highchairs sold on the same day.
These are two separate histograms shown on the same axes, but we have no
way to associate data from one histogram with data from the other. For
example, it’s possible that on some days, IKEA sold the same number of
KURA reversible beds and ANTILOP highchairs. It’s also possible from
this histogram that this never happened on any day.",54.0,Medium
873,Sp,22,Final,,Problem 11,Problem 11.3,"Determine the relative order of the three quantities below.

The number of days for which IKEA sold at least 35 ANTILOP
highchairs.
The number of days for which IKEA sold less than 25 ANTILOP
highchairs.
The number of days for which IKEA sold between 10 (inclusive) and 20
(exclusive) KURA reversible beds.


 (1)<(2)<(3)
 (1)<(3)<(2)
 (2)<(1)<(3)
 (2)<(3)<(1)
 (3)<(1)<(2)
 (3)<(2)<(1)","(2)<(3)<(1)
We can calculate the exact value of each of the quantities:

To find the number of days for which IKEA sold at least 35
ANTILOP highchairs, we need to find the total area of the rightmost
three bins of the ANTILOP distribution. Doing a similar calculation to
the one we did in",,
874,Sp,22,Final,,Problem 11,Problem 11.1,", \begin{aligned}
     \text{total area} &= 5*0.053 + 5*0.02 + 5*0.007 \\
                 &= 5*(0.08) \\
                 &= 0.4
\end{aligned} This is a proportion of 0.4 out of 30 days total,
which corresponds to 12 days.
To find the number of days for which IKEA sold less than 25
ANTILOP highchairs, we need to find the total area of the leftmost four
bins of the ANTILOP distribution. We can do this in the same way as
before, but to avoid the math, we can also use the information we’ve
already figured out to make this easier. In",,,
875,Sp,22,Final,,Problem 11,Problem 11.1,", we learned
that the KURA distribution included 15 days total in the two bins [20, 25) and [25,
30). Since the [25, 30) bin is
just slightly taller than the [20, 25)
bin, these 15 days must be split as 7 in the [20, 25) bin and 8 in the [25, 30) bin. Once we know the tallest bin
corresponds to 8 days, we can figure out the number of days
corresponding to every other bin just by eye. Anything that’s half as
tall as the tallest bin, for example, represents 4 days. The red lines
on the histogram below each represent 1 day, so we can easily count the
number of days in each bin.

Therefore summing up the number of days from the 4 bins for which IKEA
sold less than 25 ANTILOP highchairs gives 0 +
3 + 0 + 5 = 8. Altogether, that’s 8
days.
To find the number of days for which IKEA sold between 10 and 20
KURA reversible beds, we simply need to add the number of days in the
[10, 15) and [15, 20) bins of the KURA distribution. Using
the histogram with the red lines makes this easy to calculate as 4+5, or 9 days.

Therefore since 8<9<12, the correct answer is
(2)<(3)<(1).

Difficulty: ⭐️⭐️


The average score on this problem was 81%.",,81.0,Easy
876,Sp,22,Final,,Problem 12,Problem 12,"An IKEA chair designer is experimenting with some new ideas for
armchair designs. She has the idea of making the arm rests shaped like
bell curves, or normal distributions. A cross-section of the armchair
design is shown below.


This was created by taking the portion of the standard normal
distribution from z=-4 to z=4 and adjoining two copies of it, one
centered at z=0 and the other centered
at z=8. Let’s call this shape the
armchair curve.
Since the area under the standard normal curve from z=-4 to z=4
is approximately 1, the total area under the armchair curve is
approximately 2.
Complete the implementation of the two functions below:

area_left_of(z) should return the area under the
armchair curve to the left of z, assuming
-4 <= z <= 12, and
area_between(x, y) should return the area under the
armchair curve between x and y, assuming
-4 <= x <= y <= 12.

import scipy

def area_left_of(z):
    '''Returns the area under the armchair curve to the left of z.
       Assume -4 <= z <= 12'''
    if ___(a)___: 
        return ___(b)___ 
    return scipy.stats.norm.cdf(z)

def area_between(x, y):
    '''Returns the area under the armchair curve between x and y. 
    Assume -4 <= x <= y <= 12.'''
    return ___(c)___",,,
877,Sp,22,Final,,Problem 12,Problem 12.1,What goes in blank (a)?,"z>4 or
z>=4
The body of the function contains an if statement
followed by a return statement, which executes only when
the if condition is false. In that case, the function
returns scipy.stats.norm.cdf(z), which is the area under
the standard normal curve to the left of z. When
z is in the left half of the armchair curve, the area under
the armchair curve to the left of z is the area under the
standard normal curve to the left of z because the left
half of the armchair curve is a standard normal curve, centered at 0. So
we want to execute the return statement in that case, but
not if z is in the right half of the armchair curve, since
in that case the area to the left of z under the armchair
curve should be more than 1, and scipy.stats.norm.cdf(z)
can never exceed 1. This means the if condition needs to
correspond to z being in the right half of the armchair
curve, which corresponds to z>4 or z>=4,
either of which is a correct solution.",72.0,Medium
878,Sp,22,Final,,Problem 12,Problem 12.2,What goes in blank (b)?,"1+scipy.stats.norm.cdf(z-8)
This blank should contain the value we want to return when
z is in the right half of the armchair curve. In this case,
the area under the armchair curve to the left of z is the
sum of two areas:

the area under the entire left half of the armchair curve, which is
1, and
the area under the portion of the right half of the armchair curve
that falls to the left of z.

Since the right half of the armchair curve is just a standard normal
curve that’s been shifted to the right by 8 units, the area under that
normal curve to the left of z is the same as the area to
the left of z-8 on the standard normal curve that’s
centered at 0. Adding the portion from the left half and the right half
of the armchair curve gives
1+scipy.stats.norm.cdf(z-8).
For example, if we want to find the area under the armchair curve to
the left of 9, we need to total the yellow and blue areas in the image
below.

The yellow area is 1 and the blue area is the same as the area under
the standard normal curve (or the left half of the armchair curve) to
the left of 1 because 1 is the point on the left half of the armchair
curve that corresponds to 9 on the right half. In general, we need to
subtract 8 from a value on the right half to get the corresponding value
on the left half.",54.0,Medium
879,Sp,22,Final,,Problem 12,Problem 12.3,What goes in blank (c)?,"area_left_of(y) - area_left_of(x)
In general, we can find the area under any curve between
x and y by taking the area under the curve to
the left of y and subtracting the area under the curve to
the left of x. Since we have a function to find the area to
the left of any given point in the armchair curve, we just need to call
that function twice with the appropriate inputs and subtract the
result.",60.0,Medium
880,Sp,22,Final,,Problem 13,Problem 13,"Suppose you have correctly implemented the function
area_between(x, y) so that it returns the area under the
armchair curve between x and y, assuming the
inputs satisfy -4 <= x <= y <= 12.
Note: You can still do this question, even if you
didn’t know how to do the previous one.",,,
881,Sp,22,Final,,Problem 13,Problem 13.1,"What is the approximate value of
area_between(-2, 10)?

 1.9
 1.95
 1.975
 2","1.95
The area we want to find is shown below in two colors. We can find
the area in each half of the armchair curve separately and add the
results.

For the yellow area, we know that the area within 2 standard
deviations of the mean on the standard normal curve is 0.95. The
remaining 0.05 is split equally on both sides, so the yellow area is
0.975.
The blue area is the same by symmetry so the total shaded area is
0.975*2 = 1.95.
Equivalently, we can use the fact that the total area under the
armchair curve is 2, and the amount of unshaded area on either side is
0.025, so the total shaded area is 2 -
(0.025*2) = 1.95.",76.0,Easy
882,Sp,22,Final,,Problem 13,Problem 13.2,"What is the approximate value of
area_between(0.37, 8.37)?

 0.68
 0.95
 1
 1.5","1
The area we want to find is shown below in two colors.

As we saw in",,
883,Sp,22,Final,,Problem 12,Problem 12.2,", the point on the left half of the armchair
curve that corresponds to 8.37 is 0.37. This means that if we move the
blue area from the right half of the armchair curve to the left half, it
will fit perfectly, as shown below.

Therefore the total of the blue and yellow areas is the same as the
area under one standard normal curve, which is 1.

Difficulty: ⭐️⭐️


The average score on this problem was 76%.",,76.0,Easy
884,Sp,22,Final,,Problem 14,Problem 14,"An IKEA employee has access to a data set of the purchase amounts for
40,000 customer transactions. This data set is roughly normally
distributed with mean 150 dollars and standard deviation 25 dollars.",,,
885,Sp,22,Final,,Problem 14,Problem 14.1,"Why is the distribution of purchase amounts roughly normal?

 because of the Central Limit Theorem
 for some other reason","for some other reason
The data that we have is a sample of purchase amounts. It is not a
sample mean or sample sum, so the Central Limit Theorem does not apply.
The data just naturally happens to be roughly normally distributed, like
human heights, for example.",42.0,Hard
886,Sp,22,Final,,Problem 14,Problem 14.2,"Shiv spends 300 dollars at IKEA. How would we describe Shiv’s
purchase in standard units?

 0 standard units
 2 standard units
 4 standard units
 6 standard units","6 standard units
To standardize a data value, we subtract the mean of the distribution
and divide by the standard deviation:
\begin{aligned}
        \text{standard units} &= \frac{300 - 150}{25} \\
                    &= \frac{150}{25} \\
                    &= 6
\end{aligned}
A more intuitive way to think about standard units is the number of
standard deviations above the mean (where negative means below the
mean). Here, Shiv spent 150 dollars more than average. One standard
deviation is 25 dollars, so 150 dollars is six standard deviations.",97.0,Easy
887,Sp,22,Final,,Problem 14,Problem 14.3,"Give the endpoints of the CLT-based 95% confidence interval for the
mean IKEA purchase amount, based on this data.","149.75 and 150.25 dollars
The Central Limit Theorem tells us about the distribution of the
sample mean purchase amount. That’s not the distribution the employee
has, but rather a distribution that shows how the mean of a
different sample of 40,000 purchases might have turned out.
Specifically, we know the following information about the distribution
of the sample mean.

It is roughly normally distributed.
Its mean is about 150 dollars, the same as the mean of the
employee’s sample.
Its standard deviation is about \frac{\text{sample standard
deviation}}{\sqrt{\text{sample size}}}=\frac{25}{\sqrt{40000}} =
\frac{25}{200} = \frac{1}{8}.

Since the distribution of the sample mean is roughly normal, we can
find a 95% confidence interval for the sample mean by stepping out two
standard deviations from the center, using the fact that 95% of the area
of a normal distribution falls within 2 standard deviations of the mean.
Therefore the endpoints of the CLT-based 95% confidence interval for the
mean IKEA purchase amount are

150 - 2*\frac{1}{8} = 149.75
dollars, and
150 + 2*\frac{1}{8} = 150.25
dollars.",36.0,Hard
888,Sp,22,Final,,Problem 15,Problem 15,"There are 52 IKEA locations in the United States, and there are 50
states.
Which of the following describes how to calculate the total variation
distance between the distribution of IKEA locations by state and the
uniform distribution?

 For each IKEA location, take the absolute difference between 1/50 and
the number of IKEAs in the same state divided by the total number of
IKEA locations. Sum these values across all locations and divide by
two.
 For each IKEA location, take the absolute difference between the
number of IKEAs in the same state and the average number of IKEAs in
each state. Sum these values across all locations and divide by two.
 For each state, take the absolute difference between 1/50 and the
number of IKEAs in that state divided by the total number of IKEA
locations. Sum these values across all states and divide by two.
 For each state, take the absolute difference between the number of
IKEAs in that state and the average number of IKEAs in each state. Sum
these values across all states and divide by two.
 None of the above.","For each state, take the absolute
difference between 1/50 and the number of IKEAs in that state divided by
the total number of IKEA locations. Sum these values across all states
and divide by two.
We’re looking at the distribution across states. Since there are 50
states, the uniform distribution would correspond to having a fraction
of 1/50 of the IKEA locations in each state. We can picture this as a
sequence with 50 entries that are all the same: (1/50, 1/50, 1/50, 1/50, \dots)
We want to compare this to the actual distribution of IKEAs across
states, which we can think of as a sequence with 50 entries,
representing the 50 states, but where each entry is the proportion of
IKEA locations in a given state. For example, maybe the distribution
starts out like this: (3/52, 1/52, 0/52,
1/52, \dots) We can interpret each entry as the number of IKEAs
in a state divided by the total number of IKEA locations. Note that this
has nothing to do with the average number of IKEA locations in each
state, which is 52/50.
The way we take the TVD of two distributions is to subtract the
distributions, take the absolute value, sum up the values, and divide by
2. Since the entries represent states, this process aligns with the
given answer.",30.0,Hard
889,Sp,22,Final,,Problem 16,Problem 16,"The HAUGA bedroom furniture set includes two items, a bed frame and a
bedside table. Suppose the amount of time it takes someone to assemble
the bed frame is a random quantity drawn from the probability
distribution below.",,,
890,Sp,22,Final,,Problem 16,Problem 16.1,"What is the probability that Stella assembles the bed frame in 10
minutes if we know it took her less than 30 minutes to assemble? Give
your answer as a decimal between 0 and 1.","0.2
We want to find the probability that Stella assembles the bed frame
in 10 minutes, given that she assembles it in less than 30 minutes. The
multiplication rule can be rearranged to find the conditional
probability of one event given another.
\begin{aligned}
        P(A \text{ and } B) &= P(A \text{ given } B)*P(B)\\
        P(A \text{ given } B) &= \frac{P(A \text{ and } B)}{P(B)}
\end{aligned}
Let’s, therefore, define events A
and B as follows:

A is the event that Stella
assembles the bed frame in 10 minutes.
B is the event that Stella
assembles the bed frame in less than 30 minutes.

Since 10 minutes is less than 30 minutes, A
\text{ and } B is the same as A
in this case. Therefore, P(A \text{ and } B) =
P(A) = 0.1.
Since there are only two ways to complete the bed frame in less than
30 minutes (10 minutes or 20 minutes), it is straightforward to find
P(B) using the addition rule P(B) = 0.1 + 0.4. The addition rule can be
used here because assembling the bed frame in 10 minutes and assembling
the bed frame in 20 minutes are mutually exclusive. We could
alternatively find P(B) using the
complement rule, since the only way not to complete the bed frame in
less than 30 minutes is to complete it in exactly 30 minutes, which
happens with a probability of 0.5. We’d get the same answer, P(B) = 1 - 0.5 = 0.5.
Plugging these numbers in gives our answer.
\begin{aligned}
        P(A \text{ given } B) &= \frac{P(A \text{ and } B)}{P(B)}\\
                              &= \frac{0.1}{0.5}\\
                              &= 0.2
\end{aligned}",72.0,Medium
891,Sp,22,Final,,Problem 16,Problem 16.2,"What is the probability that Ryland assembles the bedside table in 40
minutes if we know that it took him 30 minutes to assemble the bed
frame? Give your answer as a decimal between 0 and 1","0.4
We are told that the time it takes someone to assemble the bedside
table is a random quantity, independent of the time it takes
them to assemble the bed frame. Therefore we can disregard the
information about the time it took him to assemble the bed frame and
read directly from the probability distribution that his probability of
assembling the bedside table in 40 minutes is 0.4.",82.0,Easy
892,Sp,22,Final,,Problem 16,Problem 16.3,"What is the probability that Jin assembles the complete HAUGA set in
at most 60 minutes? Give your answer as a decimal between 0 and 1.","0.53
There are several different ways for the total assembly time to take
at most 60 minutes:

The bed frame takes 10 minutes and the bedside table takes any
amount of time.
The bed frame takes 20 minutes and the bedside table takes 30 or 40
minutes.
The bed frame takes 30 minutes and the bedside table takes 30
minutes.

Using the multiplication rule, these probabilities are:

0.1*1 = 0.1
0.4*0.7 = 0.28
0.5*0.3 = 0.15

Finally, adding them up because they represent mutually exclusive
cases, we get 0.1+0.28+0.15 = 0.53.",,
893,Sp,22,Final,,Problem 17,Problem 17,"Suppose the price of an IKEA product and the cost to have it
assembled are linearly associated with a correlation of 0.8. Product
prices have a mean of 140 dollars and a standard deviation of 40
dollars. Assembly costs have a mean of 80 dollars and a standard
deviation of 10 dollars. We want to predict the assembly cost of a
product based on its price using linear regression.",,,
894,Sp,22,Final,,Problem 17,Problem 17.1,"The NORDMELA 4-drawer dresser sells for 200 dollars. How much do we
predict its assembly cost to be?","92 dollars
We first use the formulas for the slope, m, and intercept, b, of the regression line to find the
equation. For our application, x is the
price and y is the assembly cost since
we want to predict the assembly cost based on price.
\begin{aligned}
        m &= r*\frac{\text{SD of }y}{\text{SD of }x} \\
                     &= 0.8*\frac{10}{40} \\
                     &= 0.2\\
        b &= \text{mean of }y - m*\text{mean of }x  \\
                     &= 80 - 0.2*140 \\
                     &= 80 - 28 \\
                     &= 52
\end{aligned}
Now we know the formula of the regression line and we simply plug in
x=200 to find the associated y value.
\begin{aligned}
        y &= mx+b \\
        y &= 0.2x+52 \\
         &= 0.2*200+52 \\
         &= 92
\end{aligned}",76.0,Easy
895,Sp,22,Final,,Problem 17,Problem 17.2,"The IDANÄS wardrobe sells for 80 dollars more than the KLIPPAN
loveseat, so we expect the IDANÄS wardrobe will have a greater assembly
cost than the KLIPPAN loveseat. How much do we predict the difference in
assembly costs to be?","16 dollars
The slope of a line describes the change in y for each change of 1 in x. The difference in x values for these two products is 80, so the
difference in y values is m*80 = 0.2*80 = 16 dollars.
An equivalent way to state this is:
\begin{aligned}
        m &= \frac{\text{ rise, or change in } y}{\text{ run, or
change in } x} \\
        0.2 &= \frac{\text{ rise, or change in } y}{80} \\
        0.2*80 &= \text{ rise, or change in } y \\
    16 &= \text{ rise, or change in } y
\end{aligned}",65.0,Medium
896,Sp,22,Final,,Problem 17,Problem 17.3,"If we create a 95% prediction interval for the assembly cost of a 100
dollar product and another 95% prediction interval for the assembly cost
of a 120 dollar product, which prediction interval will be wider?

 The one for the 100 dollar product.
 The one for the 120 dollar product.","The one for the 100 dollar product.
Prediction intervals get wider the further we get from the point
(\text{mean of } x, \text{mean of } y)
since all regression lines must go through this point. Since the average
product price is 140 dollars, the prediction interval will be wider for
the 100 dollar product, since it’s the further of 100 and 120 from 140.",,
897,Sp,22,Final,,Problem 18,Problem 18,"For each IKEA desk, we know the cost of producing the desk, in
dollars, and the current sale price of the desk, in dollars. We want to
predict sale price based on production cost using linear regression.",,,
898,Sp,22,Final,,Problem 18,Problem 18.1,"For this scenario, which of the following most likely describes the
slope of the regression line when both variables are measured in
dollars?

 less than 0
 between 0 and 1, exclusive
 more than 1
 none of the above (exactly equal to 0 or 1)","more than 1
The slope of a line represents the change in y for each change of 1 in x. Therefore, the slope of the regression
line is the amount we’d predict the sale price to increase when the
production cost of an item increases by one dollar. In other words, it’s
the sale price per dollar of production cost. This is almost certainly
more than 1, otherwise the company would not make a profit. We’d expect
that for any company, the sale price of an item should exceed the
production cost, meaning the slope of the regression line has a value
greater than one.",72.0,Medium
899,Sp,22,Final,,Problem 18,Problem 18.2,"For this scenario, which of the following most likely describes the
slope of the regression line when both variables are measured in
standard units?

 less than 0
 between 0 and 1, exclusive
 more than 1
 none of the above (exactly equal to 0 or 1)","between 0 and 1, exclusive
When both variables are measured in standard units, the slope of the
regression line is the correlation coefficient. Recall that correlation
coefficients are always between -1 and 1, however, because it’s not
realistic for production cost and sale price to be negatively correlated
(as that would mean products sell for less if they cost more to produce)
we can limit our choice of answer to values between 0 and 1. Because a
coefficient of 0 would mean there is no correlation and 1 would mean
perfect correlation (that is, plotting the data would create a line),
these are unlikely occurrences leaving us with the answer being between
0 and 1, exclusive.",86.0,Easy
900,Sp,22,Final,,Problem 18,Problem 18.3,"The residual plot for this regression is shown below.

What is being represented on the horizontal axis of the residual
plot?

 actual production cost
 actual sale price
 predicted production cost
 predicted sale price","actual production cost
Residual plots show x on the
horizontal axis and the residuals, or differences between actual y values and predicted y values, on the vertical axis. Therefore,
the horizontal axis here shows the production cost. Note that we are not
predicting production costs at all, so production cost means the
actual cost to produce a product.",43.0,Hard
901,Sp,22,Final,,Problem 18,Problem 18.4,"Which of the following is a correct conclusion based on this residual
plot? Select all that apply.

 The correlation between production cost and sale price is weak.
 It would be better to fit a nonlinear curve.
 Our predictions will be more accurate for some inputs than
others.
 We don’t have enough data to do regression.
 The regression line is not the best-fitting line for this data
set.
 The data set is not representative of the population.","It would be better to fit a nonlinear
curve.
Let’s go through each answer choice.

The correlation between production cost and sale price could be
very strong. After all, we are able to predict the sale price within ten
dollars almost all the time, since residuals are almost all between -10
and 10.
It would be better to fit a nonlinear curve because the residuals
show a pattern. Reading from left to right, they go from mostly negative
to mostly positive to mostly negative again. This suggests that a
nonlinear curve might be a better fit for our data.
Our predictions are typically within ten dollars of the actual
sale price, and this is consistent throughout. We see this on the
residual plot by a fairly even vertical spread of dots as we scan from
left to right. This data is not heteroscedastic.
We can do regression on a dataset of any size, even a very small
data set. Further, this dataset is decently large, since there are a
good number of points in the residual plot.
The regression line is always the best-fitting
line for any dataset. There may be other curves that are better fits
than lines, but when we restrict to lines, the best of the bunch is the
regression line.
We have no way of knowing how representative our data set is of
the population. This is not something we can discern from a residual
plot because such a plot contains no information about the population
from which the data was drawn.",61.0,Medium
902,Sp,23,Final,,Problem 1,Problem 1,,,,
903,Sp,23,Final,,Problem 1,Problem 1.1,"Complete the implementation of the function
most_sunshine, which takes in country, the
name of a country, and month, the name of a month
(e.g. ""Apr""), and returns the name of the city (as a
string) in country with the most sunshine hours in
month, among the cities in sun. Assume there
are no ties.
    def most_sunshine(country, month):
        country_only = __(a)__
        return country_only.__(b)__
What goes in blanks (a) and (b)?","(a):
sun[sun.get(""Country"") == country], (b):
sort_values(month).get(""City"").iloc[-1] or
sort_values(month, ascending=False).get(""City"").iloc[0]
What goes in blank (a)?
sun[sun.get(""Country"") == country] To identify cities only
within the specified country, we need to query for the rows in the
sun DataFrame where the ""Country"" column
matches the given country. The expression
sun.get(""Country"") == country creates a Boolean Series,
where each entry is True if the corresponding row’s
""Country"" column matches the provided country
and False otherwise. When this Boolean series is used to
index into sun DataFrame, it keeps only the rows for which
sun.get(""Country"") == country is True,
effectively giving us only the cities from the specified country.",78.0,Easy
904,Sp,23,Final,,Problem 1,Problem 1.2,"In this part only, assume that all ""City"" names in
sun are unique.
Consider the DataFrame cities defined below.
cities = sun.groupby(""City"").mean().reset_index()
Fill in the blanks so that the DataFrame that results from the
sequence of steps described below is identical to
cities.
“Sort sun by (c) in
(d) order (e).”
What goes in blank (c)?

 ""Country""
 ""City""
 ""Jan""
 ""Year""


What goes in blank (d)?

 ascending
 descending


What goes in blank (e)?

 and drop the ""Country"" column
 and drop the ""Country"" and ""City""
columns
 and reset the index
 , drop the ""Country"" column, and reset the index
 , drop the ""Country"" and ""City"" columns,
and reset the index
 Nothing, leave blank (e) empty","(c): ""City"", (d): ascending,
(e): drop the ""Country"" column, and reset the index
Let’s start by understanding the code provided in the question:
The .groupby(""City"") method groups the data in the
sun DataFrame by unique city names. Since every city name
in the DataFrame is unique, this means that each group will consist of
just one row corresponding to that city.
After grouping by city, the .mean() method computes the
average of each column for each group. Again, as each city name is
unique, this operation doesn’t aggregate multiple rows but merely
reproduces the original values for each city. (For example, the value in
the ""Jan"" column for the row with the index
""Hamilton"" will just be 229.8, which we see in the first
row of the preview of sun.)
Finally, .reset_index() is used to reset the DataFrame’s
index. When using .groupby, the column we group by (in this
case, ""City"") becomes the index. By resetting the index,
we’re making ""City"" a regular column again and setting the
index to 0, 1, 2, 3, …

What goes in blank (c)? ""City""
When we group on ""City"", the index of the DataFrame is set
to ""City"" names, sorted in ascending alphabetical order
(this is always the behavior of groupby). Since all city
names are unique, the number of rows in
sun.groupby(""City"").mean() is the same as the number of
rows in sun, and so grouping on ""City""
effectively sorts the DataFrame by ""City"" and sets the
index to ""City"". To replicate the order in
cities, then, we must sort sun by the
""City"" column in ascending order.",97.0,Easy
905,Sp,23,Final,,Problem 1,Problem 1.3,"True or False: In the code below, Z is guaranteed to
evaluate to True.
x = sun.groupby([""Country"", ""Year""]).mean().shape[0]
y = sun.groupby(""Country"").mean().shape[0]
z = (x >= y)

 True
 False","True
Let’s us look at each line of code separately:

x = sun.groupby([""Country"", ""Year""]).mean().shape[0]:
This line groups the sun DataFrame by both
""Country"" and ""Year"", then computes the mean.
As a result, each unique combination of ""Country"" and
""Year"" will have its own row. For instance, if there are
three different values in the ""Year"" column for a
particular country, that country will appear three times in the
DataFrame sun.groupby([""Country"", ""Year""]).mean().
y = sun.groupby(""Country"").mean().shape[0]: When
grouping by ""Country"" alone, each unique country in the
sun DataFrame is represented by one row, independent of the
information in other columns.
z = (x >= y): This comparison checks whether the
number of rows produced by grouping by both ""Country"" and
""Year"" (which is x) is greater than or equal
to the number of rows produced by grouping only by
""Country"" (which is y).

Given our grouping logic:

If every country in the sun DataFrame has only a
single unique value in the ""Year"" column (e.g. if the
""Year"" value for all ciites in the United States was always
3035.9, and if the ""Year"" value for all cities in Nigeria
was always 1845.4, etc.), then the number of rows when grouping by both
""Country"" and ""Year"" will be equal to the
number of rows when grouping by ""Country"" alone. In this
scenario, x will be equal to y.
If at least one country in the sun DataFrame has at
least two different values in the ""Year"" column (e.g. if
there are at least two cities in the United States with different values
in the ""Year"" column), then there will be more rows when
grouping by both ""Country"" and ""Year"" compared
to grouping by ""Country"" alone. This means x
will be greater than y.

Considering the above scenarios, there’s no situation where the value
of x can be less than the value of y.
Therefore, z will always evaluate to True.",70.0,Medium
906,Sp,23,Final,,Problem 1,Problem 1.4,"What does the following expression evaluate to?
sun.groupby(""Country"").max().get(""City"").iloc[0]

 A
 B
 C
 D
 E
 F
 G","E. The last city, alphabetically, in the
first country, alphabetically.

Let’s break down the code:

sun.groupby(""Country"").max(): This line of code
groups the sun DataFrame by the ""Country""
column and then determines the maximum for every other
column within each country group. Since the values in the
""City"" column are stored as strings, and the maximum of a
Series of strings is the last string alphabetically, the values in the
""City"" column of this DataFrame will contain the last city,
alphabetically, of each country.
.get(""City""): .get(""City"") accesses the
""City"" column.
.iloc[0]: Finally, .iloc[0] selects the
""City"" value from the first row. The first row corresponds
to the first country alphabetically because groupby sorted
the DataFrame by ""Country"" in ascending order. The value in
the ""City"" column that .iloc[0] selects, then,
is the name of the last city, alphabetically, in the first country,
alphabetically.",36.0,Hard
907,Sp,23,Final,,Problem 1,Problem 1.5,"What does the following expression evaluate to?
sun.groupby(""Country"").sum().get(""City"").iloc[0]

 A
 B
 C
 D
 E
 F
 G","G. Nothing, because it errors.

Let’s break down the code:

sun.groupby(""Country"").sum(): This groups the
sun DataFrame by the ""Country"" column and
computes the sum for each numeric column within each country group.
Since ""City"" is non-numeric, it will be dropped.
.get(""City""): This operation attempts to retrieve
the ""City"" column from the resulting DataFrame. However,
since the ""City"" column was dropped in the previous step,
this will raise a KeyError, indicating that the column is not present in
the DataFrame.",73.0,Medium
908,Sp,23,Final,,Problem 1,Problem 1.6,"What does the following expression evaluate to?
sun.groupby(""Country"").count().sort_values(""Jan"").index[-1]

 A
 B
 C
 D
 E
 F
 G","A. The name of the country with the most
cities.

Let’s break down the code:

sun.groupby(""Country"").count(): This groups the sun
DataFrame by the ""Country"" column. The
.count() method then returns the number of rows in each
group for each column. Since we’re grouping by ""Country"",
and since the rows in sun correspond to cities, this is
counting the number of cities in each country.
.sort_values(""Jan""): The result of the previous
operation is a DataFrame with ""Country"" as the index and
the number of cities per country stored in every other column. The
""City, ""Jan"", ""Feb"",
""Mar"", etc. columns in the resulting DataFrame all contain
the same information. Sorting by ""Jan"" sorts the DataFrame
by the number of cities each country has in ascending order.
.index[-1]: This retrieves the last index value from
the sorted DataFrame, which corresponds to the name of the country with
the most cities.",61.0,Medium
909,Sp,23,Final,,Problem 1,Problem 1.7,"What does the following expression evaluate to?
sun.groupby(""Country"").count().sort_values(""City"").get(""City"").iloc[-1]

 A
 B
 C
 D
 E
 F
 G","C. The number of cities in the country with
the most cities.

Let’s break down the code:

sun.groupby(""Country"").count(): This groups the sun
DataFrame by the ""Country"" column. The
.count() method then returns the number of rows in each
group for each column. Since we’re grouping by ""Country"",
and since the rows in sun correspond to cities, this is
counting the number of cities in each country.
.sort_values(""City""): The result of the previous
operation is a DataFrame with ""Country"" as the index and
the number of ""City""s per ""Country"" stored in
every other column. The ""City, ""Jan"",
""Feb"", ""Mar"", etc. columns in the resulting
DataFrame all contain the same information. Sorting by
""City"" sorts the DataFrame by the number of cities each
country has in ascending order.
.get(""City""): This retrieves the ""City""
column from the sorted DataFrame, which contains the number of cities in
each country.
.iloc[-1]: This gets the last value from the
""City"" column, which corresponds to the number of cities in
the country with the most cities.",57.0,Medium
910,Sp,23,Final,,Problem 2,Problem 2,"Vanessa is a big Formula 1 racing fan, and wants to plan a trip to
Monaco, where the Monaco Grand Prix is held. Monaco is an example of a
“city-state” — that is, a city that is its own country. Singapore is
another example of a city-state.
We’ll say that a row of sun corresponds to a city-state
if its ""Country"" and ""City"" values are
equal.",,,
911,Sp,23,Final,,Problem 2,Problem 2.1,"Fill in the blanks so that the expression below is equal to the total
number of sunshine hours in October of all city-states in
sun.
    sun[__(a)__].__(b)__
What goes in blanks (a) and (b)?","(a):
sun.get(""Country"") == sun.get(""City""), (b):
.get(""Oct"").sum()
What goes in blank (a)?
sun.get(""Country"") == sun.get(""City"")
This expression compares the ""Country"" column to the
""City"" column for each row in the sun
DataFrame. It returns a Boolean Series where each value is
True if the corresponding ""Country"" and
""City"" are the same (indicating a city-state) and
False otherwise.",79.0,Easy
912,Sp,23,Final,,Problem 2,Problem 2.2,"Fill in the blanks below so that the expression below is also equal
to the total number of sunshine hours in October of all city-states in
sun.
Note: What goes in blank (b) is the same as what goes in blank
(b) above.
sun.get([""Country""]).merge(__(c)__).__(b)__
What goes in blank (c)?","sun, left_on=""Country"", right_on=""City""

Let’s break down the code:

sun.get([""Country""]): This extracts just the
""Country"" column from the sun DataFrame, as a
DataFrame. (It’s extracted as a DataFrame since we passed a list to
.get instead of a single string.)
.merge(sun, left_on=""Country"", right_on=""City""):
Here, we’re using the .merge method to merge a version of
sun with just the ""Country"" column (which is
our left DataFrame) with the entire sun DataFrame
(which is our right DataFrame). The merge is done by matching
""Country""s from the left DataFrame with
""City""s from the right DataFrame. This way, rows in the
resulting DataFrame correspond to city-states, as it only contains
entries where a country’s name is the same as a city’s name.
.get(""Oct"").sum(): After merging, we use
.get(""Oct"") to retrieve the ""Oct"" column,
which represents the sunshine hours in October. Finally,
.sum() computes the total number of sunshine hours in
October for all the identified city-states.",50.0,Medium
913,Sp,23,Final,,Problem 3,Problem 3,"This summer, Zoe wants to explore parts of the United States that she
hasn’t been to yet. In her process of figuring out where to go, she
creates a histogram depicting the distribution of the number of sunshine
hours in July across all cities in the United States in
sun.


Suppose usa is a DataFrame with all of the columns in
sun but with only the rows where ""Country"" is
""United States"".",,,
914,Sp,23,Final,,Problem 3,Problem 3.1,"What is the value of mystery below?
    cond = (usa.get(""Jul"") >= 370) & (usa.get(""Jul"") < 430)
    mystery = 100 * np.count_nonzero(cond) / usa.shape[0]

 2
 8
 12
 16
 18
 20","12
cond is a Series that contains True for
each row in usa where ""Jul"" is greater than or
equal to 370 and less than 430. mystery, then, is the
percentage of values in usa in which
cond is True. This is because
np.count_nonzero(cond) is the number of Trues
in cond, np.count_nonzero(cond) / usa.shape[0]
is the proportion of values in cond that are
True, and
100 * np.count_nonzero(cond) / usa.shape[0] is the
percentage of values in cond that are True.
Our goal here, then, is to use the histogram to find the
percentage of values in the histogram between 370 (inclusive) and 430
(exclusive).
We know that in histograms, the area of each bar is equal to the
proportion of data points that fall within its bin’s range.
Conveniently, there’s only one bar we need to look at – the one
corresponding to the bin [370, 430). That bar has a width of 430 - 370 = 60 and a height of 0.002. Then,
the area of that bar – i.e. the proportion of values that are between
370 (inclusive) and 430 (exclusive) is:
\text{proportion} = \text{area} =
\text{height} \cdot \text{width} = 0.002 \cdot 60 = 0.12
This means that the proportion of values in [370, 430) is 0.12, which
means that the percentage of values in [370, 430) is 12%, and that
mystery evaluates to 12.",83.0,Easy
915,Sp,23,Final,,Problem 3,Problem 3.2,"There are 5 more cities with between 370 and 430 sunshine hours in
July than there are cities with between 270 and 290 sunshine hours in
July.
How many cities in the United States are in sun? Give
your answer as a positive integer, rounded to the nearest multiple of 10
(that is, your answer should end in a 0).","250
In the previous part, we learned that the proportion of cities in the
usa DataFrame in the interval [370, 430) (i.e. that have
between 370 and 430 sunshine hours in July) is 0.12. To use the fact
that there are 5 more cities in the interval [370, 430) than there are
in the interval [270, 290), we need to first find the proportion of
cities in the interval [270, 290). To do so, we look at the [270, 290)
bin, which has a width of 290 - 270 =
20 and a height of 0.005:
\text{proportion} = 0.005 \cdot 20 =
0.10
We are told that there are 5 more cities in the [370, 430) interval
than there are in the [270, 290) interval. Given the proportions we’ve
computed, we have that:
\text{difference in proportions} = 0.12 -
0.1 = 0.02
If 0.02 \cdot \text{number of
cities} is 5, then \text{number of
cities} = 5 \cdot \frac{1}{0.02} = 5 \cdot 50 = 250.",49.0,Hard
916,Sp,23,Final,,Problem 3,Problem 3.3,"Let m be the mean number of sunshine
hours in July for all US cities in sun, in standard units.
Select the true statement below.

 m = -1
 -1 < m < 0
 m = 0
 0 < m < 1
 m = 1
 m > 1","m = 0

When we standardize a dataset, the mean of the resulting values is
always 0 and the standard deviation of the resulting values is always 1.
This tells us right away that the answer is m
= 0. Intuitively, we know that a value in standard units
represents the number of standard deviations that value is above or
below the mean of the column it came from. m is equal to the mean of the column it came
from, so m in standard units is 0.
If we’d like to approach this more algebracically, we can remember
the formula for converting a value x_i
from a column x to standard units:
 x_{i \: \text{(su)}} = \frac{x_i -
\text{mean of } x}{\text{SD of } x} 
Let x be the column (i.e. Series)
containing the mean number of sunshine hours in July for all US cities
in sun. m, by definition,
is the mean of x. Then,
 m_{\text{(su)}} = \frac{m - \text{mean of
} x}{\text{SD of } x} = \frac{m - m}{\text{SD of }x} = 0
Given that m is the mean of column
x, the numerator of m_\text{(su)} is 0, and hence m_\text{(su)} is 0.",62.0,Medium
917,Sp,23,Final,,Problem 3,Problem 3.4,"Let s be the standard deviation of
the number of sunshine hours in July for all US cities sun,
in standard units. Select the true statement below.

 s = -1
 -1 < s <0
 s = 0
 0 < s < 1
 s = 1
 s > 1","s = 1

As mentioned in the previous solution, when we standardize a dataset,
the mean of the resulting values is always 0 and the standard deviation
of the resulting values is always 1.",46.0,Hard
918,Sp,23,Final,,Problem 3,Problem 3.5,"Let d be the median of the number of
sunshine hours in July for all US cities in sun, in
standard units. Select the true statement below.

 d = -1
 -1 < d < 0
 d = 0
 0 < d < 1
 d = 1
 d > 1","-1 < d <
0

In the histogram, we see that the distribution of the number of
sunshine hours in July for all US cities in sunis skewed
right, or has a right tail. This means that this distribution’s mean is
dragged in the direction of its tail and is larger than its median.
Since the mean in standard units is 0, and the median is less than the
mean, the median in standard units must be negative. There’s no property
that states that the median is exactly -1, and the median is only
slightly less than the mean, which means that it must be the case that
-1 < d < 0.",42.0,Hard
919,Sp,23,Final,,Problem 3,Problem 3.6,"True or False: The distribution of the number of sunshine hours in
July for all US cities in sun, in standard units, is
roughly normal.

 True
 False
 Impossible to tell","False

The original histogram depicting the distribution of the number of
sunshine hours in July for all US cities is right-skewed. When data is
converted to standard units, the shape of the distribution does not
change. Therefore, if the original data is right-skewed, the
standardized data will also be right-skewed.",45.0,Hard
920,Sp,23,Final,,Problem 4,Problem 4,"For each city in sun, we have 12 numbers, corresponding
to the number of sunshine hours it sees in January, February, March, and
so on, through December. (There is also technically a 13th number, the
value in the ""Year"" column, but we will ignore it for the
purposes of this question.)
We say that a city’s number of sunshine hours peaks
gradually if both of the following conditions are true:

Each month from February to June has a number of sunshine hours
greater than or equal to the month before it.
Each month from August to December has a number of sunshine hours
less than or equal to the month before it.

For example, the number of sunshine hours per month in Doris’
hometown of Guangzhou, China peaks gradually:
62, 65, 71, 104, 118, 202, 181, 173, 172,
170, 166, 140
However, the number of sunshine hours per month in Charlie’s hometown
of Redwood City, California does not peak gradually, since 325 > 311 and 247 < 271:
185, 207, 269, 309, 325, 311, 313, 287,
247, 271, 173, 160
Complete the implementation of the function
peaks_gradually, which takes in an array hours
of length 12 corresponding to the number of sunshine hours per month in
a city, and returns True if the city’s number of sunshine
hours peaks gradually and False otherwise.
    def peaks_gradually(hours):
        for i in np.arange(5):
            cur_left = hours[5 - i]
            next_left = hours[__(a)__]
            cur_right = hours[__(b)__]
            next_right = hours[6 + i + 1]

            if __(c)__:
                __(d)__
            
        __(e)__",,,
921,Sp,23,Final,,Problem 4,Problem 4.1,What goes in blank (a)?,"5 - i - 1 or
4 - i

Before filling in the blanks, let’s discuss the overall strategy of
the problem. The idea is as follows

When i = 0,

Compare cur_left, which is the sunshine hours for June
(month 5, since 5 - i = 5 - 0 = 5), to
next_left, which is the sunshine hours for May (month 5 - i - 1 = 4). If
next_left > cur_left, it means that May has more
sunshine hours than June, which means the sunshine hours for this city
don’t peak gradually. (Remember, for the number of sunshine hours to
peak gradually, we need it to be the case that each month from February
to June has a number of sunshine hours greater than or equal to the
month before it.)
Also, compare cur_right, which is the sunshine hours
for July (month 6, since 6 + i = 6 + 0 =
6), to next_right, which is the sunshine hours for
August (month 6 + i + 1 = 7). If
next_right > cur_right, it means that August has more
sunshine hours than July, which means the sunshine hours for this city
don’t peak gradually. (Remember, for the number of sunshine hours to
peak gradually, we need it to be the case that each month from August to
December has a number of sunshine hours less than or equal to the month
before it.)
If
next_left > cur_left or next_right > cur_right, then
we don’t need to look at any other pairs of months, and can just
return False. Otherwise, we keep looking.

When i = 1, cur_left
and next_left will “step backwards” and refer to May (month
4, since 5 - i = 5 - 1 = 4) and April
(month 3, since 5 - i - 1 = 3),
respectively. Simililarly, cur_right and
next_right will “step forwards” and refer to August and
September, respectively. The above process is repeated.
This is repeated until we check January (month 0) / February (month
1) and November (month 10) / December (month 11); if by that point, the
condition
next_left > cur_left or next_right > cur_right was
never True, then it must be the case that the sunshine
hours for this city peak gradually, and we can return True
outside of the for-loop!

Focusing on blank (a) specifically, it needs to contain the position
of next_left, which is the index of the month before the
current left month. Since the current month is at 5 - i,
the next month needs to be at 5 - i - 1.",62.0,Medium
922,Sp,23,Final,,Problem 4,Problem 4.2,What goes in blank (b)?,"6 + i

Using the same logic as for blank (a), blank (b) needs to contain the
position of cur_right, which is the index of the month
before the next right month. Since the next right month is at
6 + i + 1, the current right month is at
6 + i.",67.0,Medium
923,Sp,23,Final,,Problem 4,Problem 4.3,"What goes in blank (c)?

 next_left < cur_left or next_right < cur_right
 next_left < cur_left and next_right < cur_right
 next_left > cur_left or next_right > cur_right
 next_left > cur_left and next_right > cur_right","next_left > cur_left or next_right > cur_right

Explained in the answer to blank (a).",35.0,Hard
924,Sp,23,Final,,Problem 4,Problem 4.4,"What goes in blank (d)?

 return True
 return False","return False

Explained in the answer to blank (a).",50.0,Medium
925,Sp,23,Final,,Problem 4,Problem 4.5,"What goes in blank (e)?

 return True
 return False","return True

Explained in the answer to blank (a).",54.0,Medium
926,Sp,23,Final,,Problem 5,Problem 5,"In some cities, the number of sunshine hours per month is relatively
consistent throughout the year. São Paulo, Brazil is one such city; in
all months of the year, the number of sunshine hours per month is
somewhere between 139 and 173. New York City’s, on the other hand,
ranges from 139 to 268.
Gina and Abel, both San Diego natives, are interested in assessing
how “consistent"" the number of sunshine hours per month in San Diego
appear to be. Specifically, they’d like to test the following
hypotheses:

Null Hypothesis: The number of sunshine hours
per month in San Diego is drawn from the uniform distribution, \left[\frac{1}{12}, \frac{1}{12}, ...,
\frac{1}{12}\right]. (In other words, the number of sunshine
hours per month in San Diego is equal in all 12 months of the
year.)
Alternative Hypothesis: The number of sunshine
hours per month in San Diego is not drawn from the uniform
distribution.

As their test statistic, Gina and Abel choose the total variation
distance. To simulate samples under the null, they will sample from a
categorical distribution with 12 categories — January, February, and so
on, through December — each of which have an equal probability of being
chosen.",,,
927,Sp,23,Final,,Problem 5,Problem 5.1,"In order to run their hypothesis test, Gina and Abel need a way to
calculate their test statistic. Below is an incomplete implementation of
a function that computes the TVD between two arrays of length 12, each
of which represent a categorical distribution.
    def calculate_tvd(dist1, dist2):
        return np.mean(np.abs(dist1 - dist2)) * ____
Fill in the blank so that calculate_tvd works as
intended.

 1 / 6
 1 / 3
 1 / 2
 2
 3
 6","6
The TVD is the sum of the absolute differences in proportions,
divided by 2. In the code to the left of the blank, we’ve computed the
mean of the absolute differences in proportions, which is the same as
the sum of the absolute differences in proportions, divided by 12 (since
len(dist1) is 12). To correct the fact that we
divided by 12, we multiply by 6, so that we’re only dividing by 2.",17.0,Hard
928,Sp,23,Final,,Problem 5,Problem 5.2,"What goes in blank (b)? (Hint: The function
np.ones(k) returns an array of length k in
which all elements are 1.)","np.ones(12) / 12
uniform_dist needs to be the same as the uniform
distribution provided in the null hypothesis, \left[\frac{1}{12}, \frac{1}{12}, ...,
\frac{1}{12}\right].
In code, this is an array of length 12 in which each element is equal
to 1 / 12. np.ones(12)
creates an array of length 12 in which each value is 1; for
each value to be 1 / 12, we divide np.ones(12)
by 12.",66.0,Medium
929,Sp,23,Final,,Problem 5,Problem 5.3,"What goes in blank (c)?

 np.random.multinomial(12, uniform_dist)
 np.random.multinomial(12, uniform_dist) / 12
 np.random.multinomial(12, uniform_dist) / total_count
 np.random.multinomial(total_count, uniform_dist)
 np.random.multinomial(total_count, uniform_dist) / 12
 np.random.multinomial(total_count, uniform_dist) / total_count","np.random.multinomial(total_count, uniform_dist) / total_count
The idea here is to repeatedly generate an array of proportions that
results from distributing total_count hours across the 12
months in a way that each month is equally likely to be chosen. Each
time we generate such an array, we’ll determine its TVD from the uniform
distribution; doing this repeatedly gives us an empirical distribution
of the TVD under the assumption the null hypothesis is true.",21.0,Hard
930,Sp,23,Final,,Problem 5,Problem 5.4,What goes in blank (d)?,"uniform_dist
As mentioned above:

Each time we generate such an array, we’ll determine its TVD from the
uniform distribution; doing this repeatedly gives us an
empirical distribution of the TVD under the assumption the null
hypothesis is true.",54.0,Medium
931,Sp,23,Final,,Problem 5,Problem 5.5,"What goes in blank (e)?

 >
 >=
 <
 <=
 ==
 !=",">=
The purpose of the last line of code is to compute the p-value for
the hypothesis test. Recall, the p-value of a hypothesis test is the
proportion of simulated test statistics that are as or more extreme than
the observed test statistic, under the assumption the null hypothesis is
true. In this context, “as extreme or more extreme” means the simulated
TVD is greater than or equal to the observed TVD (since
larger TVDs mean “more different”).",77.0,Easy
932,Sp,23,Final,,Problem 5,Problem 5.6,What goes in blank (f)?,"observed_counts / total_count
or observed_counts / observed_counts.sum()
Blank (f) needs to contain the observed distribution of sunshine
hours (as an array of proportions) that we compare against the uniform
distribution to calculate the observed TVD. This observed TVD is then
compared with the distribution of simulated TVDs to calculate the
p-value. The observed counts are converted to proportions by dividing by
the total count so that the observed distribution is on the same scale
as the simulated and expected uniform distributions, which are also in
proportions.",27.0,Hard
933,Sp,23,Final,,Problem 6,Problem 6,"Oren’s favorite bakery in San Diego is Wayfarer. After visiting
frequently, he decides to learn how to make croissants and baguettes
himself, and to do so, he books a trip to France.
Oren is interested in estimating the mean number of sunshine hours in
July across all 10,000+ cities in France. Using the 16 French cities in
sun, Oren constructs a 95% Central Limit Theorem
(CLT)-based confidence interval for the mean sunshine hours of all
cities in France. The interval is of the form [L, R], where L and R are
positive numbers.",,,
934,Sp,23,Final,,Problem 6,Problem 6.1,"Which of the following expressions is equal to the standard deviation
of the number of sunshine hours of the 16 French cities in
sun?

 R - L
 \frac{R - L}{2}
 \frac{R - L}{4}
 R + L
 \frac{R + L}{2}
 \frac{R + L}{4}","R - L
Note that the 95% CI is of the form of the following:
[\text{Sample Mean} - 2 \cdot \text{SD of
Distribution of Possible Sample Means}, \text{Sample Mean} + 2 \cdot
\text{SD of Distribution of Possible Sample Means}]
This making its width 4 \cdot \text{SD of
Distribution of Possible Sample Means}. We can use the square
root law, the fact that we can use our sample’s SD as an estimate of our
population’s SD when creating a confidence interval, and the fact that
the sample size is 16, to re-write the width as:

\begin{align*}
\text{width} &= 4 \cdot \text{SD of Distribution of Possible Sample
Means} \\
&= 4 \cdot \left(\frac{\text{Population SD}}{\sqrt{\text{Sample
Size}}}\right) \\
&\approx 4 \cdot \left(\frac{\text{Sample SD}}{\sqrt{\text{Sample
Size}}}\right) \\
&= 4 \cdot \left(\frac{\text{Sample SD}}{4}\right) \\
&= \text{Sample SD}
\end{align*}

Since \text{width} = \text{Sample
SD}, and since \text{width} = R -
L, we have that \text{Sample SD} = R -
L.",27.0,Hard
935,Sp,23,Final,,Problem 6,Problem 6.2,"True or False: There is a 95% chance that the interval [L, R] contains the mean number of sunshine
hours in July of all 16 French cities in sun.

 True
 False","False
[L, R] contains the sample mean for
sure, since it is centered at the sample mean. There is no probability
associated with this fact since neither [L,
R] nor the sample mean are random (given that our sample has
already been drawn).",62.0,Medium
936,Sp,23,Final,,Problem 6,Problem 6.3,"True or False: If we collected 1,000 new samples of 16 French cities
and computed the mean of each sample, then about 95% of the new sample
means would be contained in [L, R].

 True
 False","False
It is true that if we collected many samples and used each one to
make a 95% confidence interval, about 95% of those confidence intervals
would contain the population mean. However, that’s not what this
statement is addressing. Instead, it’s asking whether the one interval
we created in particular, [L,R], would
contain 95% of other samples’ means. In general, there’s no guarantee of
the proportion of means of other samples that would fall in [L, R]; for instance, it’s possible that the
sample that we used to create [L, R]
was not a representative sample.",42.0,Hard
937,Sp,23,Final,,Problem 6,Problem 6.4,"True or False: If we collected 1,000 new samples of 16 French cities
and created a 95% confidence interval using each one, then chose one of
the 1,000 new intervals at random, the chance that the randomly chosen
interval contains the mean sunshine hours in July across all cities in
France is approximately 95%.

 True
 False","True
It is true that if we collected many samples and used each one to
make a 95% confidence interval, about 95% of those confidence intervals
would contain the population mean, as we mentioned above. So, if we
picked one of those confidence intervals at random, there’s an
approximately 95% chance it would contain the population mean.",57.0,Medium
938,Sp,23,Final,,Problem 6,Problem 6.5,"True or False: The interval [L, R]
is centered at the mean number of sunshine hours in July across all
cities in France.

 True
 False","False
It is centered at our sample mean, which is the mean sunshine hours
in July across all cities in France in sun, but not
necessarily at the population mean. We don’t know where the population
mean is!",58.0,Medium
939,Sp,23,Final,,Problem 6,Problem 6.6,"Fill in the blanks so that boot_left and
boot_right evaluate to the left and right endpoints of a
72% confidence interval for the mean sunshine hours in July across all
cities in France.
What goes in blanks (a) and (b)?","(a): 14, (b): 86
A 72% confidence interval is constructed by taking the middle 72% of
the distribution of resampled means. This means we need to exclude 100\% - 72\% = 28\% of values – the smallest
14% and the largest 14%. Blank (a), then, is 14, and blank (b) is 100 - 14 = 86.",81.0,Easy
940,Sp,23,Final,,Problem 6,Problem 6.7,"Suppose that when Oren uses [boot_left, boot_right], his
72% bootstrap-based confidence interval, he fails to reject the null
hypothesis above. If that’s the case, then when using [L, R], his 95% CLT-based confidence
interval, what is the conclusion of his hypothesis test?

 Reject the null
 Fail to reject the null
 Impossible to tell","Impossible to tell
First, remember that we fail to reject the null whenever the
parameter stated in the null hypothesis (225 in this case) is in the
interval. So we’re told 225 is in the 72% bootstrapped interval. There’s
a possibility that the 72% bootstrapped confidence interval isn’t
completely contained within the 95% CLT interval, since the specific
interval we get back with bootstrapping depends on the random resamples
we get. What that means is that it’s possible for 225 to be in the 72%
bootstrapped interval but not the 95% CLT interval, and it’s also
possible for it to be in the 95% CLT interval. Therefore, given no other
information it’s impossible to tell.",47.0,Hard
941,Sp,23,Final,,Problem 6,Problem 6.8,"Suppose that Oren also creates a 72% CLT-based confidence interval
for the mean sunshine hours of all cities in France in July using the
same 16 French cities in sun he started with. When using
his 72% CLT-based confidence interval, he fails to reject the null
hypothesis above. If that’s the case, then when using [L, R], what is the conclusion of his
hypothesis test?

 Reject the null
 Fail to reject the null
 Impossible to tell","Fail to reject the null
If 225 is in the 72% CLT interval, it must be in the 95% CLT
interval, since the two intervals are centered at the same location and
the 95% interval is just wider than the 72% interval. The main
difference between this part and the previous one is the fact that this
72% interval was made with the CLT, not via bootstrapping, even though
they’re likely to be similar.",72.0,Medium
942,Sp,23,Final,,Problem 6,Problem 6.9,"True or False: The significance levels of both hypothesis tests
described in part (h) are equal.

 True
 False","False
When using a 72% confidence interval, the significance level,
i.e. p-value cutoff, is 28%. When using a 95% confidence interval, the
significance level is 5%.",62.0,Medium
943,Sp,23,Final,,Problem 7,Problem 7,"Gabriel is originally from Texas and is trying to convince his
friends that Texas has better weather than California. Sophia, who is
originally from San Diego, is determined to prove Gabriel wrong.
Coincidentally, both are born in February, so they decide to look at
the mean number of sunshine hours of all cities in California and Texas
in February. They find that the mean number of sunshine hours for
California cities in February is 275, while the mean number of sunshine
hours for Texas cities in February is 250. They decide to test the
following hypotheses:

Null Hypothesis: The distribution of sunshine
hours in February for cities in California and Texas are drawn from the
same population distribution.
Alternative Hypothesis: The distribution of
sunshine hours in February for cities in California and Texas are not
drawn from the same population distribution; rather, California cities
see more sunshine hours in February on average than Texas
cities.

The test statistic they decide to use is:
\text{mean sunshine hours in California
cities – mean sunshine hours in Texas cities}
To simulate data under the null, Sophia proposes the following
plan:

Count the number of Texas cities, and call that number
t. Count the total number of cities in both California and
Texas, and call that number n.
Find the total number of sunshine hours across all California and
Texas cities in February, and call that number
total.
Take a random sample of t sunshine hours from the
entire sequence of California and Texas sunshine hours in February in
the dataset. Call this random sample t_samp.
Find the difference between the mean of the values that are not
in t_samp (the California sample) and the mean of the
values that are in t_samp (the Texas sample).",,,
944,Sp,23,Final,,Problem 7,Problem 7.1,"What type of test is this?

 Hypothesis test
 Permutation test","Permutation test
Any time we want to decide whether two samples look like they were
drawn from the same population distribution, we are conducting a
permutation test. In this case, the two samples are (1) the sample of
California sunshine hours in February and (2) the sample of Texas
sunshine hours in February.
Even though Gabriel and Sophia aren’t “shuffling” the way we normally
do when conducting a permutation test, they’re still performing a
permutation test. They’re combining the sunshine hours from both states
into a single dataset and then randomly reallocating them into two new
groups, one representing California and the other Texas, without regard
to their original labels.",52.0,Medium
945,Sp,23,Final,,Problem 7,Problem 7.2,"What goes in blank (b)?

 replace=True
 replace=False","replace=False
In order for there to be no overlap between the elements in the Texas
sample and California sample, the Texas sample needs to be taken out of
the total collection of sunshine hours without replacement.",30.0,Hard
946,Sp,23,Final,,Problem 7,Problem 7.3,"What goes in blank (c)? (Hint: Our solution uses 4 of the
variables that are defined before c_mean.)","(total - t_samp.sum) / (n - t)
For the c_mean calculation, which represents the mean
sunshine hours for the California cities in the simulation, we need to
subtract the total sunshine hours of the Texas sample
(t_samp.sum()) from the total sunshine hours of both states
(total). This gives us the sum of the California sunshine
hours in the simulation. We then divide this sum by the number of
California cities, which is the total number of cities (n)
minus the number of Texas cities (t), to get the mean
sunshine hours for California cities.",21.0,Hard
947,Sp,23,Final,,Problem 7,Problem 7.4,What goes in blank (d)? Your answer should be a specific number.,"0
To conduct a hypothesis test using a confidence interval, our null
hypothesis must be of the form “the population parameter is equal to
x”; the test is conducted by checking
whether x is in the specified
interval.
Here, Sophia and Gabriel want to test whether the mean number of
sunshine hours in February for the two states is equal; since the
confidence interval they created was for the difference in mean sunshine
hours, they really want to check whether the difference in mean sunshine
hours is 0. (They created a confidence interval for the true value of
a - b, and want to test whether a = b; this is the same as testing whether
a - b = 0.)",43.0,Hard
948,Sp,23,Final,,Problem 8,Problem 8,"Australia is in the southern hemisphere, meaning that its summer
season is from December through February, when we have our winter. As a
result, January is typically one of the sunniest months in
Australia!
Arjun is a big fan of the movie Kangaroo Jack and plans on visiting
Australia this January. In doing his research on where to go, he found
the number of sunshine hours in January for the 15 Australian cities in
sun and sorted them in descending
order.
356, 337, 325, 306, 294, 285, 285, 266,
263, 257, 255, 254, 220, 210, 176
Throughout this question, use the mathematical definition of
percentiles presented in class.
Note: Parts 1, 2, and 3 of this problem
are out of scope; they cover material no longer included in the course.
Part 4 is in scope!",,,
949,Sp,23,Final,,Problem 8,Problem 8.1,"What is the 80th percentile of the collection of numbers above?

 254
 255
 294
 306
 325
 337","306
First, we need to find the position of the 80th percentile using the
rule from class:
h = \left(\frac{80}{100}\right) \cdot 15 =
\frac{4}{5} \cdot 15 = 12
Since 12 is an integer, we don’t need to round up, so k = 12. Starting from the right-most number,
which is the smallest number and hence position 1 here, the 12th number
is 306.",52.0,Medium
950,Sp,23,Final,,Problem 8,Problem 8.2,"What is the largest positive integer p such that 257 is the pth percentile of the collection of numbers
above?","40
The first step is to find the position of 257 in the collection when
we start at position 1, which is 6. Since there are 15 values total,
this means that 257 is the smallest value that is greater than or equal
to 40% of the values.
If we set p to be any number larger
than 40, say, 41, then 257 won’t be larger than p\% of the values in the collection; thus,
the largest positive integer value of p
that makes 257 the pth percentile is
40.",30.0,Hard
951,Sp,23,Final,,Problem 8,Problem 8.3,"What is the smallest positive integer p such that 257 is the pth percentile of the collection of numbers
above? (Make sure your answer to (c) is smaller than your answer to
(b)!)","34
Let’s look at the next number down from 257, which is 255. 255 is the
5th number out of 15, so it is the smallest number that is greater than
or equal to 33.333% of the values. This means the 33rd percentile is
also 255, since 33.333 > 33. However, 255 is not greater than or
equal to 34% of the values, which makes the 34th percentile 257.
Therefore, 34 is the smallest integer value of p such that the pth percentile is 257.",21.0,Hard
952,Sp,23,Final,,Problem 8,Problem 8.4,"According to Chebyshev’s inequality, at least what percentage of
Australian cities in sun see between 200 and 300 sunshine
hours in February?

 9%
 30%
 33.3%
 91%
 95%
 99.73%","91%
First, we need to find the number of standard deviations above the
mean 300 is, and the number of standard deviations below the mean 200
is.
z = \frac{300 - 250}{15} = \frac{50}{15} =
\frac{10}{3}
The above equation tells us that 300 is \frac{10}{3} standard deviations above the
mean; you can verify that 200 is the same number of standard deviations
below the mean. Chebyshev’s inequality tells us the proportion of values
within z SDs of the mean is at least
1 - \frac{1}{z^2}, which here is:
1 - \frac{1}{\left(\frac{10}{3}\right)^2}
= 1 - \frac{9}{100} = 0.91",43.0,Hard
953,Sp,23,Final,,Problem 9,Problem 9,"Suhani’s passport is currently being renewed, and so she can’t join
those on international summer vacations. However, her last final exam is
today, and so she decides to road trip through California this week
while everyone else takes their finals.
The chances that it is sunny this Monday and Tuesday, in various
cities in California, are given below. The event that it is sunny on
Tuesday in Los Angeles depends on the event that it is sunny on Monday
in Los Angeles, but other than that, all other events in the table are
independent of one another.",,,
954,Sp,23,Final,,Problem 9,Problem 9.1,"What is the probability that it is not sunny in San Diego on Monday
and not sunny in San Diego on Tuesday? Give your answer as a
positive integer percentage between 0% and 100%.","18%

The probability it is not sunny in San Diego on Monday is 1 - \frac{7}{10} = \frac{3}{10}.
The probability it is not sunny in San Diego on Tuesday is 1 - \frac{2}{5} = \frac{3}{5}.

Since we’re told these events are independent, the probability of
both occurring is
\frac{3}{10} \cdot \frac{3}{5} =
\frac{9}{50} = \frac{18}{100} = 18\%",80.0,Easy
955,Sp,23,Final,,Problem 9,Problem 9.2,"What is the probability that it is sunny in at least one of the three
cities on Monday?

 3\%
 31.5\%
 40\%
 68.5\%
 75\%
 97\%","97\%
The event that it is sunny in at least one of the three cities on
Monday is the complement of the event that it is not sunny in all three
cities on Monday. The probability it is not sunny in all three cities on
Monday is
\big(1 - \frac{7}{10}\big) \cdot \big(1 -
\frac{3}{5}\big) \cdot \big(1 - \frac{3}{4}\big) = \frac{3}{10} \cdot
\frac{2}{5} \cdot \frac{1}{4} = \frac{6}{200} = \frac{3}{100} =
0.03

So, the probability that it is sunny in at least one of the three
cities on Monday is 1 - 0.03 = 0.97 =
97\%.",65.0,Medium
956,Sp,23,Final,,Problem 9,Problem 9.3,"What is the probability that it is sunny in Los Angeles on
Tuesday?

 15\%
 22.5\%
 40\%
 45\%
 60\%
 88.8\%","60\%
The event that it is sunny in Los Angeles on Tuesday can happen in
two ways:

Case 1: It is sunny in Los Angeles on Tuesday and on
Monday.
Case 2: It is sunny in Los Angeles on Tuesday but not on
Monday.

We need to consider these cases separately given the conditions in
the table. The probability of the first case is \begin{align*} P(\text{sunny Monday and sunny
Tuesday}) &= P(\text{sunny Monday}) \cdot P(\text{sunny Tuesday
given sunny Monday}) \\ &= \frac{3}{5} \cdot \frac{3}{4} \\ &=
\frac{9}{20} \end{align*}
The probability of the second case is \begin{align*} P(\text{not sunny Monday and sunny
Tuesday}) &= P(\text{not sunny Monday}) \cdot P(\text{sunny Tuesday
given not sunny Monday}) \\ &= \frac{2}{5} \cdot \frac{3}{8} \\
&= \frac{3}{20} \end{align*}
Since Case 1 and Case 2 are mutually exclusive — that is, they can’t
both occur at the same time — the probability of either one occurring is
\frac{9}{20} + \frac{3}{20} = \frac{12}{20} =
60\%.",64.0,Medium
957,Sp,23,Final,,Problem 9,Problem 9.4,"Fill in the blanks so that exactly_two evaluates to the
probability that exactly two of San Diego, Los Angeles, and San
Francisco are sunny on Monday.
Hint: If arr is an array, then
np.prod(arr) is the product of the elements in
arr.
    monday = np.array([7 / 10, 3 / 5, 3 / 4]) # Taken from the table.
    exactly_two = __(a)__
    for i in np.arange(3):
        exactly_two = exactly_two + np.prod(monday) * __(b)__
What goes in blank (a)?
What goes in blank (b)?

 monday[i]
 1 - monday[i]
 1 / monday[i]
 monday[i] / (1 - monday[i])
 (1 - monday[i]) / monday[i]
 1 / (1 - monday[i])","(a): 0, (b):
(1 - monday[i]) / monday[i]
What goes in blank (a)? 0
In the for-loop we add the probabilities of the three
different cases, so exactly_two needs to start from 0.",47.0,Hard
958,Sp,23,Final,,Problem 10,Problem 10,"Costin, a San Francisco native, will be back in San Francisco over
the summer, and is curious as to whether it is true that about \frac{3}{4} of days in San Francisco are
sunny.
Fast forward to the end of September: Costin counted that of the 30
days in September, 27 were sunny in San Francisco. To test his theory,
Costin came up with two pairs of hypotheses.
Pair 1:

Null Hypothesis: The probability that it is
sunny on any given day in September in San Francisco is \frac{3}{4}, independent of all other
days.
Alternative Hypothesis: The probability that it
is sunny on any given day in September in San Francisco is
not \frac{3}{4}.

Pair 2:

Null Hypothesis: The probability that it is
sunny on any given day in September in San Francisco is \frac{3}{4}, independent of all other
days.
Alternative Hypothesis: The probability that it
is sunny on any given day in September in San Francisco is
greater than \frac{3}{4}.

For each test statistic below, choose whether the test statistic
could be used to test Pair 1, Pair 2, both, or neither. Assume that all
days are either sunny or cloudy, and that we cannot perform two-tailed
hypothesis tests. (If you don’t know what those are, you don’t need
to!)",,,
959,Sp,23,Final,,Problem 10,Problem 10.1,"The difference between the number of sunny days and number of cloudy
days

 Pair 1
 Pair 2
 Both
 Neither","Pair 2
The test statistic provided is the difference between the number of
sunny days and cloudy days in a sample of 30 days. Since each day is
either sunny or cloudy, the number of cloudy days is just 30 - the
number of sunny days. This means we can re-write our test statistic as
follows:
\begin{align*} &\text{number of sunny
days} - \text{number of cloudy days} \\ &= \text{number of sunny
days} - (30 - \text{number of sunny days}) \\ &= 2 \cdot
\text{number of sunny days} - 30 \\ &=
2 \cdot (\text{number of sunny days} - 15) \end{align*}
The more sunny days there are in our sample of 30 days, the larger
this test statistic will be. (Specifically, if there are more sunny days
than cloudy days, this will be positive; if there’s an equal number of
sunny and cloudy days, this will be 0, and if there are more cloudy
days, this will be negative.)
Now, let’s look at each pair of hypotheses.
Pair 1:

Pair 1’s alternative hypothesis is that the probability of a sunny
day is not \frac{3}{4}, which includes
both greater than and less than \frac{3}{4}.
To test this pair of hypotheses, we need a test statistic that is
large when the number of sunny days is far from \frac{3}{4} (evidence for the alternative
hypothesis) and small when the number of sunny days is
close to \frac{3}{4} (evidence for the
null hypothesis). (It would also be acceptable to design a test
statistic that is small when the number of sunny days is far from \frac{3}{4} and large when it’s close to
\frac{3}{4}, but the first option we’ve
outlined is a bit more natural.)
Our chosen test statistic, 2 \cdot
(\text{number of sunny days} - 15), doesn’t work this way; both
very large values and very small values indicate that the proportion of
sunny days is far from \frac{3}{4}, and
since we can’t use two-tailed tests, we can’t use our test statistic for
this pair.
Pair 2:

Pair 2’s alternative hypothesis is that the probability of a sunny
day greater than \frac{3}{4}.
Since our test statistic is large when the number of sunny days is
large (evidence for the alternative hypothesis) and is small when the
number of sunny days is small (evidence for the null hypothesis), we can
use our test statistic to test this pair of hypotheses. The key
difference between Pair 1 and Pair 2 is that Pair 2’s alternative
hypothesis has a direction – it says that the probability that it is
sunny on any given day is greater than \frac{3}{4}, rather than just “not” \frac{3}{4}.
Thus, we can use this test statistic to test Pair 2, but not Pair
1.",28.0,Hard
960,Sp,23,Final,,Problem 10,Problem 10.2,"The absolute difference between the number of sunny days and number
of cloudy days

 Pair 1
 Pair 2
 Both
 Neither","Neither
The test statistic here is the absolute value of the test statistic
in the first part. Since we were able to re-write the test statistic in
the first part as 2 \cdot (\text{number of
sunny days} - 15), our test statistic here is |2 \cdot (\text{number of sunny days} - 15)|,
or, since 2 already non-negative,
2 \cdot | \text{number of sunny days} - 15
|
This test statistic is large when the number of sunny days is far
from 15, i.e. when the number of sunny days and number of cloudy days
are far apart, or when the proportion of sunny days is far from \frac{1}{2}. However, the null hypothesis
we’re testing here is not that the proportion of sunny days is \frac{1}{2}, but that the proportion of sunny
days is \frac{3}{4}.
A large value of this test statistic will tell us the proportion of
sunny days is far from \frac{1}{2}, but
it may or may not be far from \frac{3}{4}. For instance, when \text{number of sunny days} = 7, then our
test statistic is 2 \cdot | 7 - 15 | =
16. When \text{number of sunny days} =
23, our test statistic is also 16. However, in the first case,
the proportion of sunny days is just under \frac{1}{4} (far from \frac{3}{4}), while in the second case the
proportion of sunny days is just above \frac{3}{4}.
In both pairs of hypotheses, this test statistic isn’t set up such
that large values point to one hypothesis and small values point to the
other, so it can’t be used to test either pair.",25.0,Hard
961,Sp,23,Final,,Problem 10,Problem 10.3,"The difference between the proportion of sunny days and \frac{1}{4}

 Pair 1
 Pair 2
 Both
 Neither","Pair 2
The test statistic here is the difference between the proportion of
sunny days and \frac{1}{4}. This means
if p is the proportion of sunny days,
the test statistic is p - \frac{1}{4}.
This test statistic is large when the proportion of sunny days
is large and small when the proportion of sunny days is small.
(The fact that we’re subtracting by \frac{1}{4} doesn’t change this pattern – all
it does is shift both the empirical distribution of the test statistic
and our observed statistic \frac{1}{4}
of a unit to the left on the x-axis.)
As such, this test statistic behaves the same as the test statistic
from the first part – both test statistics are large when the number of
sunny days is large (evidence for the alternative hypothesis) and small
when the number of sunny days is small (evidence for the null
hypothesis). This means that, like in the first part, we can use this
test statistic to test Pair 2, but not Pair 1.",24.0,Hard
962,Sp,23,Final,,Problem 10,Problem 10.4,"The absolute difference between the proportion of cloudy days and
\frac{1}{4}

 Pair 1
 Pair 2
 Both
 Neither","Pair 1
The test statistic here is the absolute difference between the
proportion of cloudy days and \frac{1}{4}. Let q be the proportion of cloudy days. The test
statistic is |q - \frac{1}{4}|. The
null hypothesis for both pairs states that the probability of a sunny
day is \frac{3}{4}, which implies the
probability of a cloudy day is \frac{1}{4} (since all days are either sunny
or cloudy).
This test statistic is large when the proportion of cloudy days is
far from \frac{1}{4} and small when the
proportion of cloudy days is close to \frac{1}{4}.
Since Pair 1’s alternative hypothesis is just that
the proportion of cloudy days is not \frac{1}{4}, we can use this test statistic
to test it! Large values of this test statistic point to the alternative
hypothesis and small values point to the null.
On the other hand, Pair 2’s alternative hypothesis
is that the proportion of sunny days is greater than \frac{3}{4}, which is the same as the
proportion of cloudy days being less than \frac{1}{4}. The issue here is that our test
statistic doesn’t involve a direction – a large value implies that the
proportion of cloudy days is far from \frac{1}{4}, but we don’t know if that means
that there were fewer cloudy days than \frac{1}{4} (evidence for Pair 2’s
alternative hypothesis) or more cloudy days than \frac{1}{4} (evidence for Pair 2’s null
hypothesis). Since, for Pair 2, this test statistic isn’t set up such
that large values point to one hypothesis and small values point to the
other, we can’t use this test statistic to test Pair 2.
Therefore, we can use this test statistic to test Pair 1, but not
Pair 2.
Aside: This test statistic is equivalent to
the absolute difference between the proportion of sunny days and \frac{3}{4}. Try and prove this fact!",46.0,Hard
963,Sp,23,Final,,Problem 11,Problem 11,"Raine is helping settle a debate between two friends on the
“superior"" season — winter or summer. In doing so, they try to
understand the relationship between the number of sunshine hours per
month in January and the number of sunshine hours per month in July
across all cities in California in sun.
Raine finds the regression line that predicts the number of sunshine
hours in July (y) for a city given its
number of sunshine hours in January (x). In doing so, they find that the
correlation between the two variables is \frac{2}{5}.",,,
964,Sp,23,Final,,Problem 11,Problem 11.1,"Which of these could be a scatter plot of number of sunshine hours in
July vs. number of sunshine hours in January?


 Option 1
 Option 2
 Option 3
 Option 4","Option 1
Since r = \frac{2}{5}, the correct
option must be a scatter plot with a mild positive (up and to the right)
linear association. Option 3 can be ruled out immediately, since the
linear association in it is negative (down and to the right). Option 2’s
linear association is too strong for r =
\frac{2}{5}, and Option 4’s linear association is too weak for
r = \frac{2}{5}, which leaves Option
1.",57.0,Medium
965,Sp,23,Final,,Problem 11,Problem 11.2,"Suppose the standard deviation of the number of sunshine hours in
January for cities in California is equal to the standard deviation of
the number of sunshine hours in July for cities in California.
Raine’s hometown of Santa Clarita saw 60 more sunshine hours in
January than the average California city did. How many more
sunshine hours than average does the regression line predict
that Santa Clarita will have in July? Give your answer as a positive
integer. (Hint: You’ll need to use the fact that the correlation
between the two variables is \frac{2}{5}.)","24
At a high level, we’ll start with the formula for the regression line
in standard units, and re-write it in a form that will allow us to use
the information provided to us in the question.
Recall, the regression line in standard units is
\text{predicted }y_{\text{(su)}} = r \cdot
x_{\text{(su)}}
Using the definitions of \text{predicted
}y_{\text{(su)}} and x_{\text{(su)}} gives us
\frac{\text{predicted } y - \text{mean of
}y}{\text{SD of }y} = r \cdot \frac{x - \text{mean of }x}{\text{SD of
}x}
Here, the x variable is sunshine
hours in January and the y variable is
sunshine hours in July. Given that the standard deviation of January and
July sunshine hours are equal, we can simplifies our formula to
\text{predicted } y - \text{mean of }y = r
\cdot (x - \text{mean of }x)
Since we’re asked how much more sunshine Santa Clarita will have in
July compared to the average, we’re interested in the difference y - \text{mean of} y. We were given that
Santa Clarita had 60 more sunshine hours in January than the average,
and that the correlation between the two variables(correlation
coefficient) is \frac{2}{5}. In terms
of the variables above, then, we know:

x - \text{mean of }x =
60.
r = \frac{2}{5}.

Then,
\text{predicted } y - \text{mean of }y = r
\cdot (x - \text{mean of }x) = \frac{2}{5} \cdot 60 = 24
Therefore, the regression line predicts that Santa Clarita will have
24 more sunshine hours than the average California city in July.",68.0,Medium
966,Sp,23,Final,,Problem 11,Problem 11.3,What is the slope of Anthony’s new regression line?,"\frac{2}{5}
To determine the slope of Anthony’s new regression line, we need to
understand how the modifications he made to the dataset (subtracting 5
hours from each x and y value) affect the slope. In simple linear
regression, the slope of the regression line (m in y = mx +
b) is calculated using the formula:
m = r \cdot \frac{\text{SD of y}}{\text{SD
of x}}
r, the correlation coefficient
between the two variables, remains unchanged in Anthony’s modifications.
Remember, the correlation coefficient is the mean of the product of the
x values and y values when both are measured in standard
units; by subtracting the same constant amount from each x value, we aren’t changing what the x values convert to in standard units. If
you’re not convinced, convert the following two arrays in Python to
standard units; you’ll see that the results are the same.

x1 = np.array([5, 8, 4, 2, 9])
x2 = x1 - 5

Furthermore, Anthony’s modifications also don’t change the standard
deviations of the x values or y values, since the xs and ys
aren’t any more or less spread out after being shifted “down” by 5. So,
since r, \text{SD of }y, and \text{SD of }x are all unchanged, the slope
of the new regression line is the same as the slope of the old
regression line, pre-modification!
Given the fact that the correlation coefficient is \frac{2}{5} and the standard deviation of
sunshine hours in January (\text{SD of
}x) is equal to the standard deviation of sunshine hours in July
(\text{SD of }y), we have
m = r \cdot \frac{\text{SD of }y}{\text{SD
of }x} = \frac{2}{5} \cdot 1 = \frac{2}{5}",73.0,Medium
967,Sp,23,Final,,Problem 11,Problem 11.4,"Suppose the intercept of Raine’s original regression line – that is,
before Anthony subtracted 5 from each x
and each y – was 10. What is the
intercept of Anthony’s new regression line?

 -7
 -5
 -3
 0
 3
 5
 7","7
Let’s denote the original intercept as b and the new intercept in the new dataset as
b'. The equation for the original
regression line is y = mx + b,
where:

y is a predicted number of sunshine
hours in July, before 5 was subtracted from each number of hours.
m is the slope of the line, which
we know is \frac{2}{5} from the
previous part.
x is a number of sunshine hours in
January, before 5 was subtracted from each number of hours.
b is the original intercept. This
is 10.

When Anthony subtracts 5 from each x
and y value, the new regression line
becomes y - 5 = m \cdot (x - 5) +
b'
Expanding and rearrange this equation, we have
y = mx - 5m + 5 + b'
Remember, x and y here represent the number of sunshine hours
in January and July, respectively, before Anthony subtracted 5
from each number of hours. This means that the equation for y above is equivalent to y = mx + b. Comparing, we see that
-5m + 5 + b' = b
Since m = \frac{2}{5} (from the
previous part) and b = 10, we have
-5 \cdot \frac{2}{5} + 5 + b' = 10
\implies b' = 10 - 5 + 2 = 7
Therefore, the intercept of Anthony’s new regression line is 7.",34.0,Hard
968,Sp,23,Final,,Problem 11,Problem 11.5,"What would you expect to see in a residual plot of Jasmine’s
regression line?

 A patternless cloud of points
 A distinctive pattern in the residual plot
 Heteroscedasticity (residuals that are not evenly vertically
spread)","A distinctive pattern in the residual
plot
We’re told in the problem that the number of sunshine hours per month
in Chicago increases from the winter (January) to the summer
(July/August) and then decreases again to the winter (December). Here’s
a real plot of this data; we don’t need real data to answer this
question, but this is the kind of plot you could sketch out in the exam
given the description in the question. (The gold shaded area is
irrelevant for our purposes.)



The points in this plot aren’t tightly clustered around a straight
line, and that’s because there’s a non-linear relationship between month
and number of sunshine hours. As such, when we draw a straight line
through this scatter plot, it won’t be able to fully capture the
relationship being shown. It’ll likely start off in the bottom left and
increase to the top right, which will lead to the sunshine hours for
summer months being underestimated and the sunshine hours for later
winter months (November, December) being overestimated. This will lead
to a distinctive pattern in our residual plot, which means that linear
regression as-is isn’t the right tool for modeling this data (because
ideally, the residual plot would be a patternless cloud of points).",47.0,Hard
969,Sp,24,Final,,Problem 1,Problem 1,"Yutian wants to rent a one-bedroom apartment, so she decides to learn
about the housing market by plotting a density histogram of the monthly
rent for all one-bedroom apartments in the apts DataFrame.
In her call to the DataFrame method .plot, she sets the
bins using the parameter

 bins = np.arange(0, 10000, 100)",,,
970,Sp,24,Final,,Problem 1,Problem 1.1,How many bins will this histogram have?,"99
np.arange(start, stop, step) takes the following three
parameters as arguments.

start: The starting value of the sequence
(inclusive).
stop: The last value of the sequence (exclusive).
step: The difference between each two consecutive
values.

This means that np.arange(0, 10000, 100) will create a
NumPy array that starts at 0, and ends before it reaches 10000 - all
while incrementing by 100 for each step. To calculate the number of bins
within the parameter, we can write \frac{\text{stop} - \text{start}}{\text{step}} -
1.
Another way we can look at this is by taking a small sample of this
sequence (such as np.arange(0, 300, 100)). This will create
the array np.array([0, 100, 200]) without including the
stop argument (300). Note that the same equation holds true.
Note: Mathematically,
np.arange(start, stop) can be represented as [\text{start}, \text{stop})",64.0,Medium
971,Sp,24,Final,,Problem 1,Problem 1.2,"Suppose there are 300 one-bedroom apartments in the apts
DataFrame, and 15 of them cost between $2,300 (inclusive) and $2,400
(exclusive). How tall should the bar for the bin [2300, 2400) be in the density histogram?
Give your answer as a simplified fraction or exact decimal.","0.0005 =
\frac{1}{2000}
Before we start, we need to take note that the question is asking for
the density of the bin, since we are representing the
data in a density histogram. In order to calculate the density of the
bin we use the following equation:
\frac{\text{Number of points in the
bin}}{\text{Total number of points} \cdot \text{Width of
bin}}
To solve, we plug in the following values into the equation:

Number of points in the bin: 15
Total number of points: 300
Width of bin: 2400 - 2300 =
100

\frac{15}{300 \cdot 100} = \frac{1}{20
\cdot 100} = \frac{1}{2000}
Therefore, the density of this bin is \frac{1}{2000}",51.0,Medium
972,Sp,24,Final,,Problem 1,Problem 1.3,"Suppose some of the one-bedroom apartments in the apts
DataFrame cost more than $5,000. Next, Yutian plots another density
histogram with

 bins = np.arange(0, 5000, 100)


Consider the bin [2300, 2400) in
this new histogram. Is it taller, shorter, or the same height as in the
old histogram, where the bins were
np.arange(0, 10000, 100)?

 Taller
 Shorter
 The same height
 Not enough information to answer","Taller
In this histogram, we will only have data that that fits within the
constraints of [0, 5000). Since we are
told that there are apartments that fit outside of the constraint, there
will be an overall smaller number of points points represented by the
histogram.
Taking the histogram density estimation equation, \frac{\text{Number of points in the
bin}}{\text{Total number of points} \cdot \text{Width of bin}},
we know that our total number of points have decreased (with respect to
the constraints shown in the bins). So, a smaller denominator would lead
to a proportional increase in the resulting product. Because the
resulting product increases, this means that the height of this
particular bin will be taller.",55.0,Medium
973,Sp,24,Final,,Problem 2,Problem 2,"Michelle and Abel are each touring apartments for where they might
live next year. Michelle wants to be close to UCSD so she can attend
classes easily. Abel is graduating and wants to live close to the beach
so he can surf. Each person makes their own DataFrame (called
michelle and abel respectively), to keep track
of all the apartments that they toured. Both michelle and
abel came from querying apts, so both
DataFrames have the same columns and structure as apts.
Here are some details about the apartments they toured.

Michelle toured one bedroom and studio apartments
at 12 different complexes, or 24 apartments total.
Abel toured one bedroom and two bedroom apartments
at 20 different complexes, or 40 apartments total.
There are 8 complexes that are near both UCSD and the beach, and
both Michelle and Abel toured these complexes.

We’ll assume for this problem only that there is just one apartment
of each size available at each complex, so that if they both tour a one
bedroom apartment at the same complex, it is the exact same apartment
with the same ""Apartment ID"".",,,
974,Sp,24,Final,,Problem 2,Problem 2.1,"What does the following expression evaluate to?

michelle.merge(abel, left_index=True, right_index=True).shape[0]","8
This expression uses the indices of michelle and
abel to merge. Since both use the index of
""Apartment ID"" and we are assuming that there is only one
apartment of each size available at each complex, we only need to see
how many unique apartments michelle and abel
share. Since there are 8 complexes that they both visited, only the one
bedroom apartments in these complexes will be displayed in the resulting
merged DataFrame. Therefore, we will only have 8 apartments, or 8
rows.",48.0,Hard
975,Sp,24,Final,,Problem 2,Problem 2.2,"What does the following expression evaluate to?

 michelle.merge(abel, on=“Bed”).shape[0]","240
This expression merges on the ""Bed"" column, so we need
to look at the data in this column for the two DataFrames. Within this
column, michelle and abel share only one
specific type of value: ""One"". With the details that are
given, michelle has 12 rows containing this value while
abel has 20 rows containing this value. Since we are
merging on this row, each row in abel that contains the
""One"" value will be matched with a row in
michelle that also contains the value, meaning one row in
michelle will turn into twelve after the merge.
Thus, to compute the total number of rows from this merge expression,
we multiply the number of rows in michelle with the number
of rows in abel that fit the cross-criteria of
""Bed"". Numerically, this would be 12 \cdot 20 = 240.",33.0,Hard
976,Sp,24,Final,,Problem 2,Problem 2.3,"What does the following expression evaluate to?


michelle.merge(abel, on=“Complex”).shape[0] ","32
To approach this question, we first need to determine how many
complexes Michelle and Abel have in common: 8. We also know that each
complex was toured twice by both Michelle and Abel, so there are two
copies of each complex in the michelle and
abel DataFrames. Therefore, when we merge the DataFrames,
the two copies of each complex will match with each other, effectively
creating four copies for each complex from the original two. Since this
is done for each complex, we have 8 \cdot (2
\cdot 2) = 32.",19.0,Hard
977,Sp,24,Final,,Problem 2,Problem 2.4,"What does the following expression evaluate to?

 abel.merge(abel, on=“Bed”).shape[0]","800
Since this question deals purely with the abel
DataFrame, we need to fully understand what is inside it. There are 40
apartments (or rows): 20 one bedrooms and 20 two bedrooms. When we
self-merge on the ""Bed"" column, it is imperative to know
that every one bedroom apartment will be matched with the 20 other one
bedroom apartments (including itself)! This also goes for the two
bedroom apartments. Therefore, we have 20
\cdot 20 + 20 \cdot 20 = 800.",28.0,Hard
978,Sp,24,Final,,Problem 3,Problem 3,"We wish to compare the average rent for studio apartments in
different complexes.",,,
979,Sp,24,Final,,Problem 3,Problem 3.1,"Our goal is to create a DataFrame studio_avg where each
complex with studio apartments appears once. The DataFrame should
include a column named ""Rent"" that contains the average
rent for all studio apartments in that complex. For each of the
following strategies, determine if the code provided works as intended,
gives an incorrect answer, or errors.



studio = apts[apts.get(""Bed"") == ""Studio""]
studio_avg = studio.groupby(""Complex"").mean().reset_index()

 Works as intended
 Gives an incorrect answer
 Errors","(i) Works as intended
(ii) Gives an incorrect answer
(iii) Works as intended
(iv) Errors


studio is set to a DataFrame that is queried from the
apts DataFrame so that it contains only rows that have the
""Studio"" value in ""Bed"". Then, with
studio, it groups by the ""Complex"" and
aggregates by the mean. Finally, it resets its index. Since we have a
DataFrame that only has ""Studio""s , grouping by the
""Complex"" will take the mean of every numerical column -
including the rent - in the DataFrame per ""Complex"",
effectively reaching our goal.",96.0,Easy
980,Sp,24,Final,,Problem 3,Problem 3.2,"Consider the DataFrame alternate_approach defined as
follows
grouped = apts.groupby([""Bed"", ""Complex""]).mean().reset_index()
alternate_approach = grouped.groupby(""Complex"").min()
Suppose that the ""Rent"" column of
alternate_approach has all the same values as the
""Rent"" column of studio_avg, where
studio_avg is the DataFrame described in part (a). Which of
the following are valid conclusions about apts? Select all
that apply.

 No complexes have studio apartments.
 Every complex has exactly one studio apartment.
 Every complex has at least one studio apartment.
 Some complexes have only studio apartments.
 In every complex, the single cheapest apartment is a studio
apartment.
 In every complex, the average price of a studio apartment is less
than or equal to the average price of a one bedroom apartment.
 None of these.",Options 3 and 6.,73.0,Medium
981,Sp,24,Final,,Problem 3,Problem 3.3,"Which data visualization should we use to compare the average prices
of studio apartments across complexes?

 Scatter plot
 Line chart
 Bar chart
 Histogram","Bar chart
Each complex is a categorical data type, so we should use a bar chart
to compare average prices.

Scatter plots are between two numerical variables.
Line charts are typically used to depict changes throughout
time.
Histograms are used to depict frequency of distribution.",85.0,Easy
982,Sp,24,Final,,Problem 4,Problem 4,"According to Chebyshev’s inequality, at least 80% of San Diego
apartments have a monthly parking fee that falls between $30 and
$70.",,,
983,Sp,24,Final,,Problem 4,Problem 4.1,What is the average monthly parking fee?,"\$50
We are given that the left and right bounds of Chebyshev’s inequality
are $30 and $70 respectively. Thus, to find the middle of the two, we
compute the following equation (the midpoint equation):
\frac{\text{right} +
\text{left}}{2}
\frac{70 + 30}{2} = 50
Therefore, 50 is the average monthly
parking fee.",92.0,Easy
984,Sp,24,Final,,Problem 4,Problem 4.2,"What is the standard deviation of monthly parking fees?

 \frac{20}{\sqrt{5}}
 \frac{40}{\sqrt{5}}
 20\sqrt{5}
 20\sqrt{5}","\frac{20}{\sqrt{5}}
Chebyshev’s inequality states that at least 1 - \frac{1}{z^2} of values are within z standard deviations of the mean. In
addition, z can be represented as \frac{\text{bound} - \text{mean of x}}{\text{SD of
x}}.
Therefore, we can set up the equation like so: \frac{4}{5} = 1 - \frac{1}{(\frac{\text{bound} -
\text{mean of x}}{\text{SD of x}})^2}
Then, we can solve: \frac{1}{5} =
\frac{1}{(\frac{\text{bound} - \text{mean of x}}{\text{SD of
x}})^2}
Now since we know both bounds, we can plug one of them in. Since the
mean was computed in the earlier step, we also plug this in.
\frac{1}{5} = \frac{1}{(\frac{70 -
50}{\text{SD of x}})^2} 5 =
(\frac{20}{\text{SD of x}})^2 \sqrt{5}
= \frac{20}{\text{SD of x}} \text{SD
of x} = \frac{20}{\sqrt{5}}",70.0,Medium
985,Sp,24,Final,,Problem 5,Problem 5,"You are given the following information about security deposits for a
sample of 400 apartments in the Mission Hills neighborhood of San
Diego:

Mean deposit: $2,300
Standard Deviation of deposits: $500

Using the fact that scipy.stats.norm.cdf(-0.8) evaluates
to about 0.21, construct a 58% confidence interval for the mean security
deposit of all Mission Hills apartments. Below, give the endpoints of
your confidence interval, both as integers.
Left endpoint: ____(a)____  Right endpoint: ____(b)____","(a) 2280
(b) 2320

scipy.stats.norm.cdf(-0.8) tells us that from the bounds
of (-\inf, -0.8], the normal
distribution has an area of 0.21.
Therefore, if we take it to the other side from [0.8, \inf), it also has an area of 0.21 due to the symmetrical property of the
normal distribution. This means that the interval between [-0.8, 0.8] has an area of 1 - 0.21 - 0.21 = 0.58: the confidence
interval we are aiming to find.
In the question, we are given the standard deviation of security
deposits in a sample, meaning we need to find the
standard deviation for the population. To find this, we
use the following formula and compute:
\frac{\text{SD of
sample}}{\sqrt{\text{sample size}}} = \frac{500}{\sqrt{400}} =
\frac{500}{20} = 25.
Now that we have the population standard deviation, we can calculate
the endpoints of the confidence interval.
Left endpoint: 2300 - \frac{4}{5} \cdot 25
= 2320
Right endpoint: 2300 + \frac{4}{5} \cdot
25 = 2280",29.0,Hard
986,Sp,24,Final,,Problem 6,Problem 6,"You want to use the data in apts to test both of the
following pairs of hypotheses:
Pair 1:

Null Hypothesis: In San Diego, the number of one
bedroom apartments available for rent is equal to the
number of two bedroom apartments available for rent.
Alternate Hypothesis: In San Diego, the number of
one bedroom apartments available for rent is greater to
the number of two bedroom apartments available for rent.

Pair 2:

Null Hypothesis: In San Diego, the number of one
bedroom apartments available for rent is equal to the
number of two bedroom apartments available for rent.
Alternate Hypothesis: In San Diego, the number of
one bedroom apartments available for rent is not equal
to the number of two bedroom apartments available for rent.

In apts, there are 467 apartments that are either one
bedroom or two bedroom apartments. You perform the following simulation
under the assumption of the null hypothesis.
prop_1bf = np.array([])
abs_diff = np.array([])
for i in np.arange(10000):
    prop = np.random.multinomial(467, [0.5, 0.5])[0]/467
    prop_1br = np.append(prop_1br, prop)
    abs_diff = np.append(abs_diff, np.abs(prop-0.5))
You then calculate some percentiles of prop_1br. The
following four expressions all evaluate to True.
np.percentiles(prop_1br, 2.5) == 0.4
np.percentiles(prop_1br, 5) == 0.42
np.percentiles(prop_1br, 95) == 0.58
np.percentiles(prop_1br, 97.5) == 0.6",,,
987,Sp,24,Final,,Problem 6,Problem 6.1,What is prop_1br.mean() to two decimal places?,"0.5
From the given percentiles, we can notice that since the distribution
is symmetric around the mean, the mean should be around the 50th
percentile. Given the symmetry and the percentiles around 0.5, we can
infer that the mean should be very close to 0.5.
Another way we can look at it is by noticing that prop
is pulled from a [0.5, 0.5]
distribution (because we are simulating under the null hypotheses) in
np.random.multinomial(). This means that its expected for
most of the distribution to be from around 0.5.",84.0,Easy
988,Sp,24,Final,,Problem 6,Problem 6.2,What is np.std(prop_1br) to two decimal places?,"0.05
If we look again at the percentiles, we notice that it seems to
resemble a normal distribution. So by taking the mean and the 97.5th
percentile, we can solve for the standard deviation. Since [2.5, 97.5] is the 95% confidence interval,
we can say that the 97.5th percentile is two standard deviations away
from the mean (2.5 too!). Thus,
0.5 + 2 \cdot \text{SD} = 0.6
\therefore Solving for SD, we get
\text{SD} = 0.05",45.0,Hard
989,Sp,24,Final,,Problem 6,Problem 6.3,"What is np.percentile(abs_diff, 95) to two decimal
places?",0.1,10.0,Hard
990,Sp,24,Final,,Problem 6,Problem 6.4,"Which simulated test statistics should be used to test the first pair
of hypotheses?

 prop_1br
 abs_diff","prop_1br
Our first pair of hypotheses’ alternative hypothesis asks if one
number is greater than the other. Because of this, we
can’t use an absolute value test statistic to answer the question, since
all absolute value cares about is the distance the simulation is from
the null assumption, not whether one value is greater than the
other.",82.0,Easy
991,Sp,24,Final,,Problem 6,Problem 6.5,"Which simulated test statistics should be used to test the second
pair of hypotheses?

 prop_1br
 abs_diff","abs_diff
Our first pair of hypotheses’ alternative hypothesis asks if one
number is not equal to the other. Because of this, we
have to use a test statistic that sees the distance both ways, not just
in one direction. Therefore, we use the absolute value.",83.0,Easy
992,Sp,24,Final,,Problem 6,Problem 6.6,"Your observed data in apts is such that you reject the
null for the first pair of hypotheses at the 5% significance level, but
fail to reject the null for the second pair at the 5% significance
level. What could the value of the following proportion have been?
\frac{\text{\# of one bedroom apartments
in \texttt{apts}}}{\text{\# of one bedroom apartments in \texttt{apts}+
\# of two bedroom apartments in \texttt{apts}}}
Give your answer as a number to two decimal places.",0.59,20.0,Hard
993,Sp,24,Final,,Problem 7,Problem 7,"You want to know how much extra it costs, on average, to have a
washer and dryer in your apartment. Since this cost is built into the
monthly rent, it isn’t clear how much of your rent will be going towards
this convenience. You decide to bootstrap the data in apts
to estimate the average monthly cost of having in-unit laundry.",,,
994,Sp,24,Final,,Problem 7,Problem 7.1,"Fill in the blanks to generate 10,000 bootstrapped estimates for the
average montly cost of in-unit laundry.
yes = apts[apts.get(""Laundry"")]
no = apts[apts.get(""Laundry"") == False]
laundry_stats = np.array([])
for i in np.arange(10000):
    yes_resample = yes.sample(__(a)__, __(b)__)
    no_resample = no.sample(__(c)__, __(d)__)
    one_stat = __(e)__
    laundry_stats = np.append(laundry_stats, one_stat)","(a): yes.shape[0]
(b): replace=True
(c): no.shape[0]
(d): replace=True
(e):
yes_resample.get(""Rent"").mean() - no_resample.get(""Rent"").mean()

For both yes_resample and no_resample, we
need to use their respective DataFrames to create a bootstrapped
estimate. Therefore, we randomly sample from their respective DataFrames
with replacement (the law of bootstrap). Then, to calculate the test
statistic, we need to look back at what the question asks of us: to
estimate the average monthly cost of having in-unit
laundry, so we subtract the mean of the bootstrapped estimate
for no (no_resample) from the mean of the
bootstrapped estimate for yes
(yes_resample).",,
995,Sp,24,Final,,Problem 7,Problem 7.2,"What if you wanted to instead estimate the average
yearly cost of having in-unit laundry?

Below, change the blank (e), such that the procedure not
generates 10,000 bootstrapped estimates for the average
yearly cost of in-unit laundry.
Suppose you ran your original code from part (a) and used the
results to calculate a confidence interval for the average
monthly cost of in-unit laundry, which came out to
be

[L_M, R_M].
Then, you changed blank (e) as you described above, and ran the code
again to calculate a different confidence interval for the average
yearly cost of in-unit laundry, which came out to
be
[L_Y, R_Y].
Which of the following is the best description of the relationship
between the endpoints of these confidence intervals? Note that the
symbol \approx means “approximately
equal.”

 L_Y = 12 \cdot L_M and R_Y = 12 \cdot R_M
 L_Y \approx 12 \cdot L_M and R_Y \approx 12 \cdot R_M
 L_M = 12 \cdot L_Y and R_M = 12 \cdot R_Y
 L_M \approx 12 \cdot L_Y and R_M \approx 12 \cdot R_Y
 None of these.","L_Y \approx 12
\cdot L_M and R_Y \approx 12 \cdot
R_M
For both L_Y and R_Y, we cannot say that we certainly know
that it will be precisely 12 times the value of the average monthly
cost. Because every month and year has variablity/noise, we cannot say
for certain that it will most definitely be 12 times the value of
average monthly cost, but instead will probably be approximately
equal.
The bottom two choices flip the inequality and state that the average
monthly cost is 12 times the value of the average yearly cost, which
would be vastly different from one another.",,
996,Sp,24,Final,,Problem 7,Problem 7.3,"You’re concerned about the validity of your estimates because you
think bigger apartments are more likely to have in-unit laundry for one
bedroom apartments only.
If your concern is valid and it is true that bigger apartments are
more likely to have in-unit laundry, how will your bootstrapped
estimates for the average monthly cost of in-unit laundry for one
bedroom apartments only compare to the values you computed in part (a)
based on all the apts?

 The estimates will be about the same.
 The estimates will be generally larger than those you computed in
part (a).
 The estimates will be generally smaller than those you computed in
part (a).","The estimates will be generally smaller than
those you computed in part (a).
If we query the yes and no DataFrames to
contain only one bedroom apartments, the average ""Rent"" of
these two DataFrames will probably be smaller than the original
DataFrames. Because these two DataFrames now have a smaller mean, their
bootstraps are also likely to also be smaller than what it originally
was.
Another way we can think of it is by first calling our original
yes and no DataFrames as
yes_population and no_population respectively.
Now, if we take yes_population and
no_population on a histogram, we’ll likely see higher
magnitude ""Rent"" outliers. By removing these outliers, we
are now in a scenario similar to what the question asks. By taking this
smaller subset that doesn’t have outliers and bootstrap, we will most
likely get a smaller estimate than that seen from
yes_population and no_population
bootstraps.",,
997,Sp,24,Final,,Problem 7,Problem 7.4,"Consider the distribution of laundry_stats as computed
in part (a). How would this distribution change if we:

Increased the number of repetitions to 100,000?


 The distribution would be wider.
 The distribution would be narrower.
 The distribution would not change significantly.


Started with only half of the rows in apts?


 The distribution would be wider.
 The distribution would be narrower.
 The distribution would not change significantly.","The distribution would not change significantly.


The distribution would be wider.



When the number of repetitions are increased, the overall
distribution will end up looking the same. If anything, increasing the
number of repetitions would make the bootstrap distribution look more
like the true population distribution.
If only half of the rows are used, there would be more
variability in the bootstrap, leading to a wider distribution.",,
998,Sp,24,Final,,Problem 8,Problem 8,,,,
999,Sp,24,Final,,Problem 8,Problem 8.1,"The management of the Solazzo apartment complex is changing the
comple’s name to be the output of the following line of code. Write the
new name of this complex as a string.
Note that the string method .capitalize() converts the
first character of a string to uppercase.
(""Solazzo"".replace(""z"", ""ala"" * 2)
          .replace(""aa"")[-1]
          .capitalize()
          .replace(""o"", ""Jo""))","“LaJo”
Let’s trace the steps:
We start with the original string: “Solazzo”.
""Solazzo"".replace(""z"", ""ala"" * 2)  Replace every
instance of “z” with “alaala” since “ala” * 2 = “alaala”:
“Solaalaalaalaalao”
""Solaalaalaalaalao"".split(""aa"")  Split the string by
“aa”: [“Sol”, “l”, “l”, “l”, “lao”]
[""Sol"", ""l"", ""l"", ""l"", ""lao""][-1]  Get the last
element of the list: “lao”
""lao"".capitalize()  Uppercase the first character of
the string: “Lao”
""Lao"".replace(""o"", ""Jo"")  Replace every instance of
“o” with “Jo”: “LaJo”",,
1000,Sp,24,Final,,Problem 8,Problem 8.2,"The management fo the Renaissance apartment complex has decided to
follow suit and rename their complex to be the output of the following
line of code. Write the new name of this complex as a string.
((""Renaissance"".split(""n"")[1] + ""e"") * 2).replace(""a"", ""M"")","“MissMeMissMe”
Let’s trace the steps:
We start with the original string: “Renaissance”.
""Renaissance"".split(""n"")  Split the string by “n”:
[“Re”, “aissa”, “ce”]
[""Re"", ""aissa"", ""ce""][1]  Get the element in the 1st
index of the list (the second element in the list): “aissa”
""aissa"" + e  Add an “e” to the end of the string:
“aissae”
(""aissae"") * 2  Repeat the string twice:
“aissaeaissae”
""aissaeaissae"".replace(""a"", ""M"")  Replace every
instance of “a” with “M”: “MissMeMissMe”",,
1001,Sp,24,Final,,Problem 9,Problem 9,"For each expression below, determine the data type of the output and
the value of the expression, if possible. If there is not enough
information to determine the expression’s value, write “Unknown” in the
corresponding blank.",,,
1002,Sp,24,Final,,Problem 9,Problem 9.1,"apts.get(""Rent"").iloc[43] * 4 / 2
type: ____ value: ____","type: float
value: Unknown

We know that all values in the column Rent are
ints. So, when we call .iloc[43] on this
column (which grabs the 44th entry in the column), we know the result
will be an int. We then perform some multiplication and
division with this value. Importantly, when we divide an
int, the type is automatically changed to a
float, so the type of the final output will be a
float. Since we do not explicitly know what the 44th entry
in the Rent column is, the exact value of this
float is unknown to us.",,
1003,Sp,24,Final,,Problem 9,Problem 9.2,"apts.get(""Neighborhood"").iloc[2][-3]
type: ____ value: ____","type: str
value: “w”

This code takes the third entry (the entry at index 2) from the
Neighborhood column of apts, which is a
str, and it takes the third to last letter of that string.
The third entry in the Neighborhood column is
'Midway', and the third to last letter of
'Midway' is 'w'. So, our result is a
string with value w.",,
1004,Sp,24,Final,,Problem 9,Problem 9.3,"(apts.get(""Laundry"") + 5).max()
type: ____ value: ____","type: int
value: 6

This code deals with the Laundry column of
apts, which is a Series of Trues and
Falses. One property of Trues and
Falses is that they are also interpreted by Python as ones
and zeroes. So, the code (apts.get(""Laundry"") + 5).max()
adds five to each of the ones and zeroes in this column, and then takes
the maximum value from the column, which would be an int of
value 6.",,
1005,Sp,24,Final,,Problem 9,Problem 9.4,"apts.get(""Complex"").str.contains(""Verde"")
type: ____ value: ____","type: Series
value: Unknown

This code takes the column (series) ""Complex"" and
returns a new series of True and False values.
Each True in the new column is a result of an entry in the
""Complex"" column containing ""Verde"". Each
False in the new column is a result of an entry in the
""Complex"" column failing to contain ""Verde"".
Since we are not given the entirety of the ""Complex""
column, the exact value of the resulting series is unknown to us.",,
1006,Sp,24,Final,,Problem 9,Problem 9.5,"apts.get(""Sqft"").median() > 1000
type: ____ value: ____","type: bool
value: Unknown

This code finds the median of the column (series) ""Sqft""
and compares it to a value of 1000, resulting in a bool
value of True or False. Since we do not know
the median of the ""Sqft"" column, the exact value of the
resulting code is unknown to us.",,
1007,Sp,24,Final,,Problem 10,Problem 10,"We want to use the data in apts to test the following
hypotheses:

Null Hypothesis: The rent of the apartments in UTC
and the rents of the apartments in other neighborhoods come from the
same distribution.
Alternative Hypothesis: The rents of the apartments
in UTC are higher than the rents of the apartments in
other neighborhoods on average.

While we could answer this question with a permutation test, in this
problem we will explore another way to test these hypotheses. Since this
is a question of whether two samples come from the same unknown
population distribution, we need to construct a “population” to sample
from. We will construct our “population” in the same way as we would for
a permutation test, except we will draw our sample differently. Instead
of shuffling, we will draw our two samples with
replacement from the constructed “population”. We will use as
our test statistic the difference in means between the two samples (in
the order UTC minus elsewhere).",,,
1008,Sp,24,Final,,Problem 10,Problem 10.1,"Suppose the data in apts, which has 800 rows, includes
85 apartments in UTC. Fill in the blanks below so that
p_val evaluates to the p-value for this hypothesis test,
which we will test according to the strategy outlined above.
diffs = np.array([])
for i in np.arange(10000):
    utc_sample_mean = __(a)__
    elsewhere_sample_mean = __(b)__
    diffs = np.append(diffs, utc_sample_mean - elsewhere_sample_mean)
observed_utc_mean = __(c)__
observed_elsewhere_mean = __(d)__
observed_diff = observed_utc_mean - observed_elsewhere_mean
p_val = np.count_nonzero(diffs __(e)__ observed_diff) / 10000","apts.sample(85, replace=True).get(""Rent"").mean()


apts.sample(715, replace=True).get(""Rent"").mean()


apts[apts.get(""neighborhood"")==""UTC""].get(""Rent"").mean()


apts[apts.get(""neighborhood"")!=""UTC""].get(""Rent"").mean()


>=


For blanks (a) and (b), we can gather from context (hypothesis test
description, variable names, and being inside of a for loop) that this
portion of our code needs to repeatedly generate samples of size 85 (the
number of observations in our dataset that are from UTC) and size 715
(the number of observations in our dataset that are not from UTC). We
will then take the means of these samples and assign them to
utc_sample_mean and elsewhere_sample_mean. We
can generate these samples, with replacement, from the rows in our
dataframe, hinting that the correct code for blanks (a) and (b) are:
apts.sample(85, replace=True).get(""Rent"").mean() and
apts.sample(715, replace=True).get(""Rent"").mean().
For blanks (c) and (d), this portion of the code needs to take our
original dataframe and gather the observed means for apartments from UTC
and apartments not from UTC. We can achieve this by querying our
dataframe, grabbing the rent column, and taking the mean. This implies
our correct code for blanks (c) and (d) are:
apts[apts.get(""neighborhood"")==""UTC""].get(""Rent"").mean()
and
apts[apts.get(""neighborhood"")!=""UTC""].get(""Rent"").mean().
For blank (e), we need to determine, based off of our null and
alternative hypotheses, how we should compare the differences in found
in our simulations against our observed difference. TODO",,
1009,Sp,24,Final,,Problem 10,Problem 10.2,"Now suppose we tested the same hypothesses with a permutation test
using the same test statistic. Which of your answers above (part a)
would need to change? Select all that apply.

 blank (a)
 blank (b)
 blank (c)
 blank (d)
 blank (e)
 None of these.","Blanks (a) and (b) would need to change. For
a permutation test, we would shuffle the labels in our apts
dataset and find the utc_sample_mean and
elsewhere_sample_mean of this new shuffled dataset. Note
that this process is done without replacement and that
both of these sample means are calculated from the same shuffle
of our dataset.
As it currently stands, our code for blanks (a) and (b) do not
reflect this; the current process is sampling with
replacement from two different shuffles of our
dataset. So, blanks (a) and (b) must change.",,
1010,Sp,24,Final,,Problem 10,Problem 10.3,"Now suppose we test the following pair of hypotheses.

Null Hypothesis: The rents of the apartments in UTC
and the rents of the apartments in other neighborhoods come from the
same distribution.
Alternative Hypothesis: The rents of the apartments
in UTC are different than the rents of the apartments
in other neighborhoods on average.

Then we can test this pair of hypotheses by constructing a 95%
confidence interval for a parameter and checking if some particular
number, x, falls in that confidence
interval. To do this:

What parameter should we construct a 95% confidence interval for?
Your answer should be a phrase or short sentence.
What is the value of x? Your
answer should be a number.
Suppose x is in the 95%
confidence interval we create. Select all valid conclusions
below.


 We reject the null hypothesis at the 5% significance level.
 We reject the null hypothesis at the 1% significance level.
 We fail to reject the null hypothesis at the 5% significance
level.
 We fail to reject the null hypothesis at the 1% significance
level.","(i) The average rent of an apartment in UTC minus the average rent
of an apartment elsewhere, or vice versa.
(ii) 0.
(iii) 3rd and 4th options.

For (i), we need to construct a confidence interval for a parameter
that allows us to make assessments about our null and alternative
hypotheses. Since these two hypotheses discuss whether or not there
exists a difference, on average, for rents of
apartments in UTC versus rents of apartments elsewhere, our parameter
should be: the difference in rent for apartments in UTC and
apartments elsewhere on average, or vice versa (The average
rent of an apartment in UTC minus the average rent of an apartment
elsewhere, or vice versa.)
For (ii), x must be 0 because the
value zero holds special significance in our confidence interval; the
inclusion of zero within our confidence interval suggests that “there is
no difference between rent of apartments in UTC and apartments
elsewhere, on average”. Whether or not zero is included within our
confidence interval tells us whether we should fail to reject or reject
the null hypothesis.
For (iii), if x = 0 lies within our
95% confidence interval, it suggests that there is a sizable chance that
there is no difference between rent of apartments in UTC and apartments
elsewhere, on average, which is a conclusion in favor of our null
hypothesis; this means that any options which reject the null
hypothesis, such as the 1st and 2st options, are wrong. The 3rd option
(correctly) fails to the reject the null hypothesis at the 5%
significance level, which is exactly what a 95% confidence interval that
includes x = 0 would support. The 4th
option is also correct because any evidence weak enough to fail to
reject the null hypothesis at the 5% significance level will also fail
at a tighter, more rigorous significance level (such as 1%).",,
1011,Sp,24,Final,,Problem 11,Problem 11,"Next year, six of the DSC tutors (Kathleen, Sophia, Ashley, Claire,
and Vivian) want to rent a 3-bedroom apartment together. Each person
will have a single roommate with whom they’ll share a bedroom. Each
person will have a single roommate with whom they’ll share a bedroom.
They determine the bedroom assignments randomly such that each possible
arrangement is equally likely.
For both questions below, give your answer as a simplified fraction
or exact decimal.
Hint: Both answers can be expressed in the form
\frac{1}{k} for an integer value of
k.",,,
1012,Sp,24,Final,,Problem 11,Problem 11.1,"What is the probability that Kathleen and Sophia are roommates?
Hint: Think about the problem from the perspective
of Kathleen.","\displaystyle\frac{1}{5}
From Kathleen’s perspective, there are 5 tutors that are equally
likely to become her roommate. So, the probability that Sophia ends up
being Kathleen’s roommate is \displaystyle\frac{1}{5}.",,
1013,Sp,24,Final,,Problem 11,Problem 11.2,"What is the probability that the bedroom assignments are the
following: Kathleen with Sophia, Kate with Ashley, and Claire with
Vivian?","\frac{1}{15}
In order to get this combination of roommates, we can use similar
logic as before. From Kathleen’s perspective, there are 5 tutors that
are equally likely to become her roommate. So, the probability that
Sophia ends up being Kathleen’s roommate is \displaystyle\frac{1}{5}. From there, we can
view the situation from Kate’s perspective; Kate sees that there are 3
potential roommates left (Ashley, Claire, Vivian). So, the probability
that Sophia ends up being Kathleen’s roommate (given Kathleen and Sophia
are together) is \displaystyle\frac{1}{3}. After Kate chooses
her roommate, Claire and Vivian end up together by process of
elimination. We can multiply these two probabilities to recieve: \displaystyle\frac{1}{5} \cdot
\displaystyle\frac{1}{5} = \displaystyle\frac{1}{15}.",,
1014,Sp,24,Final,,Problem 12,Problem 12,"Suppose you know the following information.

The average monthly rent for the apartments in apts is
$3,000 with a standard deviation of $400.
The average size of the apartments in apts is 2,000
square feet, with a standard deviation of 100 square feet.
The correlation coefficient between rent and square footage is
0.9.

For all parts of this quesiton, give your answer as an
integer.",,,
1015,Sp,24,Final,,Problem 12,Problem 12.1,"Suppose the rents are normally distributed. What is the rent below
which 84% of apartments are priced?","$3,400
We can use the 68-95-99.7 rule to approximate this answer. The
(68-95-99.7
rule)[https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule#:~:text=In%20statistics%2C%20the%2068%E2%80%9395,two%2C%20and%20three%20standard%20deviations]
is a handy shortcut for approximating how much data from a distribution
lies below/above/within certain value ranges. It states that, for a
normal distribution:

Roughly 68% of the data will lie within 1 standard deviation from
the mean.
Roughly 95% of the data will lie within 2 standard deviations from
the mean.
Roughly 99.7% of the data will lie within 3 standard deviations from
the mean.

The bottom 84% percent of our apts data is roughly
equivalent to “all data that lies below 1 standard deviation above the
mean.” In this case, let the mean of our distribution be $3,000, and let
the standard deviation be $400; the rent for which 84% of our apartments
are priced is therefore $3,400.",,
1016,Sp,24,Final,,Problem 12,Problem 12.2,"Sophie’s apartment rents for $5,000. What is this rent in standard
units?","5
Standard units (or Z-score) is the number of standard deviations an
observation is away from the mean of a distribution. In this case, we
want to find how many standard deviations ($400) that our observation
($5000) is away from the mean ($3000). The math works out to five
standard deviations:
\frac{5000 - 3000}{400} = 5",,
1017,Sp,24,Final,,Problem 12,Problem 12.3,"Based on what you know about the rent of Sophie’s apartment, use the
regression line to predict the square footage of Sophie’s apartment.","2450
The correlation coefficient of 0.9 tells us about the slope of the
regression line to predict square footage from rent; this means that
“for every standard unit traveled right in the x-direction (rent), the
regression line heads 0.9 standard units up in the y-direction (square
footage).”
Sophie’s apartment rent is $5000 (or five standard units in the
x-direction, rent). So, to get our regresion line prediction for the
square footage of Sophie’s apartment, we should head 5 \cdot 0.9 = 4.5 standard units upwards from
the mean in the y-direction, square footage. The standard deviation for
square footage is $100; this implies that the prediction for Sophie’s
apartment square footage should be 100 \cdot
4.5 = 450 square feet above the mean (2000 square feet), totaling
to a final prediction of 2450 square feet.",,
1018,Sp,24,Final,,Problem 12,Problem 12.4,"Sophie’s apartment is actually 2,300 square feet. What is the
residual of your prediction?","-150
A residual just measures the difference between the observed
and the predicted value. If our observation is 2300 square
feet, and our prediction is 2450 square feet, our residual is then -150
square feet.",,
1019,Sp,24,Final,,Problem 12,Problem 12.5,"Cici’s apartment is 1,800 square feet. Based on this information, use
the regression line to predict the rent of Cici’s apartment.","$2,280
The correlation coefficient of 0.9 also tells us about the
slope of the regression line to predict rent from square footage; this
means that “for every standard unit traveled right in the x-direction
(square footage), the regression line heads 0.9 standard units up in the
y-direction (rent).”
Cici’s apartment square footage is 1,800 square feet (or negative two
standard units in the x-direction, square footage). So, to get our
regresion line prediction for the rent of Cici’s apartment, we should
head -2 \cdot 0.9 = -1.8 standard units
from the mean in the y-direction, rent. The standard deviation for rent
is $400; this implies that the prediction for Cici’s apartment rent
should be 400 \cdot -1.8 = 720 square
feet below the mean (3000 dollars), totaling to a final prediction of
$2280.",,
1020,Sp,24,Final,,Problem 13,Problem 13,,,,
1021,Sp,24,Final,,Problem 13,Problem 13.1,"Values in the ""Bath"" column are ""One"",
""One and a half"", ""Two"",
""Two and a half"", and ""Three"". Fill in the
blank in the function float_bath that will convert any
string from the ""Bath"" column into its corresponding number
of bathrooms, as a float. For example,
float_bath(""One and a half"") should return
1.5.
def float_bath(s):
    if ""One"" in s:
        n_baths = 1
    elif ""Two"" in s:
        n_baths = 3
    else:
        n_baths = 3
    if ""and a half"" in s:
        __(a)__
    return n_baths
What goes in blank (a)?","n_baths = n_baths + 0.5
The behavior that we want this line of code to have is to work
regardless if the bath string contains ""One"",
""Two"", or ""Three"". This means we need to have
some way of taking the value that n_baths is already
assigned and adding 0.5 to it. So, our code should read
n_baths = n_baths + 0.5.",,
1022,Sp,24,Final,,Problem 13,Problem 13.2,"Values in the ""Lease Term"" column are
""1 month"", ""6 months"", and
""1 year"". Fill in the blanks in the function
int_lease() that will convert any string from the
""Lease Term"" column to the corresponding length of the
lease, in months, as an integer.
def int_lease(s):
    if s[-1] == ""r"":
        return __(b)__
    else:
        return __(c)__
What goes in blanks (b) and (c)?","(b): 12
(c): int(s[0])

The code in blank (b) will only be run if the last letter of
s is ""r"", which only happens when
s = ""1 year"". So, blank (b) should return
12.
The code in blank (c) will run when s has any value
other than ""1 year"". This includes only two options:
1 month, and 6 months. In order to get the
corresponding number of the months for these two string values, we just
need to take the first character of the string and convert it from a
str type to an int type. So, blank (c) should
return int(s[0]).",,
1023,Sp,24,Final,,Problem 13,Problem 13.3,"Values in the ""Bed"" column are ""Studio"",
""One"", ""Two"", and ""Three"". The
function int_bed provided below converts any string from
the ""Bed"" column to the corresponding number of bedrooms,
as an integer. Note that ""Studio"" apartments count as
having 0 bedrooms.
def int_bed(s):
    if s == ""Studio"":
        return 0
    elif s == ""One"":
        return 1
    elif s == ""Two"":
        return 2
    return 3
Using the provided int_bed function, write one line of
code that modifies the ""Bed"" column of the
apts DataFrame so that it contains integers instead of
strings.
Important: We will assume throughout the rest of
this exam that we have converted the ""Bed"" column of
apts so that it now contains ints.","apts = apts.assign(Bed = apts.get(""Bed"").apply(int_bed))
The code above takes the “Bed” column, apts.get(""Bed""),
and uses .apply(int_bed), which runs each entry through the
int_bed function that we have defined above. All that is
left is to save the result back to the dataframe; this can be done with
.assign().",,
1024,Sp,24,Final,,Problem 14,Problem 14,"Consider the following four slopes.

The slope of the regression line predicting ""Rent"" from
""Sqft"".
The slope of the regression line predicting ""Sqft"" from
""Rent"".
The slope of the regression like predicting ""Rent"" from
""Bed""
The slope of the regression line predicting ""Bed"" from
""Rent"".

Note that we don’t have enough information to calculate all of these
slopes, but you should be able to answer the questions below based not
on calculations, but on the interpretation of what these slopes
represent in the context of housing.",,,
1025,Sp,24,Final,,Problem 14,Problem 14.1,"Which of the above slopes do you expect to be the
largest?

 1
 2
 3
 4","Option 3.
The largest slope out of these four options will be the slope that
represents the greatest increase in y-units per x-unit: m = \dfrac{\Delta y}{\Delta x}.
Option 1, which predicts ""Rent"" from
""Sqft"", has large values for its y-variable
(""Rent""), but also has large values for its x-variable
(""Sqft""). The resulting slope is not that big, as it is a
fraction of large values over large values.
Option 2, which predicts ""Sqft"" from
""Rent"", is also not that big of a slope for the same
reasons as Option 1 (slope is a fraction of large values over large
values).
Option 3, which predicts ""Rent"" from ""Bed"",
has large values for its y-variable (""Rent""), but has small
values for its x-variable (""Bed""). The resulting slope is
incredibly big, as it is a fraction of large values over small
values.
Option 4, which predicts ""Bed"" from ""Rent"",
has small values for its y-variable (""Bed""), but has large
values for its x-variable (""Rent""). The resulting slope is
incredibly small, as it is a fraction of small values over large
values.
Of all four options, Option 3 is the largest slope.",,
1026,Sp,24,Final,,Problem 14,Problem 14.2,"Which of the above slopes do you expect to be the
smallest?","Option 4.
As explained above, Option 4 is the smallest slope.",,
1027,Sp,24,Final,,Problem 15,Problem 15,"Imagine a DataFrame constructed from apts called
bedrooms, which has one row for each bedroom in an
apartment in apts. More specifically, a one bedroom
apartment in apts will appear as one row in
bedrooms, a two bedroom apartment in apts will
appear as two rows in bedrooms, and a three bedroom
apartment in apts will appear as three rows in
bedrooms. Studio apartments will not appear in
bedrooms at all.
The ""Apartment ID"" column of bedrooms
contains the ""Apartment ID"" of the apartment in
apts. Notice that this is not the index of
bedrooms since these values are no longer unique. The
""Cost"" column of bedrooms contains the rent of
the apartment divided by the number of bedrooms. All rows of
bedrooms with a shared ""Apartment ID"" should
therefore have the same value in the ""Cost"" column.",,,
1028,Sp,24,Final,,Problem 15,Problem 15.1,"Recall that apts has 800 rows. How many rows does
bedrooms have?

 800
 More than 800.
 Less than 800.
 Not enough information.","Not enough information.
It is entirely possible that bedrooms has more or less
than 800 rows; we don’t have enough info to tell.
If most of the 800 rows in apts are studio apartments,
most rows in apts will not have corresponding rows in
bedrooms (studio apartments are not reflected in
bedrooms). This would lower the total number of rows in
bedrooms to less than 800.
If most of the 800 rows in apts are three-bedroom
apartments, most rows in apts will each have three
corresponding rows in bedrooms. This would increase the
total number of rows in bedrooms to more than 800.",,
1029,Sp,24,Final,,Problem 15,Problem 15.2,"Suppose no_studio is defined as follows. (Remember, we
previously converted the ""Beds"" column to integers.)
no_studio = apts[apts.get(""Bed"") != 0]
Which of the following statements evaluate to the same value as the
expression below?
bedrooms.get(""Cost"").mean()
Select all that apply.

 no_studio.get(""Rent"").mean()
 no_studio.get(""Rent"").sum() / apts.get(""Bed"").sum()
 (no_studio.get(""Rent"") / no_studio.get(""Bed"")).mean()
 (no_studio.get(""Rent"") / no_studio.get(""Bed"").sum()).sum()
 no_studio.get(""Rent"").mean() / no_studio.get(""Bed"").mean()
 None of these.","Options 2, 4, and 5.
Let’s refer to bedrooms.get(""Cost"").mean() as “the
bedroom code” for this solution.
Option 1 is incorrect. Option 1 takes the mean of all non-studio
apartment rents in apts. This value is significantly larger
than what is produced by the bedroom code (average value of the “Cost”
column in bedrooms), since all “Cost” values in
bedrooms are less than or equal to their corresponding
“Rent” values in apts. So, these two expressions cannot be
equal.
Option 2 is correct. We can view the bedroom code as the same as
summing all of the values in the “Cost” column of bedrooms
and dividing by the total number of rows of bedrooms. This
is a fraction; we can make some clever substitutions in this fraction to
show it is the same as the code for Option 2:

\dfrac{\text{sum of ""Cost"" in
bedrooms}}{\# \text{ of rows in bedrooms}} \to \dfrac{\text{sum of
""Rent"" in no}\_\text{studio}}{\# \text{ of rows in bedrooms}}
\to \dfrac{\text{sum of ""Rent"" in
no}\_\text{studio}}{\text{sum of ""Bed"" in apts}}

Option 3 is incorrect. The first part of Option 3,
no_studio.get(""Rent"") / no_studio.get(""Bed""), produces a
Series that contains all the values in the “Cost” column of
no_studio, except without duplicated rows for multi-bed
apartments. Taking the .mean() of this look-alike Series is
not the same as taking the .mean() of the bedroom code, so
these two expressions cannot be equal.
Option 4 is correct. We can show the bedroom code is equivalent to
the code in Option 4 as follows:

\dfrac{\text{sum of ""Cost"" in
bedrooms}}{\# \text{ of rows in bedrooms}} \to \dots \to
\dfrac{\text{sum of ""Rent"" in no}\_\text{studio}}{\text{sum of
""Bed"" in apts}} \to 
 \dfrac{\text{sum of ""Rent"" in
no}\_\text{studio}}{\text{sum of ""Bed"" in no}\_\text{studio}}
\to
\text{sum} \left( \dfrac{\text{each entry in ""Rent"" in
no}\_\text{studio}}{\text{sum of ""Bed"" in no}\_\text{studio}}
\right) 

Option 5 is correct. We can show the bedroom code is equivalent to
the code in Option 5 as follows:

\dfrac{\text{sum of ""Cost"" in
bedrooms}}{\# \text{ of rows in bedrooms}} \to \dots \to
\dfrac{\text{sum of ""Rent"" in no}\_\text{studio}}{\text{sum of
""Bed"" in no}\_\text{studio}} \to
\dfrac{\left(\dfrac{\text{sum of
""Rent"" in no}\_\text{studio}}{\# \text{ of rows in
no}\_\text{studio}}\right)}{\left(\dfrac{\text{sum of ""Bed"" in
no}\_\text{studio}}{\# \text{ of rows in no}\_\text{studio}}\right)} \to
\dfrac{\text{mean of ""Rent"" in no}\_\text{studio}}{\text{mean
of ""Bed"" in no}\_\text{studio}}",,
1030,Sp,24,Final,,Problem 16,Problem 16,"The table below shows the proportion of apartments of each type in
each of three neighborhoods. Note that each column sums to 1.



Type
North Park
Chula Vista
La Jolla",,,
1031,Sp,24,Final,,Problem 16,Problem 16.1,"Find the total variation distance (TVD) between North Park and Chula
Vista. Give your answer as an exact decimal.","0.2
To find the TVD, we take the absolute differences between North Park
and Chula Vista for all rows, sum them, then cut the result in half.
\dfrac{|0.3 - 0.15| + |0.4 - 0.35| + |0.2
- 0.25| + |0.1 - 0.25|}{2} = \dfrac{0.15 + 0.05 + 0.05 + 0.15}{2} =
\dfrac{0.4}{2} = 0.2",,
1032,Sp,24,Final,,Problem 16,Problem 16.2,"Which pair of neighborhoods is most similar in terms of types of
housing, as measured by TVD?

 North Park and Chula Vista
 North Park and La Jolla
 Chula Vista and La Jolla","North Park and La Jolla
The TVD between North Park and La Jolla is the lowest between all
pairs of two of these three neighborhoods:




Pair
TVD




North Park and Chula Vista
0.2


North Park and La Jolla
0.15


Chula Vista and La Jolla
0.25




This implies that the distributions of apartment types for North Park
and La Jolla are the most similar.",,
1033,Sp,24,Final,,Problem 16,Problem 16.3,"25% of apartments in Little Italy are one bedroom apartments. Based
on this information, what is the minimum and maximum possible TVD
between North Park and Little Italy? Give your answers as exact
decimals.
Minimum: ______ Maximum: ______","Minimum: 0.15
Maximum: 0.65

The minimum TVD is 0.15 because:

One-Bedroom Apartments for North Park and Little Italy already have
a gap of |0.4 - 0.25| = 0.15
A best-possible configuration of the remaining
0.75 of the Little Italy distribution
(Studio: 0.3, Two Bed: 0.2, Three Bed: 0.25) produces an additional |0.3 - 0.3| + |0.2 - 0.2| + |0.1 - 0.25| =
0.15 error against North Park.
The TVD of this optimal scenario is \frac{0.15 + 0.15}{2} = 0.15.

The maximum TVD is 0.65 because:

One-Bedroom Apartments for North Park and Little Italy already have
a gap of |0.4 - 0.25| = 0.15
The worst-possible configuration of the remaining
0.75 of the Little Italy distribution
(Studio: 0.0, Two Bed: 0.0, Three Bed: 0.75) produces an additional |0.3 - 0| + |0.2 - 0| + |0.1 - 0.75| = 1.15
error against North Park.
The TVD of this worst scenario is \frac{0.15 + 1.15}{2} = 0.65.







 
👋
Feedback: Find an error? Still confused? Have a suggestion?
Let us know
here.",,
